{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heng's Starter code for training the classification model on Kaggle.**\n",
    "\n",
    "I have not run the kernel with GPU enabled because I do not have much of Kaggle GPU left as of now. So the kernel as expected is giving CUDA error.\n",
    "This is just a simple kernel for training model on Kaggle easily. Made some minor changes to his code and seems like it will run fine here on kaggle.\n",
    "I have not tested the training time. It can exceed the 9 hour limit.\n",
    "\n",
    "The kernel is based on Heng's starter kit version 20190910 you can find it [here](https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/106462#latest-645576) .\n",
    "I have imported 2 utility scripts one for the utility functions with plotting code and another one is for model.\n",
    "You can fork and edit the utility scripts and add the model classes as you feel like.\n",
    "The model architecture can be changed from this kernel below by changing the Net() class.\n",
    "\n",
    "If you face any problems or errors then feel free to comment them.\n",
    "At last thank you very much [Heng](https://www.kaggle.com/hengck23) and other leaderboard rankers for helping newbies like me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "\n",
    "from lib.utility_functions import *\n",
    "from lib.models_all import *\n",
    "from lib.rate import *\n",
    "\n",
    "#import pretrainedmodels\n",
    "\n",
    "PI = np.pi\n",
    "IMAGE_RGB_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGE_RGB_STD  = [0.229, 0.224, 0.225]\n",
    "DEFECT_COLOR = [(0,0,0),(0,0,255),(0,255,0),(255,0,0),(0,255,255)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_DIR = 'data/split'\n",
    "DATA_DIR = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def load_pretrain(self, skip=['logit.'], is_print=True):\n",
    "        load_pretrain(self, skip, pretrain_file=PRETRAIN_FILE, conversion=CONVERSION, is_print=is_print)\n",
    "\n",
    "    def __init__(self, num_class=4):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        e = ResNext50()\n",
    "        self.block0 = e.block0\n",
    "        self.block1 = e.block1\n",
    "        self.block2 = e.block2\n",
    "        self.block3 = e.block3\n",
    "        self.block4 = e.block4\n",
    "        e = None  #dropped\n",
    "\n",
    "        self.feature = nn.Conv2d(2048, 64, kernel_size=1) #dummy conv for dim reduction\n",
    "        self.logit   = nn.Conv2d(64, num_class, kernel_size=1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size,C,H,W = x.shape\n",
    "        x = x.clone()\n",
    "        x = x-torch.FloatTensor(IMAGE_RGB_MEAN).to(x.device).view(1,-1,1,1)\n",
    "        x = x/torch.FloatTensor(IMAGE_RGB_STD).to(x.device).view(1,-1,1,1)\n",
    "\n",
    "        x = self.block0(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "\n",
    "        x = F.dropout(x,0.5,training=self.training)\n",
    "        x = F.avg_pool2d(x, kernel_size=(8, 13),stride=(8, 8))\n",
    "        #x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = self.feature(x)\n",
    "\n",
    "        logit = self.logit(x) #.view(batch_size,-1)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class which is used by the infor object in __get_item__\n",
    "class Struct(object):\n",
    "    def __init__(self, is_copy=False, **kwargs):\n",
    "        self.add(is_copy, **kwargs)\n",
    "\n",
    "    def add(self, is_copy=False, **kwargs):\n",
    "        #self.__dict__.update(kwargs)\n",
    "\n",
    "        if is_copy == False:\n",
    "            for key, value in kwargs.items():\n",
    "                setattr(self, key, value)\n",
    "        else:\n",
    "            for key, value in kwargs.items():\n",
    "                try:\n",
    "                    setattr(self, key, copy.deepcopy(value))\n",
    "                    #setattr(self, key, value.copy())\n",
    "                except Exception:\n",
    "                    setattr(self, key, value)\n",
    "\n",
    "    def __str__(self):\n",
    "        text =''\n",
    "        for k,v in self.__dict__.items():\n",
    "            text += '\\t%s : %s\\n'%(k, str(v))\n",
    "        return text\n",
    "\n",
    "# Creating masks\n",
    "def run_length_decode(rle, height=256, width=1600, fill_value=1):\n",
    "    mask = np.zeros((height,width), np.float32)\n",
    "    if rle != '':\n",
    "        mask=mask.reshape(-1)\n",
    "        r = [int(r) for r in rle.split(' ')]\n",
    "        r = np.array(r).reshape(-1, 2)\n",
    "        for start,length in r:\n",
    "            start = start-1  #???? 0 or 1 index ???\n",
    "            mask[start:(start + length)] = fill_value\n",
    "        mask=mask.reshape(width, height).T\n",
    "    return mask\n",
    "\n",
    "# Collations\n",
    "def null_collate(batch):\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    input = []\n",
    "    truth_mask  = []\n",
    "    truth_label = []\n",
    "    infor = []\n",
    "    for b in range(batch_size):\n",
    "        input.append(batch[b][0])\n",
    "        truth_mask.append(batch[b][1])\n",
    "        infor.append(batch[b][2])\n",
    "\n",
    "        label = (batch[b][1].reshape(4,-1).sum(1)>8).astype(np.int32)\n",
    "        truth_label.append(label)\n",
    "\n",
    "\n",
    "    input = np.stack(input)\n",
    "    input = image_to_input(input, IMAGE_RGB_MEAN,IMAGE_RGB_STD)\n",
    "    input = torch.from_numpy(input).float()\n",
    "\n",
    "    truth_mask = np.stack(truth_mask)\n",
    "    truth_mask = (truth_mask>0.5).astype(np.float32)\n",
    "    truth_mask = torch.from_numpy(truth_mask).float()\n",
    "\n",
    "    truth_label = np.array(truth_label)\n",
    "    truth_label = torch.from_numpy(truth_label).float()\n",
    "\n",
    "    return input, truth_mask, truth_label, infor\n",
    "\n",
    "# Metric\n",
    "def metric_hit(logit, truth, threshold=0.5):\n",
    "    batch_size,num_class, H,W = logit.shape\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logit = logit.view(batch_size,num_class,-1)\n",
    "        truth = truth.view(batch_size,num_class,-1)\n",
    "\n",
    "        probability = torch.sigmoid(logit)\n",
    "        p = (probability>threshold).float()\n",
    "        t = (truth>0.5).float()\n",
    "\n",
    "        tp = ((p + t) == 2).float()  # True positives\n",
    "        tn = ((p + t) == 0).float()  # True negatives\n",
    "\n",
    "        tp = tp.sum(dim=[0,2])\n",
    "        tn = tn.sum(dim=[0,2])\n",
    "        num_pos = t.sum(dim=[0,2])\n",
    "        num_neg = batch_size*H*W - num_pos\n",
    "\n",
    "        tp = tp.data.cpu().numpy()\n",
    "        tn = tn.data.cpu().numpy().sum()\n",
    "        num_pos = num_pos.data.cpu().numpy()\n",
    "        num_neg = num_neg.data.cpu().numpy().sum()\n",
    "\n",
    "        tp = np.nan_to_num(tp/(num_pos+1e-12),0)\n",
    "        tn = np.nan_to_num(tn/(num_neg+1e-12),0)\n",
    "\n",
    "        tp = list(tp)\n",
    "        num_pos = list(num_pos)\n",
    "\n",
    "    return tn,tp, num_neg,num_pos\n",
    "\n",
    "# Loss\n",
    "#def criterion(logit, truth, weight=None):\n",
    "#    batch_size,num_class, H,W = logit.shape\n",
    "#    logit = logit.view(batch_size,num_class)\n",
    "#    truth = truth.view(batch_size,num_class)\n",
    "#    assert(logit.shape==truth.shape)\n",
    "#\n",
    "#    loss = F.binary_cross_entropy_with_logits(logit, truth, reduction='none')\n",
    "#\n",
    "#    if weight is None:\n",
    "#        loss = loss.mean()\n",
    "#\n",
    "#    else:\n",
    "#        pos = (truth>0.5).float()\n",
    "#        neg = (truth<0.5).float()\n",
    "#        pos_sum = pos.sum().item() + 1e-12\n",
    "#        neg_sum = neg.sum().item() + 1e-12\n",
    "#        loss = (weight[1]*pos*loss/pos_sum + weight[0]*neg*loss/neg_sum).sum()\n",
    "#        #raise NotImplementedError\n",
    "#\n",
    "#    return loss\n",
    "\n",
    "\n",
    "def criterion(logit, truth, weight=None):\n",
    "    batch_size,num_class = logit.shape[:2]\n",
    "    logit = logit.view(batch_size,num_class)\n",
    "    truth = truth.view(batch_size,num_class)\n",
    "\n",
    "    if weight is None: weight=[1,1,1,1]\n",
    "    weight = torch.FloatTensor(weight).to(truth.device).view(1,-1)\n",
    "\n",
    "    loss = F.binary_cross_entropy_with_logits(logit, truth, reduction='none')\n",
    "\n",
    "    loss = loss*weight\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n",
    "# Learning Rate Adjustments\n",
    "def adjust_learning_rate(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def get_learning_rate(optimizer):\n",
    "    lr=[]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr += [param_group['lr']]\n",
    "\n",
    "    assert(len(lr)==1) #we support only one param_group\n",
    "    lr = lr[0]\n",
    "    return lr\n",
    "\n",
    "# Learning Rate Schedule\n",
    "class NullScheduler():\n",
    "    def __init__(self, lr=0.01 ):\n",
    "        super(NullScheduler, self).__init__()\n",
    "        self.lr    = lr\n",
    "        self.cycle = 0\n",
    "\n",
    "    def __call__(self, time):\n",
    "        return self.lr\n",
    "\n",
    "    def __str__(self):\n",
    "        string = 'NullScheduler\\n' \\\n",
    "                + 'lr=%0.5f '%(self.lr)\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteelDataset(Dataset):\n",
    "    def __init__(self, split, csv, mode, augment=None):\n",
    "        self.split   = split\n",
    "        self.csv     = csv\n",
    "        self.mode    = mode\n",
    "        self.augment = augment\n",
    "\n",
    "        self.uid = list(np.concatenate([np.load(SPLIT_DIR + '/%s'%f , allow_pickle=True) for f in split]))\n",
    "        df = pd.concat([pd.read_csv(DATA_DIR + '/%s'%f) for f in csv])\n",
    "        df.fillna('', inplace=True)\n",
    "        df['Class'] = df['ImageId_ClassId'].str[-1].astype(np.int32)\n",
    "        df['Label'] = (df['EncodedPixels']!='').astype(np.int32)\n",
    "        df = df_loc_by_list(df, 'ImageId_ClassId', [ u.split('/')[-1] + '_%d'%c  for u in self.uid for c in [1,2,3,4] ])\n",
    "        self.df = df\n",
    "\n",
    "    def __str__(self):\n",
    "        num1 = (self.df['Class']==1).sum()\n",
    "        num2 = (self.df['Class']==2).sum()\n",
    "        num3 = (self.df['Class']==3).sum()\n",
    "        num4 = (self.df['Class']==4).sum()\n",
    "        pos1 = ((self.df['Class']==1) & (self.df['Label']==1)).sum()\n",
    "        pos2 = ((self.df['Class']==2) & (self.df['Label']==1)).sum()\n",
    "        pos3 = ((self.df['Class']==3) & (self.df['Label']==1)).sum()\n",
    "        pos4 = ((self.df['Class']==4) & (self.df['Label']==1)).sum()\n",
    "\n",
    "        length = len(self)\n",
    "        num = len(self)*4\n",
    "        pos = (self.df['Label']==1).sum()\n",
    "        neg = num-pos\n",
    "\n",
    "        string  = ''\n",
    "        string += '\\tmode    = %s\\n'%self.mode\n",
    "        string += '\\tsplit   = %s\\n'%self.split\n",
    "        string += '\\tcsv     = %s\\n'%str(self.csv)\n",
    "        string += '\\t\\tlen   = %5d\\n'%len(self)\n",
    "        if self.mode == 'train':\n",
    "            string += '\\t\\tnum   = %5d\\n'%num\n",
    "            string += '\\t\\tneg   = %5d  %0.3f\\n'%(neg,neg/num)\n",
    "            string += '\\t\\tpos   = %5d  %0.3f\\n'%(pos,pos/num)\n",
    "            string += '\\t\\tpos1  = %5d  %0.3f  %0.3f\\n'%(pos1,pos1/length,pos1/pos)\n",
    "            string += '\\t\\tpos2  = %5d  %0.3f  %0.3f\\n'%(pos2,pos2/length,pos2/pos)\n",
    "            string += '\\t\\tpos3  = %5d  %0.3f  %0.3f\\n'%(pos3,pos3/length,pos3/pos)\n",
    "            string += '\\t\\tpos4  = %5d  %0.3f  %0.3f\\n'%(pos4,pos4/length,pos4/pos)\n",
    "        return string\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.uid)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        folder, image_id = self.uid[index].split('/')\n",
    "        rle = [\n",
    "            self.df.loc[self.df['ImageId_ClassId']==image_id + '_1','EncodedPixels'].values[0],\n",
    "            self.df.loc[self.df['ImageId_ClassId']==image_id + '_2','EncodedPixels'].values[0],\n",
    "            self.df.loc[self.df['ImageId_ClassId']==image_id + '_3','EncodedPixels'].values[0],\n",
    "            self.df.loc[self.df['ImageId_ClassId']==image_id + '_4','EncodedPixels'].values[0],\n",
    "        ]\n",
    "        \n",
    "        image = cv2.imread(DATA_DIR + '/%s/%s'%(folder,image_id), cv2.IMREAD_COLOR)\n",
    "        mask  = np.array([run_length_decode(r, height=256, width=1600, fill_value=1) for r in rle])\n",
    "\n",
    "        infor = Struct(\n",
    "            index    = index,\n",
    "            folder   = folder,\n",
    "            image_id = image_id,\n",
    "        )\n",
    "\n",
    "        if self.augment is None:\n",
    "            return image, mask, infor\n",
    "        else:\n",
    "            return self.augment(image, mask, infor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiveBalanceClassSampler(Sampler):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        label = (self.dataset.df['Label'].values)\n",
    "        \n",
    "        #cannot reshape array of size 49155 into shape (4)\n",
    "        label = label.reshape(-1,4)\n",
    "        label = np.hstack([label.sum(1,keepdims=True)==0,label]).T\n",
    "\n",
    "        self.neg_index  = np.where(label[0])[0]\n",
    "        self.pos1_index = np.where(label[1])[0]\n",
    "        self.pos2_index = np.where(label[2])[0]\n",
    "        self.pos3_index = np.where(label[3])[0]\n",
    "        self.pos4_index = np.where(label[4])[0]\n",
    "\n",
    "        #5x\n",
    "        self.num_image = len(self.dataset.df)//4\n",
    "        self.length = self.num_image*5\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        neg  = np.random.choice(self.neg_index,  self.num_image, replace=True)\n",
    "        pos1 = np.random.choice(self.pos1_index, self.num_image, replace=True)\n",
    "        pos2 = np.random.choice(self.pos2_index, self.num_image, replace=True)\n",
    "        pos3 = np.random.choice(self.pos3_index, self.num_image, replace=True)\n",
    "        pos4 = np.random.choice(self.pos4_index, self.num_image, replace=True)\n",
    "\n",
    "        l = np.stack([neg,pos1,pos2,pos3,pos4]).T\n",
    "        l = l.reshape(-1)\n",
    "        return iter(l)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_valid(net, valid_loader, displays=None):\n",
    "    valid_num  = np.zeros(6, np.float32)\n",
    "    valid_loss = np.zeros(6, np.float32)\n",
    "    \n",
    "    for t, (input, truth_mask, truth_label, infor) in enumerate(valid_loader):\n",
    "\n",
    "        net.eval()\n",
    "        input = input.cuda()\n",
    "        truth_mask  = truth_mask.cuda()\n",
    "        truth_label = truth_label.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logit = net(input) #data_parallel(net, input)  \n",
    "            logit = logit.max(-1,True)[0]\n",
    "            loss  = criterion(logit, truth_label)\n",
    "            tn,tp, num_neg,num_pos = metric_hit(logit, truth_label)\n",
    "\n",
    "        batch_size = len(infor)\n",
    "        l = np.array([ loss.item(), tn,*tp])\n",
    "        n = np.array([ batch_size, num_neg,*num_pos])\n",
    "        valid_loss += l*n\n",
    "        valid_num  += n\n",
    "\n",
    "        if displays is not None:\n",
    "            probability = torch.sigmoid(logit)\n",
    "            image = input_to_image(input, IMAGE_RGB_MEAN,IMAGE_RGB_STD)\n",
    "\n",
    "            probability_label = probability.data.cpu().numpy()\n",
    "            truth_label = truth_label.data.cpu().numpy()\n",
    "            truth_mask  = truth_mask.data.cpu().numpy()\n",
    "\n",
    "            for b in range(0, batch_size, 4):\n",
    "                image_id = infor[b].image_id[:-4]\n",
    "                result = draw_predict_result_label(image[b], truth_mask[b], truth_label[b], probability_label[b], stack='vertical')\n",
    "                draw_shadow_text(result,'%05d    %s.jpg'%(valid_num[0]-batch_size+b, image_id),(5,24),0.75,[255,255,255],1)\n",
    "                image_show('result',result,resize=1)\n",
    "\n",
    "        print('\\r %8d /%8d'%(valid_num[0], len(valid_loader.dataset)),end='',flush=True)\n",
    "\n",
    "    assert(valid_num[0] == len(valid_loader.dataset))\n",
    "    valid_loss = valid_loss/valid_num\n",
    "\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERSION=[\n",
    " 'block0.0.weight',\t(64, 3, 7, 7),\t 'layer0.conv1.weight',\t(64, 3, 7, 7),\n",
    " 'block0.1.weight',\t(64,),\t 'layer0.bn1.weight',\t(64,),\n",
    " 'block0.1.bias',\t(64,),\t 'layer0.bn1.bias',\t(64,),\n",
    " 'block0.1.running_mean',\t(64,),\t 'layer0.bn1.running_mean',\t(64,),\n",
    " 'block0.1.running_var',\t(64,),\t 'layer0.bn1.running_var',\t(64,),\n",
    " 'block1.1.conv_bn1.conv.weight',\t(128, 64, 1, 1),\t 'layer1.0.conv1.weight',\t(128, 64, 1, 1),\n",
    " 'block1.1.conv_bn1.bn.weight',\t(128,),\t 'layer1.0.bn1.weight',\t(128,),\n",
    " 'block1.1.conv_bn1.bn.bias',\t(128,),\t 'layer1.0.bn1.bias',\t(128,),\n",
    " 'block1.1.conv_bn1.bn.running_mean',\t(128,),\t 'layer1.0.bn1.running_mean',\t(128,),\n",
    " 'block1.1.conv_bn1.bn.running_var',\t(128,),\t 'layer1.0.bn1.running_var',\t(128,),\n",
    " 'block1.1.conv_bn2.conv.weight',\t(128, 4, 3, 3),\t 'layer1.0.conv2.weight',\t(128, 4, 3, 3),\n",
    " 'block1.1.conv_bn2.bn.weight',\t(128,),\t 'layer1.0.bn2.weight',\t(128,),\n",
    " 'block1.1.conv_bn2.bn.bias',\t(128,),\t 'layer1.0.bn2.bias',\t(128,),\n",
    " 'block1.1.conv_bn2.bn.running_mean',\t(128,),\t 'layer1.0.bn2.running_mean',\t(128,),\n",
    " 'block1.1.conv_bn2.bn.running_var',\t(128,),\t 'layer1.0.bn2.running_var',\t(128,),\n",
    " 'block1.1.conv_bn3.conv.weight',\t(256, 128, 1, 1),\t 'layer1.0.conv3.weight',\t(256, 128, 1, 1),\n",
    " 'block1.1.conv_bn3.bn.weight',\t(256,),\t 'layer1.0.bn3.weight',\t(256,),\n",
    " 'block1.1.conv_bn3.bn.bias',\t(256,),\t 'layer1.0.bn3.bias',\t(256,),\n",
    " 'block1.1.conv_bn3.bn.running_mean',\t(256,),\t 'layer1.0.bn3.running_mean',\t(256,),\n",
    " 'block1.1.conv_bn3.bn.running_var',\t(256,),\t 'layer1.0.bn3.running_var',\t(256,),\n",
    " 'block1.1.scale.fc1.weight',\t(16, 256, 1, 1),\t 'layer1.0.se_module.fc1.weight',\t(16, 256, 1, 1),\n",
    " 'block1.1.scale.fc1.bias',\t(16,),\t 'layer1.0.se_module.fc1.bias',\t(16,),\n",
    " 'block1.1.scale.fc2.weight',\t(256, 16, 1, 1),\t 'layer1.0.se_module.fc2.weight',\t(256, 16, 1, 1),\n",
    " 'block1.1.scale.fc2.bias',\t(256,),\t 'layer1.0.se_module.fc2.bias',\t(256,),\n",
    " 'block1.1.shortcut.conv.weight',\t(256, 64, 1, 1),\t 'layer1.0.downsample.0.weight',\t(256, 64, 1, 1),\n",
    " 'block1.1.shortcut.bn.weight',\t(256,),\t 'layer1.0.downsample.1.weight',\t(256,),\n",
    " 'block1.1.shortcut.bn.bias',\t(256,),\t 'layer1.0.downsample.1.bias',\t(256,),\n",
    " 'block1.1.shortcut.bn.running_mean',\t(256,),\t 'layer1.0.downsample.1.running_mean',\t(256,),\n",
    " 'block1.1.shortcut.bn.running_var',\t(256,),\t 'layer1.0.downsample.1.running_var',\t(256,),\n",
    " 'block1.2.conv_bn1.conv.weight',\t(128, 256, 1, 1),\t 'layer1.1.conv1.weight',\t(128, 256, 1, 1),\n",
    " 'block1.2.conv_bn1.bn.weight',\t(128,),\t 'layer1.1.bn1.weight',\t(128,),\n",
    " 'block1.2.conv_bn1.bn.bias',\t(128,),\t 'layer1.1.bn1.bias',\t(128,),\n",
    " 'block1.2.conv_bn1.bn.running_mean',\t(128,),\t 'layer1.1.bn1.running_mean',\t(128,),\n",
    " 'block1.2.conv_bn1.bn.running_var',\t(128,),\t 'layer1.1.bn1.running_var',\t(128,),\n",
    " 'block1.2.conv_bn2.conv.weight',\t(128, 4, 3, 3),\t 'layer1.1.conv2.weight',\t(128, 4, 3, 3),\n",
    " 'block1.2.conv_bn2.bn.weight',\t(128,),\t 'layer1.1.bn2.weight',\t(128,),\n",
    " 'block1.2.conv_bn2.bn.bias',\t(128,),\t 'layer1.1.bn2.bias',\t(128,),\n",
    " 'block1.2.conv_bn2.bn.running_mean',\t(128,),\t 'layer1.1.bn2.running_mean',\t(128,),\n",
    " 'block1.2.conv_bn2.bn.running_var',\t(128,),\t 'layer1.1.bn2.running_var',\t(128,),\n",
    " 'block1.2.conv_bn3.conv.weight',\t(256, 128, 1, 1),\t 'layer1.1.conv3.weight',\t(256, 128, 1, 1),\n",
    " 'block1.2.conv_bn3.bn.weight',\t(256,),\t 'layer1.1.bn3.weight',\t(256,),\n",
    " 'block1.2.conv_bn3.bn.bias',\t(256,),\t 'layer1.1.bn3.bias',\t(256,),\n",
    " 'block1.2.conv_bn3.bn.running_mean',\t(256,),\t 'layer1.1.bn3.running_mean',\t(256,),\n",
    " 'block1.2.conv_bn3.bn.running_var',\t(256,),\t 'layer1.1.bn3.running_var',\t(256,),\n",
    " 'block1.2.scale.fc1.weight',\t(16, 256, 1, 1),\t 'layer1.1.se_module.fc1.weight',\t(16, 256, 1, 1),\n",
    " 'block1.2.scale.fc1.bias',\t(16,),\t 'layer1.1.se_module.fc1.bias',\t(16,),\n",
    " 'block1.2.scale.fc2.weight',\t(256, 16, 1, 1),\t 'layer1.1.se_module.fc2.weight',\t(256, 16, 1, 1),\n",
    " 'block1.2.scale.fc2.bias',\t(256,),\t 'layer1.1.se_module.fc2.bias',\t(256,),\n",
    " 'block1.3.conv_bn1.conv.weight',\t(128, 256, 1, 1),\t 'layer1.2.conv1.weight',\t(128, 256, 1, 1),\n",
    " 'block1.3.conv_bn1.bn.weight',\t(128,),\t 'layer1.2.bn1.weight',\t(128,),\n",
    " 'block1.3.conv_bn1.bn.bias',\t(128,),\t 'layer1.2.bn1.bias',\t(128,),\n",
    " 'block1.3.conv_bn1.bn.running_mean',\t(128,),\t 'layer1.2.bn1.running_mean',\t(128,),\n",
    " 'block1.3.conv_bn1.bn.running_var',\t(128,),\t 'layer1.2.bn1.running_var',\t(128,),\n",
    " 'block1.3.conv_bn2.conv.weight',\t(128, 4, 3, 3),\t 'layer1.2.conv2.weight',\t(128, 4, 3, 3),\n",
    " 'block1.3.conv_bn2.bn.weight',\t(128,),\t 'layer1.2.bn2.weight',\t(128,),\n",
    " 'block1.3.conv_bn2.bn.bias',\t(128,),\t 'layer1.2.bn2.bias',\t(128,),\n",
    " 'block1.3.conv_bn2.bn.running_mean',\t(128,),\t 'layer1.2.bn2.running_mean',\t(128,),\n",
    " 'block1.3.conv_bn2.bn.running_var',\t(128,),\t 'layer1.2.bn2.running_var',\t(128,),\n",
    " 'block1.3.conv_bn3.conv.weight',\t(256, 128, 1, 1),\t 'layer1.2.conv3.weight',\t(256, 128, 1, 1),\n",
    " 'block1.3.conv_bn3.bn.weight',\t(256,),\t 'layer1.2.bn3.weight',\t(256,),\n",
    " 'block1.3.conv_bn3.bn.bias',\t(256,),\t 'layer1.2.bn3.bias',\t(256,),\n",
    " 'block1.3.conv_bn3.bn.running_mean',\t(256,),\t 'layer1.2.bn3.running_mean',\t(256,),\n",
    " 'block1.3.conv_bn3.bn.running_var',\t(256,),\t 'layer1.2.bn3.running_var',\t(256,),\n",
    " 'block1.3.scale.fc1.weight',\t(16, 256, 1, 1),\t 'layer1.2.se_module.fc1.weight',\t(16, 256, 1, 1),\n",
    " 'block1.3.scale.fc1.bias',\t(16,),\t 'layer1.2.se_module.fc1.bias',\t(16,),\n",
    " 'block1.3.scale.fc2.weight',\t(256, 16, 1, 1),\t 'layer1.2.se_module.fc2.weight',\t(256, 16, 1, 1),\n",
    " 'block1.3.scale.fc2.bias',\t(256,),\t 'layer1.2.se_module.fc2.bias',\t(256,),\n",
    " 'block2.0.conv_bn1.conv.weight',\t(256, 256, 1, 1),\t 'layer2.0.conv1.weight',\t(256, 256, 1, 1),\n",
    " 'block2.0.conv_bn1.bn.weight',\t(256,),\t 'layer2.0.bn1.weight',\t(256,),\n",
    " 'block2.0.conv_bn1.bn.bias',\t(256,),\t 'layer2.0.bn1.bias',\t(256,),\n",
    " 'block2.0.conv_bn1.bn.running_mean',\t(256,),\t 'layer2.0.bn1.running_mean',\t(256,),\n",
    " 'block2.0.conv_bn1.bn.running_var',\t(256,),\t 'layer2.0.bn1.running_var',\t(256,),\n",
    " 'block2.0.conv_bn2.conv.weight',\t(256, 8, 3, 3),\t 'layer2.0.conv2.weight',\t(256, 8, 3, 3),\n",
    " 'block2.0.conv_bn2.bn.weight',\t(256,),\t 'layer2.0.bn2.weight',\t(256,),\n",
    " 'block2.0.conv_bn2.bn.bias',\t(256,),\t 'layer2.0.bn2.bias',\t(256,),\n",
    " 'block2.0.conv_bn2.bn.running_mean',\t(256,),\t 'layer2.0.bn2.running_mean',\t(256,),\n",
    " 'block2.0.conv_bn2.bn.running_var',\t(256,),\t 'layer2.0.bn2.running_var',\t(256,),\n",
    " 'block2.0.conv_bn3.conv.weight',\t(512, 256, 1, 1),\t 'layer2.0.conv3.weight',\t(512, 256, 1, 1),\n",
    " 'block2.0.conv_bn3.bn.weight',\t(512,),\t 'layer2.0.bn3.weight',\t(512,),\n",
    " 'block2.0.conv_bn3.bn.bias',\t(512,),\t 'layer2.0.bn3.bias',\t(512,),\n",
    " 'block2.0.conv_bn3.bn.running_mean',\t(512,),\t 'layer2.0.bn3.running_mean',\t(512,),\n",
    " 'block2.0.conv_bn3.bn.running_var',\t(512,),\t 'layer2.0.bn3.running_var',\t(512,),\n",
    " 'block2.0.scale.fc1.weight',\t(32, 512, 1, 1),\t 'layer2.0.se_module.fc1.weight',\t(32, 512, 1, 1),\n",
    " 'block2.0.scale.fc1.bias',\t(32,),\t 'layer2.0.se_module.fc1.bias',\t(32,),\n",
    " 'block2.0.scale.fc2.weight',\t(512, 32, 1, 1),\t 'layer2.0.se_module.fc2.weight',\t(512, 32, 1, 1),\n",
    " 'block2.0.scale.fc2.bias',\t(512,),\t 'layer2.0.se_module.fc2.bias',\t(512,),\n",
    " 'block2.0.shortcut.conv.weight',\t(512, 256, 1, 1),\t 'layer2.0.downsample.0.weight',\t(512, 256, 1, 1),\n",
    " 'block2.0.shortcut.bn.weight',\t(512,),\t 'layer2.0.downsample.1.weight',\t(512,),\n",
    " 'block2.0.shortcut.bn.bias',\t(512,),\t 'layer2.0.downsample.1.bias',\t(512,),\n",
    " 'block2.0.shortcut.bn.running_mean',\t(512,),\t 'layer2.0.downsample.1.running_mean',\t(512,),\n",
    " 'block2.0.shortcut.bn.running_var',\t(512,),\t 'layer2.0.downsample.1.running_var',\t(512,),\n",
    " 'block2.1.conv_bn1.conv.weight',\t(256, 512, 1, 1),\t 'layer2.1.conv1.weight',\t(256, 512, 1, 1),\n",
    " 'block2.1.conv_bn1.bn.weight',\t(256,),\t 'layer2.1.bn1.weight',\t(256,),\n",
    " 'block2.1.conv_bn1.bn.bias',\t(256,),\t 'layer2.1.bn1.bias',\t(256,),\n",
    " 'block2.1.conv_bn1.bn.running_mean',\t(256,),\t 'layer2.1.bn1.running_mean',\t(256,),\n",
    " 'block2.1.conv_bn1.bn.running_var',\t(256,),\t 'layer2.1.bn1.running_var',\t(256,),\n",
    " 'block2.1.conv_bn2.conv.weight',\t(256, 8, 3, 3),\t 'layer2.1.conv2.weight',\t(256, 8, 3, 3),\n",
    " 'block2.1.conv_bn2.bn.weight',\t(256,),\t 'layer2.1.bn2.weight',\t(256,),\n",
    " 'block2.1.conv_bn2.bn.bias',\t(256,),\t 'layer2.1.bn2.bias',\t(256,),\n",
    " 'block2.1.conv_bn2.bn.running_mean',\t(256,),\t 'layer2.1.bn2.running_mean',\t(256,),\n",
    " 'block2.1.conv_bn2.bn.running_var',\t(256,),\t 'layer2.1.bn2.running_var',\t(256,),\n",
    " 'block2.1.conv_bn3.conv.weight',\t(512, 256, 1, 1),\t 'layer2.1.conv3.weight',\t(512, 256, 1, 1),\n",
    " 'block2.1.conv_bn3.bn.weight',\t(512,),\t 'layer2.1.bn3.weight',\t(512,),\n",
    " 'block2.1.conv_bn3.bn.bias',\t(512,),\t 'layer2.1.bn3.bias',\t(512,),\n",
    " 'block2.1.conv_bn3.bn.running_mean',\t(512,),\t 'layer2.1.bn3.running_mean',\t(512,),\n",
    " 'block2.1.conv_bn3.bn.running_var',\t(512,),\t 'layer2.1.bn3.running_var',\t(512,),\n",
    " 'block2.1.scale.fc1.weight',\t(32, 512, 1, 1),\t 'layer2.1.se_module.fc1.weight',\t(32, 512, 1, 1),\n",
    " 'block2.1.scale.fc1.bias',\t(32,),\t 'layer2.1.se_module.fc1.bias',\t(32,),\n",
    " 'block2.1.scale.fc2.weight',\t(512, 32, 1, 1),\t 'layer2.1.se_module.fc2.weight',\t(512, 32, 1, 1),\n",
    " 'block2.1.scale.fc2.bias',\t(512,),\t 'layer2.1.se_module.fc2.bias',\t(512,),\n",
    " 'block2.2.conv_bn1.conv.weight',\t(256, 512, 1, 1),\t 'layer2.2.conv1.weight',\t(256, 512, 1, 1),\n",
    " 'block2.2.conv_bn1.bn.weight',\t(256,),\t 'layer2.2.bn1.weight',\t(256,),\n",
    " 'block2.2.conv_bn1.bn.bias',\t(256,),\t 'layer2.2.bn1.bias',\t(256,),\n",
    " 'block2.2.conv_bn1.bn.running_mean',\t(256,),\t 'layer2.2.bn1.running_mean',\t(256,),\n",
    " 'block2.2.conv_bn1.bn.running_var',\t(256,),\t 'layer2.2.bn1.running_var',\t(256,),\n",
    " 'block2.2.conv_bn2.conv.weight',\t(256, 8, 3, 3),\t 'layer2.2.conv2.weight',\t(256, 8, 3, 3),\n",
    " 'block2.2.conv_bn2.bn.weight',\t(256,),\t 'layer2.2.bn2.weight',\t(256,),\n",
    " 'block2.2.conv_bn2.bn.bias',\t(256,),\t 'layer2.2.bn2.bias',\t(256,),\n",
    " 'block2.2.conv_bn2.bn.running_mean',\t(256,),\t 'layer2.2.bn2.running_mean',\t(256,),\n",
    " 'block2.2.conv_bn2.bn.running_var',\t(256,),\t 'layer2.2.bn2.running_var',\t(256,),\n",
    " 'block2.2.conv_bn3.conv.weight',\t(512, 256, 1, 1),\t 'layer2.2.conv3.weight',\t(512, 256, 1, 1),\n",
    " 'block2.2.conv_bn3.bn.weight',\t(512,),\t 'layer2.2.bn3.weight',\t(512,),\n",
    " 'block2.2.conv_bn3.bn.bias',\t(512,),\t 'layer2.2.bn3.bias',\t(512,),\n",
    " 'block2.2.conv_bn3.bn.running_mean',\t(512,),\t 'layer2.2.bn3.running_mean',\t(512,),\n",
    " 'block2.2.conv_bn3.bn.running_var',\t(512,),\t 'layer2.2.bn3.running_var',\t(512,),\n",
    " 'block2.2.scale.fc1.weight',\t(32, 512, 1, 1),\t 'layer2.2.se_module.fc1.weight',\t(32, 512, 1, 1),\n",
    " 'block2.2.scale.fc1.bias',\t(32,),\t 'layer2.2.se_module.fc1.bias',\t(32,),\n",
    " 'block2.2.scale.fc2.weight',\t(512, 32, 1, 1),\t 'layer2.2.se_module.fc2.weight',\t(512, 32, 1, 1),\n",
    " 'block2.2.scale.fc2.bias',\t(512,),\t 'layer2.2.se_module.fc2.bias',\t(512,),\n",
    " 'block2.3.conv_bn1.conv.weight',\t(256, 512, 1, 1),\t 'layer2.3.conv1.weight',\t(256, 512, 1, 1),\n",
    " 'block2.3.conv_bn1.bn.weight',\t(256,),\t 'layer2.3.bn1.weight',\t(256,),\n",
    " 'block2.3.conv_bn1.bn.bias',\t(256,),\t 'layer2.3.bn1.bias',\t(256,),\n",
    " 'block2.3.conv_bn1.bn.running_mean',\t(256,),\t 'layer2.3.bn1.running_mean',\t(256,),\n",
    " 'block2.3.conv_bn1.bn.running_var',\t(256,),\t 'layer2.3.bn1.running_var',\t(256,),\n",
    " 'block2.3.conv_bn2.conv.weight',\t(256, 8, 3, 3),\t 'layer2.3.conv2.weight',\t(256, 8, 3, 3),\n",
    " 'block2.3.conv_bn2.bn.weight',\t(256,),\t 'layer2.3.bn2.weight',\t(256,),\n",
    " 'block2.3.conv_bn2.bn.bias',\t(256,),\t 'layer2.3.bn2.bias',\t(256,),\n",
    " 'block2.3.conv_bn2.bn.running_mean',\t(256,),\t 'layer2.3.bn2.running_mean',\t(256,),\n",
    " 'block2.3.conv_bn2.bn.running_var',\t(256,),\t 'layer2.3.bn2.running_var',\t(256,),\n",
    " 'block2.3.conv_bn3.conv.weight',\t(512, 256, 1, 1),\t 'layer2.3.conv3.weight',\t(512, 256, 1, 1),\n",
    " 'block2.3.conv_bn3.bn.weight',\t(512,),\t 'layer2.3.bn3.weight',\t(512,),\n",
    " 'block2.3.conv_bn3.bn.bias',\t(512,),\t 'layer2.3.bn3.bias',\t(512,),\n",
    " 'block2.3.conv_bn3.bn.running_mean',\t(512,),\t 'layer2.3.bn3.running_mean',\t(512,),\n",
    " 'block2.3.conv_bn3.bn.running_var',\t(512,),\t 'layer2.3.bn3.running_var',\t(512,),\n",
    " 'block2.3.scale.fc1.weight',\t(32, 512, 1, 1),\t 'layer2.3.se_module.fc1.weight',\t(32, 512, 1, 1),\n",
    " 'block2.3.scale.fc1.bias',\t(32,),\t 'layer2.3.se_module.fc1.bias',\t(32,),\n",
    " 'block2.3.scale.fc2.weight',\t(512, 32, 1, 1),\t 'layer2.3.se_module.fc2.weight',\t(512, 32, 1, 1),\n",
    " 'block2.3.scale.fc2.bias',\t(512,),\t 'layer2.3.se_module.fc2.bias',\t(512,),\n",
    " 'block3.0.conv_bn1.conv.weight',\t(512, 512, 1, 1),\t 'layer3.0.conv1.weight',\t(512, 512, 1, 1),\n",
    " 'block3.0.conv_bn1.bn.weight',\t(512,),\t 'layer3.0.bn1.weight',\t(512,),\n",
    " 'block3.0.conv_bn1.bn.bias',\t(512,),\t 'layer3.0.bn1.bias',\t(512,),\n",
    " 'block3.0.conv_bn1.bn.running_mean',\t(512,),\t 'layer3.0.bn1.running_mean',\t(512,),\n",
    " 'block3.0.conv_bn1.bn.running_var',\t(512,),\t 'layer3.0.bn1.running_var',\t(512,),\n",
    " 'block3.0.conv_bn2.conv.weight',\t(512, 16, 3, 3),\t 'layer3.0.conv2.weight',\t(512, 16, 3, 3),\n",
    " 'block3.0.conv_bn2.bn.weight',\t(512,),\t 'layer3.0.bn2.weight',\t(512,),\n",
    " 'block3.0.conv_bn2.bn.bias',\t(512,),\t 'layer3.0.bn2.bias',\t(512,),\n",
    " 'block3.0.conv_bn2.bn.running_mean',\t(512,),\t 'layer3.0.bn2.running_mean',\t(512,),\n",
    " 'block3.0.conv_bn2.bn.running_var',\t(512,),\t 'layer3.0.bn2.running_var',\t(512,),\n",
    " 'block3.0.conv_bn3.conv.weight',\t(1024, 512, 1, 1),\t 'layer3.0.conv3.weight',\t(1024, 512, 1, 1),\n",
    " 'block3.0.conv_bn3.bn.weight',\t(1024,),\t 'layer3.0.bn3.weight',\t(1024,),\n",
    " 'block3.0.conv_bn3.bn.bias',\t(1024,),\t 'layer3.0.bn3.bias',\t(1024,),\n",
    " 'block3.0.conv_bn3.bn.running_mean',\t(1024,),\t 'layer3.0.bn3.running_mean',\t(1024,),\n",
    " 'block3.0.conv_bn3.bn.running_var',\t(1024,),\t 'layer3.0.bn3.running_var',\t(1024,),\n",
    " 'block3.0.scale.fc1.weight',\t(64, 1024, 1, 1),\t 'layer3.0.se_module.fc1.weight',\t(64, 1024, 1, 1),\n",
    " 'block3.0.scale.fc1.bias',\t(64,),\t 'layer3.0.se_module.fc1.bias',\t(64,),\n",
    " 'block3.0.scale.fc2.weight',\t(1024, 64, 1, 1),\t 'layer3.0.se_module.fc2.weight',\t(1024, 64, 1, 1),\n",
    " 'block3.0.scale.fc2.bias',\t(1024,),\t 'layer3.0.se_module.fc2.bias',\t(1024,),\n",
    " 'block3.0.shortcut.conv.weight',\t(1024, 512, 1, 1),\t 'layer3.0.downsample.0.weight',\t(1024, 512, 1, 1),\n",
    " 'block3.0.shortcut.bn.weight',\t(1024,),\t 'layer3.0.downsample.1.weight',\t(1024,),\n",
    " 'block3.0.shortcut.bn.bias',\t(1024,),\t 'layer3.0.downsample.1.bias',\t(1024,),\n",
    " 'block3.0.shortcut.bn.running_mean',\t(1024,),\t 'layer3.0.downsample.1.running_mean',\t(1024,),\n",
    " 'block3.0.shortcut.bn.running_var',\t(1024,),\t 'layer3.0.downsample.1.running_var',\t(1024,),\n",
    " 'block3.1.conv_bn1.conv.weight',\t(512, 1024, 1, 1),\t 'layer3.1.conv1.weight',\t(512, 1024, 1, 1),\n",
    " 'block3.1.conv_bn1.bn.weight',\t(512,),\t 'layer3.1.bn1.weight',\t(512,),\n",
    " 'block3.1.conv_bn1.bn.bias',\t(512,),\t 'layer3.1.bn1.bias',\t(512,),\n",
    " 'block3.1.conv_bn1.bn.running_mean',\t(512,),\t 'layer3.1.bn1.running_mean',\t(512,),\n",
    " 'block3.1.conv_bn1.bn.running_var',\t(512,),\t 'layer3.1.bn1.running_var',\t(512,),\n",
    " 'block3.1.conv_bn2.conv.weight',\t(512, 16, 3, 3),\t 'layer3.1.conv2.weight',\t(512, 16, 3, 3),\n",
    " 'block3.1.conv_bn2.bn.weight',\t(512,),\t 'layer3.1.bn2.weight',\t(512,),\n",
    " 'block3.1.conv_bn2.bn.bias',\t(512,),\t 'layer3.1.bn2.bias',\t(512,),\n",
    " 'block3.1.conv_bn2.bn.running_mean',\t(512,),\t 'layer3.1.bn2.running_mean',\t(512,),\n",
    " 'block3.1.conv_bn2.bn.running_var',\t(512,),\t 'layer3.1.bn2.running_var',\t(512,),\n",
    " 'block3.1.conv_bn3.conv.weight',\t(1024, 512, 1, 1),\t 'layer3.1.conv3.weight',\t(1024, 512, 1, 1),\n",
    " 'block3.1.conv_bn3.bn.weight',\t(1024,),\t 'layer3.1.bn3.weight',\t(1024,),\n",
    " 'block3.1.conv_bn3.bn.bias',\t(1024,),\t 'layer3.1.bn3.bias',\t(1024,),\n",
    " 'block3.1.conv_bn3.bn.running_mean',\t(1024,),\t 'layer3.1.bn3.running_mean',\t(1024,),\n",
    " 'block3.1.conv_bn3.bn.running_var',\t(1024,),\t 'layer3.1.bn3.running_var',\t(1024,),\n",
    " 'block3.1.scale.fc1.weight',\t(64, 1024, 1, 1),\t 'layer3.1.se_module.fc1.weight',\t(64, 1024, 1, 1),\n",
    " 'block3.1.scale.fc1.bias',\t(64,),\t 'layer3.1.se_module.fc1.bias',\t(64,),\n",
    " 'block3.1.scale.fc2.weight',\t(1024, 64, 1, 1),\t 'layer3.1.se_module.fc2.weight',\t(1024, 64, 1, 1),\n",
    " 'block3.1.scale.fc2.bias',\t(1024,),\t 'layer3.1.se_module.fc2.bias',\t(1024,),\n",
    " 'block3.2.conv_bn1.conv.weight',\t(512, 1024, 1, 1),\t 'layer3.2.conv1.weight',\t(512, 1024, 1, 1),\n",
    " 'block3.2.conv_bn1.bn.weight',\t(512,),\t 'layer3.2.bn1.weight',\t(512,),\n",
    " 'block3.2.conv_bn1.bn.bias',\t(512,),\t 'layer3.2.bn1.bias',\t(512,),\n",
    " 'block3.2.conv_bn1.bn.running_mean',\t(512,),\t 'layer3.2.bn1.running_mean',\t(512,),\n",
    " 'block3.2.conv_bn1.bn.running_var',\t(512,),\t 'layer3.2.bn1.running_var',\t(512,),\n",
    " 'block3.2.conv_bn2.conv.weight',\t(512, 16, 3, 3),\t 'layer3.2.conv2.weight',\t(512, 16, 3, 3),\n",
    " 'block3.2.conv_bn2.bn.weight',\t(512,),\t 'layer3.2.bn2.weight',\t(512,),\n",
    " 'block3.2.conv_bn2.bn.bias',\t(512,),\t 'layer3.2.bn2.bias',\t(512,),\n",
    " 'block3.2.conv_bn2.bn.running_mean',\t(512,),\t 'layer3.2.bn2.running_mean',\t(512,),\n",
    " 'block3.2.conv_bn2.bn.running_var',\t(512,),\t 'layer3.2.bn2.running_var',\t(512,),\n",
    " 'block3.2.conv_bn3.conv.weight',\t(1024, 512, 1, 1),\t 'layer3.2.conv3.weight',\t(1024, 512, 1, 1),\n",
    " 'block3.2.conv_bn3.bn.weight',\t(1024,),\t 'layer3.2.bn3.weight',\t(1024,),\n",
    " 'block3.2.conv_bn3.bn.bias',\t(1024,),\t 'layer3.2.bn3.bias',\t(1024,),\n",
    " 'block3.2.conv_bn3.bn.running_mean',\t(1024,),\t 'layer3.2.bn3.running_mean',\t(1024,),\n",
    " 'block3.2.conv_bn3.bn.running_var',\t(1024,),\t 'layer3.2.bn3.running_var',\t(1024,),\n",
    " 'block3.2.scale.fc1.weight',\t(64, 1024, 1, 1),\t 'layer3.2.se_module.fc1.weight',\t(64, 1024, 1, 1),\n",
    " 'block3.2.scale.fc1.bias',\t(64,),\t 'layer3.2.se_module.fc1.bias',\t(64,),\n",
    " 'block3.2.scale.fc2.weight',\t(1024, 64, 1, 1),\t 'layer3.2.se_module.fc2.weight',\t(1024, 64, 1, 1),\n",
    " 'block3.2.scale.fc2.bias',\t(1024,),\t 'layer3.2.se_module.fc2.bias',\t(1024,),\n",
    " 'block3.3.conv_bn1.conv.weight',\t(512, 1024, 1, 1),\t 'layer3.3.conv1.weight',\t(512, 1024, 1, 1),\n",
    " 'block3.3.conv_bn1.bn.weight',\t(512,),\t 'layer3.3.bn1.weight',\t(512,),\n",
    " 'block3.3.conv_bn1.bn.bias',\t(512,),\t 'layer3.3.bn1.bias',\t(512,),\n",
    " 'block3.3.conv_bn1.bn.running_mean',\t(512,),\t 'layer3.3.bn1.running_mean',\t(512,),\n",
    " 'block3.3.conv_bn1.bn.running_var',\t(512,),\t 'layer3.3.bn1.running_var',\t(512,),\n",
    " 'block3.3.conv_bn2.conv.weight',\t(512, 16, 3, 3),\t 'layer3.3.conv2.weight',\t(512, 16, 3, 3),\n",
    " 'block3.3.conv_bn2.bn.weight',\t(512,),\t 'layer3.3.bn2.weight',\t(512,),\n",
    " 'block3.3.conv_bn2.bn.bias',\t(512,),\t 'layer3.3.bn2.bias',\t(512,),\n",
    " 'block3.3.conv_bn2.bn.running_mean',\t(512,),\t 'layer3.3.bn2.running_mean',\t(512,),\n",
    " 'block3.3.conv_bn2.bn.running_var',\t(512,),\t 'layer3.3.bn2.running_var',\t(512,),\n",
    " 'block3.3.conv_bn3.conv.weight',\t(1024, 512, 1, 1),\t 'layer3.3.conv3.weight',\t(1024, 512, 1, 1),\n",
    " 'block3.3.conv_bn3.bn.weight',\t(1024,),\t 'layer3.3.bn3.weight',\t(1024,),\n",
    " 'block3.3.conv_bn3.bn.bias',\t(1024,),\t 'layer3.3.bn3.bias',\t(1024,),\n",
    " 'block3.3.conv_bn3.bn.running_mean',\t(1024,),\t 'layer3.3.bn3.running_mean',\t(1024,),\n",
    " 'block3.3.conv_bn3.bn.running_var',\t(1024,),\t 'layer3.3.bn3.running_var',\t(1024,),\n",
    " 'block3.3.scale.fc1.weight',\t(64, 1024, 1, 1),\t 'layer3.3.se_module.fc1.weight',\t(64, 1024, 1, 1),\n",
    " 'block3.3.scale.fc1.bias',\t(64,),\t 'layer3.3.se_module.fc1.bias',\t(64,),\n",
    " 'block3.3.scale.fc2.weight',\t(1024, 64, 1, 1),\t 'layer3.3.se_module.fc2.weight',\t(1024, 64, 1, 1),\n",
    " 'block3.3.scale.fc2.bias',\t(1024,),\t 'layer3.3.se_module.fc2.bias',\t(1024,),\n",
    " 'block3.4.conv_bn1.conv.weight',\t(512, 1024, 1, 1),\t 'layer3.4.conv1.weight',\t(512, 1024, 1, 1),\n",
    " 'block3.4.conv_bn1.bn.weight',\t(512,),\t 'layer3.4.bn1.weight',\t(512,),\n",
    " 'block3.4.conv_bn1.bn.bias',\t(512,),\t 'layer3.4.bn1.bias',\t(512,),\n",
    " 'block3.4.conv_bn1.bn.running_mean',\t(512,),\t 'layer3.4.bn1.running_mean',\t(512,),\n",
    " 'block3.4.conv_bn1.bn.running_var',\t(512,),\t 'layer3.4.bn1.running_var',\t(512,),\n",
    " 'block3.4.conv_bn2.conv.weight',\t(512, 16, 3, 3),\t 'layer3.4.conv2.weight',\t(512, 16, 3, 3),\n",
    " 'block3.4.conv_bn2.bn.weight',\t(512,),\t 'layer3.4.bn2.weight',\t(512,),\n",
    " 'block3.4.conv_bn2.bn.bias',\t(512,),\t 'layer3.4.bn2.bias',\t(512,),\n",
    " 'block3.4.conv_bn2.bn.running_mean',\t(512,),\t 'layer3.4.bn2.running_mean',\t(512,),\n",
    " 'block3.4.conv_bn2.bn.running_var',\t(512,),\t 'layer3.4.bn2.running_var',\t(512,),\n",
    " 'block3.4.conv_bn3.conv.weight',\t(1024, 512, 1, 1),\t 'layer3.4.conv3.weight',\t(1024, 512, 1, 1),\n",
    " 'block3.4.conv_bn3.bn.weight',\t(1024,),\t 'layer3.4.bn3.weight',\t(1024,),\n",
    " 'block3.4.conv_bn3.bn.bias',\t(1024,),\t 'layer3.4.bn3.bias',\t(1024,),\n",
    " 'block3.4.conv_bn3.bn.running_mean',\t(1024,),\t 'layer3.4.bn3.running_mean',\t(1024,),\n",
    " 'block3.4.conv_bn3.bn.running_var',\t(1024,),\t 'layer3.4.bn3.running_var',\t(1024,),\n",
    " 'block3.4.scale.fc1.weight',\t(64, 1024, 1, 1),\t 'layer3.4.se_module.fc1.weight',\t(64, 1024, 1, 1),\n",
    " 'block3.4.scale.fc1.bias',\t(64,),\t 'layer3.4.se_module.fc1.bias',\t(64,),\n",
    " 'block3.4.scale.fc2.weight',\t(1024, 64, 1, 1),\t 'layer3.4.se_module.fc2.weight',\t(1024, 64, 1, 1),\n",
    " 'block3.4.scale.fc2.bias',\t(1024,),\t 'layer3.4.se_module.fc2.bias',\t(1024,),\n",
    " 'block3.5.conv_bn1.conv.weight',\t(512, 1024, 1, 1),\t 'layer3.5.conv1.weight',\t(512, 1024, 1, 1),\n",
    " 'block3.5.conv_bn1.bn.weight',\t(512,),\t 'layer3.5.bn1.weight',\t(512,),\n",
    " 'block3.5.conv_bn1.bn.bias',\t(512,),\t 'layer3.5.bn1.bias',\t(512,),\n",
    " 'block3.5.conv_bn1.bn.running_mean',\t(512,),\t 'layer3.5.bn1.running_mean',\t(512,),\n",
    " 'block3.5.conv_bn1.bn.running_var',\t(512,),\t 'layer3.5.bn1.running_var',\t(512,),\n",
    " 'block3.5.conv_bn2.conv.weight',\t(512, 16, 3, 3),\t 'layer3.5.conv2.weight',\t(512, 16, 3, 3),\n",
    " 'block3.5.conv_bn2.bn.weight',\t(512,),\t 'layer3.5.bn2.weight',\t(512,),\n",
    " 'block3.5.conv_bn2.bn.bias',\t(512,),\t 'layer3.5.bn2.bias',\t(512,),\n",
    " 'block3.5.conv_bn2.bn.running_mean',\t(512,),\t 'layer3.5.bn2.running_mean',\t(512,),\n",
    " 'block3.5.conv_bn2.bn.running_var',\t(512,),\t 'layer3.5.bn2.running_var',\t(512,),\n",
    " 'block3.5.conv_bn3.conv.weight',\t(1024, 512, 1, 1),\t 'layer3.5.conv3.weight',\t(1024, 512, 1, 1),\n",
    " 'block3.5.conv_bn3.bn.weight',\t(1024,),\t 'layer3.5.bn3.weight',\t(1024,),\n",
    " 'block3.5.conv_bn3.bn.bias',\t(1024,),\t 'layer3.5.bn3.bias',\t(1024,),\n",
    " 'block3.5.conv_bn3.bn.running_mean',\t(1024,),\t 'layer3.5.bn3.running_mean',\t(1024,),\n",
    " 'block3.5.conv_bn3.bn.running_var',\t(1024,),\t 'layer3.5.bn3.running_var',\t(1024,),\n",
    " 'block3.5.scale.fc1.weight',\t(64, 1024, 1, 1),\t 'layer3.5.se_module.fc1.weight',\t(64, 1024, 1, 1),\n",
    " 'block3.5.scale.fc1.bias',\t(64,),\t 'layer3.5.se_module.fc1.bias',\t(64,),\n",
    " 'block3.5.scale.fc2.weight',\t(1024, 64, 1, 1),\t 'layer3.5.se_module.fc2.weight',\t(1024, 64, 1, 1),\n",
    " 'block3.5.scale.fc2.bias',\t(1024,),\t 'layer3.5.se_module.fc2.bias',\t(1024,),\n",
    " 'block4.0.conv_bn1.conv.weight',\t(1024, 1024, 1, 1),\t 'layer4.0.conv1.weight',\t(1024, 1024, 1, 1),\n",
    " 'block4.0.conv_bn1.bn.weight',\t(1024,),\t 'layer4.0.bn1.weight',\t(1024,),\n",
    " 'block4.0.conv_bn1.bn.bias',\t(1024,),\t 'layer4.0.bn1.bias',\t(1024,),\n",
    " 'block4.0.conv_bn1.bn.running_mean',\t(1024,),\t 'layer4.0.bn1.running_mean',\t(1024,),\n",
    " 'block4.0.conv_bn1.bn.running_var',\t(1024,),\t 'layer4.0.bn1.running_var',\t(1024,),\n",
    " 'block4.0.conv_bn2.conv.weight',\t(1024, 32, 3, 3),\t 'layer4.0.conv2.weight',\t(1024, 32, 3, 3),\n",
    " 'block4.0.conv_bn2.bn.weight',\t(1024,),\t 'layer4.0.bn2.weight',\t(1024,),\n",
    " 'block4.0.conv_bn2.bn.bias',\t(1024,),\t 'layer4.0.bn2.bias',\t(1024,),\n",
    " 'block4.0.conv_bn2.bn.running_mean',\t(1024,),\t 'layer4.0.bn2.running_mean',\t(1024,),\n",
    " 'block4.0.conv_bn2.bn.running_var',\t(1024,),\t 'layer4.0.bn2.running_var',\t(1024,),\n",
    " 'block4.0.conv_bn3.conv.weight',\t(2048, 1024, 1, 1),\t 'layer4.0.conv3.weight',\t(2048, 1024, 1, 1),\n",
    " 'block4.0.conv_bn3.bn.weight',\t(2048,),\t 'layer4.0.bn3.weight',\t(2048,),\n",
    " 'block4.0.conv_bn3.bn.bias',\t(2048,),\t 'layer4.0.bn3.bias',\t(2048,),\n",
    " 'block4.0.conv_bn3.bn.running_mean',\t(2048,),\t 'layer4.0.bn3.running_mean',\t(2048,),\n",
    " 'block4.0.conv_bn3.bn.running_var',\t(2048,),\t 'layer4.0.bn3.running_var',\t(2048,),\n",
    " 'block4.0.scale.fc1.weight',\t(128, 2048, 1, 1),\t 'layer4.0.se_module.fc1.weight',\t(128, 2048, 1, 1),\n",
    " 'block4.0.scale.fc1.bias',\t(128,),\t 'layer4.0.se_module.fc1.bias',\t(128,),\n",
    " 'block4.0.scale.fc2.weight',\t(2048, 128, 1, 1),\t 'layer4.0.se_module.fc2.weight',\t(2048, 128, 1, 1),\n",
    " 'block4.0.scale.fc2.bias',\t(2048,),\t 'layer4.0.se_module.fc2.bias',\t(2048,),\n",
    " 'block4.0.shortcut.conv.weight',\t(2048, 1024, 1, 1),\t 'layer4.0.downsample.0.weight',\t(2048, 1024, 1, 1),\n",
    " 'block4.0.shortcut.bn.weight',\t(2048,),\t 'layer4.0.downsample.1.weight',\t(2048,),\n",
    " 'block4.0.shortcut.bn.bias',\t(2048,),\t 'layer4.0.downsample.1.bias',\t(2048,),\n",
    " 'block4.0.shortcut.bn.running_mean',\t(2048,),\t 'layer4.0.downsample.1.running_mean',\t(2048,),\n",
    " 'block4.0.shortcut.bn.running_var',\t(2048,),\t 'layer4.0.downsample.1.running_var',\t(2048,),\n",
    " 'block4.1.conv_bn1.conv.weight',\t(1024, 2048, 1, 1),\t 'layer4.1.conv1.weight',\t(1024, 2048, 1, 1),\n",
    " 'block4.1.conv_bn1.bn.weight',\t(1024,),\t 'layer4.1.bn1.weight',\t(1024,),\n",
    " 'block4.1.conv_bn1.bn.bias',\t(1024,),\t 'layer4.1.bn1.bias',\t(1024,),\n",
    " 'block4.1.conv_bn1.bn.running_mean',\t(1024,),\t 'layer4.1.bn1.running_mean',\t(1024,),\n",
    " 'block4.1.conv_bn1.bn.running_var',\t(1024,),\t 'layer4.1.bn1.running_var',\t(1024,),\n",
    " 'block4.1.conv_bn2.conv.weight',\t(1024, 32, 3, 3),\t 'layer4.1.conv2.weight',\t(1024, 32, 3, 3),\n",
    " 'block4.1.conv_bn2.bn.weight',\t(1024,),\t 'layer4.1.bn2.weight',\t(1024,),\n",
    " 'block4.1.conv_bn2.bn.bias',\t(1024,),\t 'layer4.1.bn2.bias',\t(1024,),\n",
    " 'block4.1.conv_bn2.bn.running_mean',\t(1024,),\t 'layer4.1.bn2.running_mean',\t(1024,),\n",
    " 'block4.1.conv_bn2.bn.running_var',\t(1024,),\t 'layer4.1.bn2.running_var',\t(1024,),\n",
    " 'block4.1.conv_bn3.conv.weight',\t(2048, 1024, 1, 1),\t 'layer4.1.conv3.weight',\t(2048, 1024, 1, 1),\n",
    " 'block4.1.conv_bn3.bn.weight',\t(2048,),\t 'layer4.1.bn3.weight',\t(2048,),\n",
    " 'block4.1.conv_bn3.bn.bias',\t(2048,),\t 'layer4.1.bn3.bias',\t(2048,),\n",
    " 'block4.1.conv_bn3.bn.running_mean',\t(2048,),\t 'layer4.1.bn3.running_mean',\t(2048,),\n",
    " 'block4.1.conv_bn3.bn.running_var',\t(2048,),\t 'layer4.1.bn3.running_var',\t(2048,),\n",
    " 'block4.1.scale.fc1.weight',\t(128, 2048, 1, 1),\t 'layer4.1.se_module.fc1.weight',\t(128, 2048, 1, 1),\n",
    " 'block4.1.scale.fc1.bias',\t(128,),\t 'layer4.1.se_module.fc1.bias',\t(128,),\n",
    " 'block4.1.scale.fc2.weight',\t(2048, 128, 1, 1),\t 'layer4.1.se_module.fc2.weight',\t(2048, 128, 1, 1),\n",
    " 'block4.1.scale.fc2.bias',\t(2048,),\t 'layer4.1.se_module.fc2.bias',\t(2048,),\n",
    " 'block4.2.conv_bn1.conv.weight',\t(1024, 2048, 1, 1),\t 'layer4.2.conv1.weight',\t(1024, 2048, 1, 1),\n",
    " 'block4.2.conv_bn1.bn.weight',\t(1024,),\t 'layer4.2.bn1.weight',\t(1024,),\n",
    " 'block4.2.conv_bn1.bn.bias',\t(1024,),\t 'layer4.2.bn1.bias',\t(1024,),\n",
    " 'block4.2.conv_bn1.bn.running_mean',\t(1024,),\t 'layer4.2.bn1.running_mean',\t(1024,),\n",
    " 'block4.2.conv_bn1.bn.running_var',\t(1024,),\t 'layer4.2.bn1.running_var',\t(1024,),\n",
    " 'block4.2.conv_bn2.conv.weight',\t(1024, 32, 3, 3),\t 'layer4.2.conv2.weight',\t(1024, 32, 3, 3),\n",
    " 'block4.2.conv_bn2.bn.weight',\t(1024,),\t 'layer4.2.bn2.weight',\t(1024,),\n",
    " 'block4.2.conv_bn2.bn.bias',\t(1024,),\t 'layer4.2.bn2.bias',\t(1024,),\n",
    " 'block4.2.conv_bn2.bn.running_mean',\t(1024,),\t 'layer4.2.bn2.running_mean',\t(1024,),\n",
    " 'block4.2.conv_bn2.bn.running_var',\t(1024,),\t 'layer4.2.bn2.running_var',\t(1024,),\n",
    " 'block4.2.conv_bn3.conv.weight',\t(2048, 1024, 1, 1),\t 'layer4.2.conv3.weight',\t(2048, 1024, 1, 1),\n",
    " 'block4.2.conv_bn3.bn.weight',\t(2048,),\t 'layer4.2.bn3.weight',\t(2048,),\n",
    " 'block4.2.conv_bn3.bn.bias',\t(2048,),\t 'layer4.2.bn3.bias',\t(2048,),\n",
    " 'block4.2.conv_bn3.bn.running_mean',\t(2048,),\t 'layer4.2.bn3.running_mean',\t(2048,),\n",
    " 'block4.2.conv_bn3.bn.running_var',\t(2048,),\t 'layer4.2.bn3.running_var',\t(2048,),\n",
    " 'block4.2.scale.fc1.weight',\t(128, 2048, 1, 1),\t 'layer4.2.se_module.fc1.weight',\t(128, 2048, 1, 1),\n",
    " 'block4.2.scale.fc1.bias',\t(128,),\t 'layer4.2.se_module.fc1.bias',\t(128,),\n",
    " 'block4.2.scale.fc2.weight',\t(2048, 128, 1, 1),\t 'layer4.2.se_module.fc2.weight',\t(2048, 128, 1, 1),\n",
    " 'block4.2.scale.fc2.bias',\t(2048,),\t 'layer4.2.se_module.fc2.bias',\t(2048,),\n",
    " 'logit.weight',\t(1000, 1280),\t 'last_linear.weight',\t(1000, 2048),\n",
    " 'logit.bias',\t(1000,),\t 'last_linear.bias',\t(1000,),\n",
    "]\n",
    "\n",
    "PRETRAIN_FILE = 'data/pretrained_models/se_resnext50_32x4d-a260b3a4.pth'\n",
    "\n",
    "def load_pretrain(net, skip=[], pretrain_file=PRETRAIN_FILE, conversion=CONVERSION, is_print=True):\n",
    "\n",
    "    print('\\tload pretrain_file: %s'%pretrain_file)\n",
    "\n",
    "    #pretrain_state_dict = torch.load(pretrain_file)\n",
    "    pretrain_state_dict = torch.load(pretrain_file, map_location=lambda storage, loc: storage)\n",
    "    state_dict = net.state_dict()\n",
    "\n",
    "    i = 0\n",
    "    conversion = np.array(CONVERSION).reshape(-1,4)\n",
    "    for key,_,pretrain_key,_ in conversion:\n",
    "        if any(s in key for s in\n",
    "            ['.num_batches_tracked',]+skip):\n",
    "            continue\n",
    "\n",
    "        #print('\\t\\t',key)\n",
    "        if is_print:\n",
    "            print('\\t\\t','%-48s  %-24s  <---  %-32s  %-24s'%(\n",
    "                key, str(state_dict[key].shape),\n",
    "                pretrain_key, str(pretrain_state_dict[pretrain_key].shape),\n",
    "            ))\n",
    "        i = i+1\n",
    "\n",
    "        state_dict[key] = pretrain_state_dict[pretrain_key]\n",
    "\n",
    "    net.load_state_dict(state_dict)\n",
    "    print('')\n",
    "    print('len(pretrain_state_dict.keys()) = %d'%len(pretrain_state_dict.keys()))\n",
    "    print('len(state_dict.keys())          = %d'%len(state_dict.keys()))\n",
    "    print('loaded    = %d'%i)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_file =  glob.glob('data/train_pseudolabel_images/*.jpg') #train_images\n",
    "#image_file = ['train_pseudolabel_images/'+i.split('/')[-1] for i in image_file] #train_images\n",
    "#random.shuffle(image_file)\n",
    "#\n",
    "##12568\n",
    "#num_valid = 1000\n",
    "#num_all   = len(image_file)\n",
    "#num_train = num_all-num_valid\n",
    "#\n",
    "#train=np.array(image_file[num_valid:])\n",
    "#valid=np.array(image_file[:num_valid])\n",
    "#\n",
    "#print(len(image_file))\n",
    "#print(len(train))\n",
    "#print(len(valid))\n",
    "#print(len(valid)/len(train))\n",
    "#\n",
    "#np.save('data/split/train0_%d.npy'%len(train),train)\n",
    "#np.save('data/split/valid0_%d.npy'%len(valid),valid)\n",
    "#\n",
    "#print('train0_%d.npy'%len(train))\n",
    "#print('valid0_%d.npy'%len(valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    batch_size = 6\n",
    "    \n",
    "    initial_checkpoint = None\n",
    "    #'data/classification_models/00028500_model.pth'\n",
    "    #'resnet34-cls-full-foldb0-0/checkpoint/00007500_model.pth'\n",
    "    \n",
    "    train_dataset = SteelDataset(\n",
    "        mode    = 'train',\n",
    "        csv     = ['train_pseudolabel_segmentation.csv',], #train_pseudolabel_segmentation.csv\n",
    "        split   = ['train0_13369.npy'], #train0_11968.npy\n",
    "\n",
    "        augment = train_augment,\n",
    "    )\n",
    "    train_loader  = DataLoader(\n",
    "        train_dataset,\n",
    "        #sampler     = BalanceClassSampler(train_dataset, 3*len(train_dataset)),\n",
    "        #sampler    = SequentialSampler(train_dataset),\n",
    "        #sampler    = RandomSampler(train_dataset),\n",
    "        sampler    = FiveBalanceClassSampler(train_dataset),\n",
    "        batch_size  = batch_size,\n",
    "        drop_last   = True,\n",
    "        num_workers = 4,\n",
    "        pin_memory  = True,\n",
    "        collate_fn  = null_collate\n",
    "    )\n",
    "\n",
    "    valid_dataset = SteelDataset(\n",
    "        mode    = 'train',\n",
    "        csv     = ['train_pseudolabel_segmentation.csv'], #train_pseudolabel_segmentation.csv\n",
    "        split   = ['valid0_1000.npy'], #valid_b1_1000.npy\n",
    "        augment = valid_augment,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        sampler    = SequentialSampler(valid_dataset),\n",
    "        #sampler     = RandomSampler(valid_dataset),\n",
    "        batch_size  = 4,\n",
    "        drop_last   = False,\n",
    "        num_workers = 4,\n",
    "        pin_memory  = True,\n",
    "        collate_fn  = null_collate\n",
    "    )\n",
    "    \n",
    "    assert(len(train_dataset)>=batch_size)\n",
    "    \n",
    "    net = Net().cuda()\n",
    "    #model_name = 'se_resnext50_32x4d'\n",
    "    #net = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\n",
    "    #net.last_linear = torch.nn.Linear(in_features=180224, out_features=4, bias=True)\n",
    "    #net = net.cuda()\n",
    "    \n",
    "    if initial_checkpoint is not None:\n",
    "        state_dict = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)\n",
    "        #for k in ['logit.weight','logit.bias']: state_dict.pop(k, None)\n",
    "        net.load_state_dict(state_dict,strict=False)\n",
    "    else:\n",
    "        net.load_pretrain(skip=['logit'], is_print=False)\n",
    "\n",
    "    num_iters   = 50*1000 #50*1000\n",
    "    iter_smooth = 50\n",
    "    iter_log    = 500\n",
    "    iter_valid  = 500\n",
    "    iter_save   = [num_iters-1] + list(range(0, num_iters, 1000))#1*1000\n",
    "    \n",
    "    #optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=schduler(0), momentum=0.9, weight_decay=0.0001)\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()))\n",
    "    #scheduler = NullScheduler(lr=0.001)\n",
    "    max_lr = 0.001 #0.0002\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=max_lr, div_factor=25, pct_start=0.3, total_steps=num_iters)\n",
    "    lr = scheduler.get_lr()[0]\n",
    "    \n",
    "    start_iter = 0\n",
    "    start_epoch= 0\n",
    "    rate       = 0\n",
    "    if initial_checkpoint is not None:\n",
    "        initial_optimizer = initial_checkpoint.replace('_model.pth','_optimizer.pth')\n",
    "        if os.path.exists(initial_optimizer):\n",
    "            checkpoint  = torch.load(initial_optimizer)\n",
    "            start_iter  = checkpoint['iter' ]\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        pass\n",
    "    \n",
    "    train_loss = np.zeros(20,np.float32)\n",
    "    valid_loss = np.zeros(20,np.float32)\n",
    "    batch_loss = np.zeros(20,np.float32)\n",
    "    iter_accum = 8\n",
    "    iter = 0\n",
    "    i    = 0\n",
    "    \n",
    "    start = timer()\n",
    "    while  iter<num_iters:\n",
    "        sum_train_loss = np.zeros(20,np.float32)\n",
    "        sum = np.zeros(20,np.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        for t, (input, truth_mask, truth_label, infor) in enumerate(train_loader):\n",
    "            batch_size = len(infor)\n",
    "            iter  = i + start_iter\n",
    "            epoch = (iter-start_iter)*batch_size/len(train_dataset) + start_epoch\n",
    "            \n",
    "            # Weather to display images or not while in validation loss\n",
    "            displays = None\n",
    "            #if 0:\n",
    "            if (iter % iter_valid==0):\n",
    "                valid_loss = do_valid(net, valid_loader, displays) # omitted outdir variable\n",
    "                #pass\n",
    "\n",
    "            if (iter % iter_log==0):\n",
    "                print('\\r',end='',flush=True)\n",
    "                asterisk = '*' if iter in iter_save else ' '\n",
    "                print('%0.8f  %5.1f%s %5.1f |  %5.3f   %4.4f [%4.4f,%4.4f,%4.4f,%4.4f]  |  %5.3f   %4.4f [%4.4f,%4.4f,%4.4f,%4.4f]  | %s' % (\\\n",
    "                         lr, iter/1000, asterisk, epoch,\n",
    "                         *valid_loss[:6],\n",
    "                         *train_loss[:6],\n",
    "                         time_to_str((timer() - start)))\n",
    "                )\n",
    "                print('\\n')\n",
    "                \n",
    "            #if 0:\n",
    "            if iter in iter_save:\n",
    "                torch.save(net.state_dict(),'data/classification_models/se-resnext50/%08d_model.pth'%(iter))\n",
    "                torch.save({\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'iter'     : iter,\n",
    "                    'epoch'    : epoch,\n",
    "                }, 'data/classification_models/se-resnext50/%08d_optimizer.pth'%(iter))\n",
    "                pass\n",
    "\n",
    "            # learning rate schduler -------------\n",
    "            lr = scheduler.get_lr()[0]\n",
    "            if lr<0 : break\n",
    "            #print(lr)\n",
    "            #adjust_learning_rate(optimizer, lr)\n",
    "            #rate = get_learning_rate(optimizer)\n",
    "            \n",
    "            net.train()\n",
    "            input = input.cuda()\n",
    "            truth_label = truth_label.cuda()\n",
    "            truth_mask  = truth_mask.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logit =  net(input) #data_parallel(net,input)  \n",
    "            logit = logit.max(-1,True)[0]\n",
    "            loss = criterion(logit, truth_label)\n",
    "            tn,tp, num_neg,num_pos = metric_hit(logit, truth_label)\n",
    "            \n",
    "            (loss/iter_accum).backward()\n",
    "            if (iter % iter_accum)==0:\n",
    "                optimizer.step()\n",
    "                if iter < num_iters:\n",
    "                    scheduler.step(iter)\n",
    "\n",
    "            # print statistics  ------------\n",
    "            l = np.array([ loss.item(), tn,*tp ])\n",
    "            n = np.array([ batch_size, num_neg,*num_pos ])\n",
    "\n",
    "            batch_loss[:6] = l\n",
    "            sum_train_loss[:6] += l*n\n",
    "            sum[:6] += n\n",
    "            if iter%iter_smooth == 0:\n",
    "                train_loss = sum_train_loss/(sum+1e-12)\n",
    "                sum_train_loss[...] = 0\n",
    "                sum[...]            = 0\n",
    "\n",
    "\n",
    "            print('\\r',end='',flush=True)\n",
    "            asterisk = ' '\n",
    "            print('%0.8f  %5.1f%s %5.1f |  %5.3f   %4.4f [%4.4f,%4.4f,%4.4f,%4.4f]  |  %5.3f   %4.2f [%4.4f,%4.4f,%4.4f,%4.4f]  | %s' % (\\\n",
    "                         lr, iter/1000, asterisk, epoch,\n",
    "                         *valid_loss[:6],\n",
    "                         *batch_loss[:6],\n",
    "                         time_to_str((timer() - start)))\n",
    "            , end='',flush=True)\n",
    "            i=i+1\n",
    "           \n",
    "            # debug-----------------------------\n",
    "            if 1:\n",
    "                for di in range(3):\n",
    "                    if (iter+di)%1000==0:\n",
    "\n",
    "                        probability = torch.sigmoid(logit)\n",
    "                        image = input_to_image(input, IMAGE_RGB_MEAN,IMAGE_RGB_STD)\n",
    "\n",
    "                        probability_label = probability.data.cpu().numpy()\n",
    "                        truth_label = truth_label.data.cpu().numpy()\n",
    "                        truth_mask  = truth_mask.data.cpu().numpy()\n",
    "\n",
    "\n",
    "                        for b in range(batch_size):\n",
    "                            result = draw_predict_result_label(image[b], truth_mask[b], truth_label[b], probability_label[b], stack='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr          iter   epoch |  loss    tn, [tp1,tp2,tp3,tp4]       |  loss    tn, [tp1,tp2,tp3,tp4]       | time           \n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "0.00040000    0.0*   0.0 |  0.709   0.4453 [1.0000,1.0000,0.0000,0.0000]  |  0.000   0.0000 [0.0000,0.0000,0.0000,0.0000]  |  0 hr 00 min\n",
      "\n",
      "\n",
      "0.00103347    0.5    0.2 |  1.156   0.9940 [0.0000,0.0000,0.0000,0.0000]  |  0.585   0.9163 [0.0000,0.5246,0.1429,0.0000]  |  0 hr 07 min\n",
      "\n",
      "\n",
      "0.00276669    1.0*   0.4 |  0.716   0.9736 [0.0000,0.3077,0.0470,0.0952]  |  0.502   0.9010 [0.0000,0.6087,0.2000,0.2903]  |  0 hr 14 min\n",
      "\n",
      "\n",
      "0.00518240    1.5    0.7 |  0.503   0.9338 [0.0000,0.4615,0.0000,0.0000]  |  0.550   0.9210 [0.0000,0.0000,0.1474,0.2742]  |  0 hr 21 min\n",
      "\n",
      "\n",
      "0.00756800    2.0*   0.9 |  0.663   1.0000 [0.0000,0.0000,0.0000,0.0000]  |  0.495   0.9763 [0.0000,0.3651,0.0000,0.0156]  |  0 hr 28 min\n",
      "\n",
      "\n",
      "0.00934894    2.5    1.1 |  0.376   0.9974 [0.0000,0.0000,0.0000,0.0476]  |  0.519   0.9891 [0.0000,0.2951,0.0000,0.0000]  |  0 hr 34 min\n",
      "\n",
      "\n",
      "0.00999987    3.0*   1.3 |  0.367   1.0000 [0.0000,0.0000,0.0000,0.0000]  |  0.502   0.9581 [0.0000,0.2222,0.1183,0.1311]  |  0 hr 41 min\n",
      "\n",
      "\n",
      "0.00987613    3.5    1.6 |  0.409   0.9670 [0.0000,0.3077,0.0000,0.0000]  |  0.539   0.9946 [0.0000,0.0000,0.0698,0.0000]  |  0 hr 48 min\n",
      "\n",
      "\n",
      "0.00951164    4.0*   1.8 |  0.389   1.0000 [0.0000,0.0000,0.0000,0.0000]  |  0.568   0.9586 [0.0000,0.3636,0.0500,0.0000]  |  0 hr 55 min\n",
      "\n",
      "\n",
      "0.00937177    4.1    1.9 |  0.389   1.0000 [0.0000,0.0000,0.0000,0.0000]  |  0.508   1.00 [0.0000,0.0000,0.0000,0.0000]  |  0 hr 57 min"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-75afe7b8ebb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lr          iter   epoch |  loss    tn, [tp1,tp2,tp3,tp4]       |  loss    tn, [tp1,tp2,tp3,tp4]       | time           '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--------------------------------------------------------------------------------------------------------------------\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-73f9c5145259>\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#data_parallel(net,input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_hit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-552226336bcd>\u001b[0m in \u001b[0;36mcriterion\u001b[0;34m(logit, truth, weight)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('lr          iter   epoch |  loss    tn, [tp1,tp2,tp3,tp4]       |  loss    tn, [tp1,tp2,tp3,tp4]       | time           ')\n",
    "print('--------------------------------------------------------------------------------------------------------------------\\n')\n",
    "run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
