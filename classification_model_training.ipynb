{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heng's Starter code for training the classification model on Kaggle.**\n",
    "\n",
    "I have not run the kernel with GPU enabled because I do not have much of Kaggle GPU left as of now. So the kernel as expected is giving CUDA error.\n",
    "This is just a simple kernel for training model on Kaggle easily. Made some minor changes to his code and seems like it will run fine here on kaggle.\n",
    "I have not tested the training time. It can exceed the 9 hour limit.\n",
    "\n",
    "The kernel is based on Heng's starter kit version 20190910 you can find it [here](https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/106462#latest-645576) .\n",
    "I have imported 2 utility scripts one for the utility functions with plotting code and another one is for model.\n",
    "You can fork and edit the utility scripts and add the model classes as you feel like.\n",
    "The model architecture can be changed from this kernel below by changing the Net() class.\n",
    "\n",
    "If you face any problems or errors then feel free to comment them.\n",
    "At last thank you very much [Heng](https://www.kaggle.com/hengck23) and other leaderboard rankers for helping newbies like me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "\n",
    "from lib.utility_functions import *\n",
    "from lib.models_all import *\n",
    "from lib.rate import *\n",
    "\n",
    "PI = np.pi\n",
    "IMAGE_RGB_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGE_RGB_STD  = [0.229, 0.224, 0.225]\n",
    "DEFECT_COLOR = [(0,0,0),(0,0,255),(0,255,0),(255,0,0),(0,255,255)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_DIR = 'data/split'\n",
    "DATA_DIR = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def load_pretrain(self, skip=['logit.'], is_print=True):\n",
    "        load_pretrain(self, skip, pretrain_file=PRETRAIN_FILE, conversion=CONVERSION, is_print=is_print)\n",
    "\n",
    "    def __init__(self, num_class=4):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        e = ResNext50()\n",
    "        self.block0 = e.block0\n",
    "        self.block1 = e.block1\n",
    "        self.block2 = e.block2\n",
    "        self.block3 = e.block3\n",
    "        self.block4 = e.block4\n",
    "        e = None  #dropped\n",
    "\n",
    "        self.feature = nn.Conv2d(2048, 64, kernel_size=1) #dummy conv for dim reduction\n",
    "        self.logit   = nn.Conv2d(64, num_class, kernel_size=1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size,C,H,W = x.shape\n",
    "        x = x.clone()\n",
    "        x = x-torch.FloatTensor(IMAGE_RGB_MEAN).to(x.device).view(1,-1,1,1)\n",
    "        x = x/torch.FloatTensor(IMAGE_RGB_STD).to(x.device).view(1,-1,1,1)\n",
    "\n",
    "        x = self.block0(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "\n",
    "        x = F.dropout(x,0.5,training=self.training)\n",
    "        x = F.avg_pool2d(x, kernel_size=(8, 13),stride=(8, 8))\n",
    "        #x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = self.feature(x)\n",
    "\n",
    "        logit = self.logit(x) #.view(batch_size,-1)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class which is used by the infor object in __get_item__\n",
    "class Struct(object):\n",
    "    def __init__(self, is_copy=False, **kwargs):\n",
    "        self.add(is_copy, **kwargs)\n",
    "\n",
    "    def add(self, is_copy=False, **kwargs):\n",
    "        #self.__dict__.update(kwargs)\n",
    "\n",
    "        if is_copy == False:\n",
    "            for key, value in kwargs.items():\n",
    "                setattr(self, key, value)\n",
    "        else:\n",
    "            for key, value in kwargs.items():\n",
    "                try:\n",
    "                    setattr(self, key, copy.deepcopy(value))\n",
    "                    #setattr(self, key, value.copy())\n",
    "                except Exception:\n",
    "                    setattr(self, key, value)\n",
    "\n",
    "    def __str__(self):\n",
    "        text =''\n",
    "        for k,v in self.__dict__.items():\n",
    "            text += '\\t%s : %s\\n'%(k, str(v))\n",
    "        return text\n",
    "\n",
    "# Creating masks\n",
    "def run_length_decode(rle, height=256, width=1600, fill_value=1):\n",
    "    mask = np.zeros((height,width), np.float32)\n",
    "    if rle != '':\n",
    "        mask=mask.reshape(-1)\n",
    "        r = [int(r) for r in rle.split(' ')]\n",
    "        r = np.array(r).reshape(-1, 2)\n",
    "        for start,length in r:\n",
    "            start = start-1  #???? 0 or 1 index ???\n",
    "            mask[start:(start + length)] = fill_value\n",
    "        mask=mask.reshape(width, height).T\n",
    "    return mask\n",
    "\n",
    "# Collations\n",
    "def null_collate(batch):\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    input = []\n",
    "    truth_mask  = []\n",
    "    truth_label = []\n",
    "    infor = []\n",
    "    for b in range(batch_size):\n",
    "        input.append(batch[b][0])\n",
    "        truth_mask.append(batch[b][1])\n",
    "        infor.append(batch[b][2])\n",
    "\n",
    "        label = (batch[b][1].reshape(4,-1).sum(1)>8).astype(np.int32)\n",
    "        truth_label.append(label)\n",
    "\n",
    "\n",
    "    input = np.stack(input)\n",
    "    input = image_to_input(input, IMAGE_RGB_MEAN,IMAGE_RGB_STD)\n",
    "    input = torch.from_numpy(input).float()\n",
    "\n",
    "    truth_mask = np.stack(truth_mask)\n",
    "    truth_mask = (truth_mask>0.5).astype(np.float32)\n",
    "    truth_mask = torch.from_numpy(truth_mask).float()\n",
    "\n",
    "    truth_label = np.array(truth_label)\n",
    "    truth_label = torch.from_numpy(truth_label).float()\n",
    "\n",
    "    return input, truth_mask, truth_label, infor\n",
    "\n",
    "# Metric\n",
    "def metric_hit(logit, truth, threshold=0.5):\n",
    "    batch_size,num_class, H,W = logit.shape\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logit = logit.view(batch_size,num_class,-1)\n",
    "        truth = truth.view(batch_size,num_class,-1)\n",
    "\n",
    "        probability = torch.sigmoid(logit)\n",
    "        p = (probability>threshold).float()\n",
    "        t = (truth>0.5).float()\n",
    "\n",
    "        tp = ((p + t) == 2).float()  # True positives\n",
    "        tn = ((p + t) == 0).float()  # True negatives\n",
    "\n",
    "        tp = tp.sum(dim=[0,2])\n",
    "        tn = tn.sum(dim=[0,2])\n",
    "        num_pos = t.sum(dim=[0,2])\n",
    "        num_neg = batch_size*H*W - num_pos\n",
    "\n",
    "        tp = tp.data.cpu().numpy()\n",
    "        tn = tn.data.cpu().numpy().sum()\n",
    "        num_pos = num_pos.data.cpu().numpy()\n",
    "        num_neg = num_neg.data.cpu().numpy().sum()\n",
    "\n",
    "        tp = np.nan_to_num(tp/(num_pos+1e-12),0)\n",
    "        tn = np.nan_to_num(tn/(num_neg+1e-12),0)\n",
    "\n",
    "        tp = list(tp)\n",
    "        num_pos = list(num_pos)\n",
    "\n",
    "    return tn,tp, num_neg,num_pos\n",
    "\n",
    "# Loss\n",
    "#def criterion(logit, truth, weight=None):\n",
    "#    batch_size,num_class, H,W = logit.shape\n",
    "#    logit = logit.view(batch_size,num_class)\n",
    "#    truth = truth.view(batch_size,num_class)\n",
    "#    assert(logit.shape==truth.shape)\n",
    "#\n",
    "#    loss = F.binary_cross_entropy_with_logits(logit, truth, reduction='none')\n",
    "#\n",
    "#    if weight is None:\n",
    "#        loss = loss.mean()\n",
    "#\n",
    "#    else:\n",
    "#        pos = (truth>0.5).float()\n",
    "#        neg = (truth<0.5).float()\n",
    "#        pos_sum = pos.sum().item() + 1e-12\n",
    "#        neg_sum = neg.sum().item() + 1e-12\n",
    "#        loss = (weight[1]*pos*loss/pos_sum + weight[0]*neg*loss/neg_sum).sum()\n",
    "#        #raise NotImplementedError\n",
    "#\n",
    "#    return loss\n",
    "\n",
    "\n",
    "def criterion(logit, truth, weight=None):\n",
    "    batch_size,num_class = logit.shape[:2]\n",
    "    logit = logit.view(batch_size,num_class)\n",
    "    truth = truth.view(batch_size,num_class)\n",
    "\n",
    "    if weight is None: weight=[1,1,1,1]\n",
    "    weight = torch.FloatTensor(weight).to(truth.device).view(1,-1)\n",
    "\n",
    "    loss = F.binary_cross_entropy_with_logits(logit, truth, reduction='none')\n",
    "\n",
    "    loss = loss*weight\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n",
    "# Learning Rate Adjustments\n",
    "def adjust_learning_rate(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def get_learning_rate(optimizer):\n",
    "    lr=[]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr += [param_group['lr']]\n",
    "\n",
    "    assert(len(lr)==1) #we support only one param_group\n",
    "    lr = lr[0]\n",
    "    return lr\n",
    "\n",
    "# Learning Rate Schedule\n",
    "class NullScheduler():\n",
    "    def __init__(self, lr=0.01 ):\n",
    "        super(NullScheduler, self).__init__()\n",
    "        self.lr    = lr\n",
    "        self.cycle = 0\n",
    "\n",
    "    def __call__(self, time):\n",
    "        return self.lr\n",
    "\n",
    "    def __str__(self):\n",
    "        string = 'NullScheduler\\n' \\\n",
    "                + 'lr=%0.5f '%(self.lr)\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteelDataset(Dataset):\n",
    "    def __init__(self, split, csv, mode, augment=None):\n",
    "        self.split   = split\n",
    "        self.csv     = csv\n",
    "        self.mode    = mode\n",
    "        self.augment = augment\n",
    "\n",
    "        self.uid = list(np.concatenate([np.load(SPLIT_DIR + '/%s'%f , allow_pickle=True) for f in split]))\n",
    "        df = pd.concat([pd.read_csv(DATA_DIR + '/%s'%f) for f in csv])\n",
    "        df.fillna('', inplace=True)\n",
    "        df['Class'] = df['ImageId_ClassId'].str[-1].astype(np.int32)\n",
    "        df['Label'] = (df['EncodedPixels']!='').astype(np.int32)\n",
    "        df = df_loc_by_list(df, 'ImageId_ClassId', [ u.split('/')[-1] + '_%d'%c  for u in self.uid for c in [1,2,3,4] ])\n",
    "        self.df = df\n",
    "\n",
    "    def __str__(self):\n",
    "        num1 = (self.df['Class']==1).sum()\n",
    "        num2 = (self.df['Class']==2).sum()\n",
    "        num3 = (self.df['Class']==3).sum()\n",
    "        num4 = (self.df['Class']==4).sum()\n",
    "        pos1 = ((self.df['Class']==1) & (self.df['Label']==1)).sum()\n",
    "        pos2 = ((self.df['Class']==2) & (self.df['Label']==1)).sum()\n",
    "        pos3 = ((self.df['Class']==3) & (self.df['Label']==1)).sum()\n",
    "        pos4 = ((self.df['Class']==4) & (self.df['Label']==1)).sum()\n",
    "\n",
    "        length = len(self)\n",
    "        num = len(self)*4\n",
    "        pos = (self.df['Label']==1).sum()\n",
    "        neg = num-pos\n",
    "\n",
    "        string  = ''\n",
    "        string += '\\tmode    = %s\\n'%self.mode\n",
    "        string += '\\tsplit   = %s\\n'%self.split\n",
    "        string += '\\tcsv     = %s\\n'%str(self.csv)\n",
    "        string += '\\t\\tlen   = %5d\\n'%len(self)\n",
    "        if self.mode == 'train':\n",
    "            string += '\\t\\tnum   = %5d\\n'%num\n",
    "            string += '\\t\\tneg   = %5d  %0.3f\\n'%(neg,neg/num)\n",
    "            string += '\\t\\tpos   = %5d  %0.3f\\n'%(pos,pos/num)\n",
    "            string += '\\t\\tpos1  = %5d  %0.3f  %0.3f\\n'%(pos1,pos1/length,pos1/pos)\n",
    "            string += '\\t\\tpos2  = %5d  %0.3f  %0.3f\\n'%(pos2,pos2/length,pos2/pos)\n",
    "            string += '\\t\\tpos3  = %5d  %0.3f  %0.3f\\n'%(pos3,pos3/length,pos3/pos)\n",
    "            string += '\\t\\tpos4  = %5d  %0.3f  %0.3f\\n'%(pos4,pos4/length,pos4/pos)\n",
    "        return string\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.uid)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        folder, image_id = self.uid[index].split('/')\n",
    "        rle = [\n",
    "            self.df.loc[self.df['ImageId_ClassId']==image_id + '_1','EncodedPixels'].values[0],\n",
    "            self.df.loc[self.df['ImageId_ClassId']==image_id + '_2','EncodedPixels'].values[0],\n",
    "            self.df.loc[self.df['ImageId_ClassId']==image_id + '_3','EncodedPixels'].values[0],\n",
    "            self.df.loc[self.df['ImageId_ClassId']==image_id + '_4','EncodedPixels'].values[0],\n",
    "        ]\n",
    "        \n",
    "        image = cv2.imread(DATA_DIR + '/%s/%s'%(folder,image_id), cv2.IMREAD_COLOR)\n",
    "        mask  = np.array([run_length_decode(r, height=256, width=1600, fill_value=1) for r in rle])\n",
    "\n",
    "        infor = Struct(\n",
    "            index    = index,\n",
    "            folder   = folder,\n",
    "            image_id = image_id,\n",
    "        )\n",
    "\n",
    "        if self.augment is None:\n",
    "            return image, mask, infor\n",
    "        else:\n",
    "            return self.augment(image, mask, infor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiveBalanceClassSampler(Sampler):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        label = (self.dataset.df['Label'].values)\n",
    "        \n",
    "        #cannot reshape array of size 49155 into shape (4)\n",
    "        label = label.reshape(-1,4)\n",
    "        label = np.hstack([label.sum(1,keepdims=True)==0,label]).T\n",
    "\n",
    "        self.neg_index  = np.where(label[0])[0]\n",
    "        self.pos1_index = np.where(label[1])[0]\n",
    "        self.pos2_index = np.where(label[2])[0]\n",
    "        self.pos3_index = np.where(label[3])[0]\n",
    "        self.pos4_index = np.where(label[4])[0]\n",
    "\n",
    "        #5x\n",
    "        self.num_image = len(self.dataset.df)//4\n",
    "        self.length = self.num_image*5\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        neg  = np.random.choice(self.neg_index,  self.num_image, replace=True)\n",
    "        pos1 = np.random.choice(self.pos1_index, self.num_image, replace=True)\n",
    "        pos2 = np.random.choice(self.pos2_index, self.num_image, replace=True)\n",
    "        pos3 = np.random.choice(self.pos3_index, self.num_image, replace=True)\n",
    "        pos4 = np.random.choice(self.pos4_index, self.num_image, replace=True)\n",
    "\n",
    "        l = np.stack([neg,pos1,pos2,pos3,pos4]).T\n",
    "        l = l.reshape(-1)\n",
    "        return iter(l)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_valid(net, valid_loader, displays=None):\n",
    "    valid_num  = np.zeros(6, np.float32)\n",
    "    valid_loss = np.zeros(6, np.float32)\n",
    "    \n",
    "    for t, (input, truth_mask, truth_label, infor) in enumerate(valid_loader):\n",
    "\n",
    "        net.eval()\n",
    "        input = input.cuda()\n",
    "        truth_mask  = truth_mask.cuda()\n",
    "        truth_label = truth_label.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logit = net(input) #data_parallel(net, input)  \n",
    "            logit = logit.max(-1,True)[0]\n",
    "            loss  = criterion(logit, truth_label)\n",
    "            tn,tp, num_neg,num_pos = metric_hit(logit, truth_label)\n",
    "\n",
    "        batch_size = len(infor)\n",
    "        l = np.array([ loss.item(), tn,*tp])\n",
    "        n = np.array([ batch_size, num_neg,*num_pos])\n",
    "        valid_loss += l*n\n",
    "        valid_num  += n\n",
    "\n",
    "        if displays is not None:\n",
    "            probability = torch.sigmoid(logit)\n",
    "            image = input_to_image(input, IMAGE_RGB_MEAN,IMAGE_RGB_STD)\n",
    "\n",
    "            probability_label = probability.data.cpu().numpy()\n",
    "            truth_label = truth_label.data.cpu().numpy()\n",
    "            truth_mask  = truth_mask.data.cpu().numpy()\n",
    "\n",
    "            for b in range(0, batch_size, 4):\n",
    "                image_id = infor[b].image_id[:-4]\n",
    "                result = draw_predict_result_label(image[b], truth_mask[b], truth_label[b], probability_label[b], stack='vertical')\n",
    "                draw_shadow_text(result,'%05d    %s.jpg'%(valid_num[0]-batch_size+b, image_id),(5,24),0.75,[255,255,255],1)\n",
    "                image_show('result',result,resize=1)\n",
    "\n",
    "        print('\\r %8d /%8d'%(valid_num[0], len(valid_loader.dataset)),end='',flush=True)\n",
    "\n",
    "    assert(valid_num[0] == len(valid_loader.dataset))\n",
    "    valid_loss = valid_loss/valid_num\n",
    "\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERSION=[\n",
    " 'block0.0.weight',\t(64, 3, 7, 7),\t 'layer0.conv1.weight',\t(64, 3, 7, 7),\n",
    " 'block0.1.weight',\t(64,),\t 'layer0.bn1.weight',\t(64,),\n",
    " 'block0.1.bias',\t(64,),\t 'layer0.bn1.bias',\t(64,),\n",
    " 'block0.1.running_mean',\t(64,),\t 'layer0.bn1.running_mean',\t(64,),\n",
    " 'block0.1.running_var',\t(64,),\t 'layer0.bn1.running_var',\t(64,),\n",
    " 'block1.1.conv_bn1.conv.weight',\t(128, 64, 1, 1),\t 'layer1.0.conv1.weight',\t(128, 64, 1, 1),\n",
    " 'block1.1.conv_bn1.bn.weight',\t(128,),\t 'layer1.0.bn1.weight',\t(128,),\n",
    " 'block1.1.conv_bn1.bn.bias',\t(128,),\t 'layer1.0.bn1.bias',\t(128,),\n",
    " 'block1.1.conv_bn1.bn.running_mean',\t(128,),\t 'layer1.0.bn1.running_mean',\t(128,),\n",
    " 'block1.1.conv_bn1.bn.running_var',\t(128,),\t 'layer1.0.bn1.running_var',\t(128,),\n",
    " 'block1.1.conv_bn2.conv.weight',\t(128, 4, 3, 3),\t 'layer1.0.conv2.weight',\t(128, 4, 3, 3),\n",
    " 'block1.1.conv_bn2.bn.weight',\t(128,),\t 'layer1.0.bn2.weight',\t(128,),\n",
    " 'block1.1.conv_bn2.bn.bias',\t(128,),\t 'layer1.0.bn2.bias',\t(128,),\n",
    " 'block1.1.conv_bn2.bn.running_mean',\t(128,),\t 'layer1.0.bn2.running_mean',\t(128,),\n",
    " 'block1.1.conv_bn2.bn.running_var',\t(128,),\t 'layer1.0.bn2.running_var',\t(128,),\n",
    " 'block1.1.conv_bn3.conv.weight',\t(256, 128, 1, 1),\t 'layer1.0.conv3.weight',\t(256, 128, 1, 1),\n",
    " 'block1.1.conv_bn3.bn.weight',\t(256,),\t 'layer1.0.bn3.weight',\t(256,),\n",
    " 'block1.1.conv_bn3.bn.bias',\t(256,),\t 'layer1.0.bn3.bias',\t(256,),\n",
    " 'block1.1.conv_bn3.bn.running_mean',\t(256,),\t 'layer1.0.bn3.running_mean',\t(256,),\n",
    " 'block1.1.conv_bn3.bn.running_var',\t(256,),\t 'layer1.0.bn3.running_var',\t(256,),\n",
    " 'block1.1.scale.fc1.weight',\t(16, 256, 1, 1),\t 'layer1.0.se_module.fc1.weight',\t(16, 256, 1, 1),\n",
    " 'block1.1.scale.fc1.bias',\t(16,),\t 'layer1.0.se_module.fc1.bias',\t(16,),\n",
    " 'block1.1.scale.fc2.weight',\t(256, 16, 1, 1),\t 'layer1.0.se_module.fc2.weight',\t(256, 16, 1, 1),\n",
    " 'block1.1.scale.fc2.bias',\t(256,),\t 'layer1.0.se_module.fc2.bias',\t(256,),\n",
    " 'block1.1.shortcut.conv.weight',\t(256, 64, 1, 1),\t 'layer1.0.downsample.0.weight',\t(256, 64, 1, 1),\n",
    " 'block1.1.shortcut.bn.weight',\t(256,),\t 'layer1.0.downsample.1.weight',\t(256,),\n",
    " 'block1.1.shortcut.bn.bias',\t(256,),\t 'layer1.0.downsample.1.bias',\t(256,),\n",
    " 'block1.1.shortcut.bn.running_mean',\t(256,),\t 'layer1.0.downsample.1.running_mean',\t(256,),\n",
    " 'block1.1.shortcut.bn.running_var',\t(256,),\t 'layer1.0.downsample.1.running_var',\t(256,),\n",
    " 'block1.2.conv_bn1.conv.weight',\t(128, 256, 1, 1),\t 'layer1.1.conv1.weight',\t(128, 256, 1, 1),\n",
    " 'block1.2.conv_bn1.bn.weight',\t(128,),\t 'layer1.1.bn1.weight',\t(128,),\n",
    " 'block1.2.conv_bn1.bn.bias',\t(128,),\t 'layer1.1.bn1.bias',\t(128,),\n",
    " 'block1.2.conv_bn1.bn.running_mean',\t(128,),\t 'layer1.1.bn1.running_mean',\t(128,),\n",
    " 'block1.2.conv_bn1.bn.running_var',\t(128,),\t 'layer1.1.bn1.running_var',\t(128,),\n",
    " 'block1.2.conv_bn2.conv.weight',\t(128, 4, 3, 3),\t 'layer1.1.conv2.weight',\t(128, 4, 3, 3),\n",
    " 'block1.2.conv_bn2.bn.weight',\t(128,),\t 'layer1.1.bn2.weight',\t(128,),\n",
    " 'block1.2.conv_bn2.bn.bias',\t(128,),\t 'layer1.1.bn2.bias',\t(128,),\n",
    " 'block1.2.conv_bn2.bn.running_mean',\t(128,),\t 'layer1.1.bn2.running_mean',\t(128,),\n",
    " 'block1.2.conv_bn2.bn.running_var',\t(128,),\t 'layer1.1.bn2.running_var',\t(128,),\n",
    " 'block1.2.conv_bn3.conv.weight',\t(256, 128, 1, 1),\t 'layer1.1.conv3.weight',\t(256, 128, 1, 1),\n",
    " 'block1.2.conv_bn3.bn.weight',\t(256,),\t 'layer1.1.bn3.weight',\t(256,),\n",
    " 'block1.2.conv_bn3.bn.bias',\t(256,),\t 'layer1.1.bn3.bias',\t(256,),\n",
    " 'block1.2.conv_bn3.bn.running_mean',\t(256,),\t 'layer1.1.bn3.running_mean',\t(256,),\n",
    " 'block1.2.conv_bn3.bn.running_var',\t(256,),\t 'layer1.1.bn3.running_var',\t(256,),\n",
    " 'block1.2.scale.fc1.weight',\t(16, 256, 1, 1),\t 'layer1.1.se_module.fc1.weight',\t(16, 256, 1, 1),\n",
    " 'block1.2.scale.fc1.bias',\t(16,),\t 'layer1.1.se_module.fc1.bias',\t(16,),\n",
    " 'block1.2.scale.fc2.weight',\t(256, 16, 1, 1),\t 'layer1.1.se_module.fc2.weight',\t(256, 16, 1, 1),\n",
    " 'block1.2.scale.fc2.bias',\t(256,),\t 'layer1.1.se_module.fc2.bias',\t(256,),\n",
    " 'block1.3.conv_bn1.conv.weight',\t(128, 256, 1, 1),\t 'layer1.2.conv1.weight',\t(128, 256, 1, 1),\n",
    " 'block1.3.conv_bn1.bn.weight',\t(128,),\t 'layer1.2.bn1.weight',\t(128,),\n",
    " 'block1.3.conv_bn1.bn.bias',\t(128,),\t 'layer1.2.bn1.bias',\t(128,),\n",
    " 'block1.3.conv_bn1.bn.running_mean',\t(128,),\t 'layer1.2.bn1.running_mean',\t(128,),\n",
    " 'block1.3.conv_bn1.bn.running_var',\t(128,),\t 'layer1.2.bn1.running_var',\t(128,),\n",
    " 'block1.3.conv_bn2.conv.weight',\t(128, 4, 3, 3),\t 'layer1.2.conv2.weight',\t(128, 4, 3, 3),\n",
    " 'block1.3.conv_bn2.bn.weight',\t(128,),\t 'layer1.2.bn2.weight',\t(128,),\n",
    " 'block1.3.conv_bn2.bn.bias',\t(128,),\t 'layer1.2.bn2.bias',\t(128,),\n",
    " 'block1.3.conv_bn2.bn.running_mean',\t(128,),\t 'layer1.2.bn2.running_mean',\t(128,),\n",
    " 'block1.3.conv_bn2.bn.running_var',\t(128,),\t 'layer1.2.bn2.running_var',\t(128,),\n",
    " 'block1.3.conv_bn3.conv.weight',\t(256, 128, 1, 1),\t 'layer1.2.conv3.weight',\t(256, 128, 1, 1),\n",
    " 'block1.3.conv_bn3.bn.weight',\t(256,),\t 'layer1.2.bn3.weight',\t(256,),\n",
    " 'block1.3.conv_bn3.bn.bias',\t(256,),\t 'layer1.2.bn3.bias',\t(256,),\n",
    " 'block1.3.conv_bn3.bn.running_mean',\t(256,),\t 'layer1.2.bn3.running_mean',\t(256,),\n",
    " 'block1.3.conv_bn3.bn.running_var',\t(256,),\t 'layer1.2.bn3.running_var',\t(256,),\n",
    " 'block1.3.scale.fc1.weight',\t(16, 256, 1, 1),\t 'layer1.2.se_module.fc1.weight',\t(16, 256, 1, 1),\n",
    " 'block1.3.scale.fc1.bias',\t(16,),\t 'layer1.2.se_module.fc1.bias',\t(16,),\n",
    " 'block1.3.scale.fc2.weight',\t(256, 16, 1, 1),\t 'layer1.2.se_module.fc2.weight',\t(256, 16, 1, 1),\n",
    " 'block1.3.scale.fc2.bias',\t(256,),\t 'layer1.2.se_module.fc2.bias',\t(256,),\n",
    " 'block2.0.conv_bn1.conv.weight',\t(256, 256, 1, 1),\t 'layer2.0.conv1.weight',\t(256, 256, 1, 1),\n",
    " 'block2.0.conv_bn1.bn.weight',\t(256,),\t 'layer2.0.bn1.weight',\t(256,),\n",
    " 'block2.0.conv_bn1.bn.bias',\t(256,),\t 'layer2.0.bn1.bias',\t(256,),\n",
    " 'block2.0.conv_bn1.bn.running_mean',\t(256,),\t 'layer2.0.bn1.running_mean',\t(256,),\n",
    " 'block2.0.conv_bn1.bn.running_var',\t(256,),\t 'layer2.0.bn1.running_var',\t(256,),\n",
    " 'block2.0.conv_bn2.conv.weight',\t(256, 8, 3, 3),\t 'layer2.0.conv2.weight',\t(256, 8, 3, 3),\n",
    " 'block2.0.conv_bn2.bn.weight',\t(256,),\t 'layer2.0.bn2.weight',\t(256,),\n",
    " 'block2.0.conv_bn2.bn.bias',\t(256,),\t 'layer2.0.bn2.bias',\t(256,),\n",
    " 'block2.0.conv_bn2.bn.running_mean',\t(256,),\t 'layer2.0.bn2.running_mean',\t(256,),\n",
    " 'block2.0.conv_bn2.bn.running_var',\t(256,),\t 'layer2.0.bn2.running_var',\t(256,),\n",
    " 'block2.0.conv_bn3.conv.weight',\t(512, 256, 1, 1),\t 'layer2.0.conv3.weight',\t(512, 256, 1, 1),\n",
    " 'block2.0.conv_bn3.bn.weight',\t(512,),\t 'layer2.0.bn3.weight',\t(512,),\n",
    " 'block2.0.conv_bn3.bn.bias',\t(512,),\t 'layer2.0.bn3.bias',\t(512,),\n",
    " 'block2.0.conv_bn3.bn.running_mean',\t(512,),\t 'layer2.0.bn3.running_mean',\t(512,),\n",
    " 'block2.0.conv_bn3.bn.running_var',\t(512,),\t 'layer2.0.bn3.running_var',\t(512,),\n",
    " 'block2.0.scale.fc1.weight',\t(32, 512, 1, 1),\t 'layer2.0.se_module.fc1.weight',\t(32, 512, 1, 1),\n",
    " 'block2.0.scale.fc1.bias',\t(32,),\t 'layer2.0.se_module.fc1.bias',\t(32,),\n",
    " 'block2.0.scale.fc2.weight',\t(512, 32, 1, 1),\t 'layer2.0.se_module.fc2.weight',\t(512, 32, 1, 1),\n",
    " 'block2.0.scale.fc2.bias',\t(512,),\t 'layer2.0.se_module.fc2.bias',\t(512,),\n",
    " 'block2.0.shortcut.conv.weight',\t(512, 256, 1, 1),\t 'layer2.0.downsample.0.weight',\t(512, 256, 1, 1),\n",
    " 'block2.0.shortcut.bn.weight',\t(512,),\t 'layer2.0.downsample.1.weight',\t(512,),\n",
    " 'block2.0.shortcut.bn.bias',\t(512,),\t 'layer2.0.downsample.1.bias',\t(512,),\n",
    " 'block2.0.shortcut.bn.running_mean',\t(512,),\t 'layer2.0.downsample.1.running_mean',\t(512,),\n",
    " 'block2.0.shortcut.bn.running_var',\t(512,),\t 'layer2.0.downsample.1.running_var',\t(512,),\n",
    " 'block2.1.conv_bn1.conv.weight',\t(256, 512, 1, 1),\t 'layer2.1.conv1.weight',\t(256, 512, 1, 1),\n",
    " 'block2.1.conv_bn1.bn.weight',\t(256,),\t 'layer2.1.bn1.weight',\t(256,),\n",
    " 'block2.1.conv_bn1.bn.bias',\t(256,),\t 'layer2.1.bn1.bias',\t(256,),\n",
    " 'block2.1.conv_bn1.bn.running_mean',\t(256,),\t 'layer2.1.bn1.running_mean',\t(256,),\n",
    " 'block2.1.conv_bn1.bn.running_var',\t(256,),\t 'layer2.1.bn1.running_var',\t(256,),\n",
    " 'block2.1.conv_bn2.conv.weight',\t(256, 8, 3, 3),\t 'layer2.1.conv2.weight',\t(256, 8, 3, 3),\n",
    " 'block2.1.conv_bn2.bn.weight',\t(256,),\t 'layer2.1.bn2.weight',\t(256,),\n",
    " 'block2.1.conv_bn2.bn.bias',\t(256,),\t 'layer2.1.bn2.bias',\t(256,),\n",
    " 'block2.1.conv_bn2.bn.running_mean',\t(256,),\t 'layer2.1.bn2.running_mean',\t(256,),\n",
    " 'block2.1.conv_bn2.bn.running_var',\t(256,),\t 'layer2.1.bn2.running_var',\t(256,),\n",
    " 'block2.1.conv_bn3.conv.weight',\t(512, 256, 1, 1),\t 'layer2.1.conv3.weight',\t(512, 256, 1, 1),\n",
    " 'block2.1.conv_bn3.bn.weight',\t(512,),\t 'layer2.1.bn3.weight',\t(512,),\n",
    " 'block2.1.conv_bn3.bn.bias',\t(512,),\t 'layer2.1.bn3.bias',\t(512,),\n",
    " 'block2.1.conv_bn3.bn.running_mean',\t(512,),\t 'layer2.1.bn3.running_mean',\t(512,),\n",
    " 'block2.1.conv_bn3.bn.running_var',\t(512,),\t 'layer2.1.bn3.running_var',\t(512,),\n",
    " 'block2.1.scale.fc1.weight',\t(32, 512, 1, 1),\t 'layer2.1.se_module.fc1.weight',\t(32, 512, 1, 1),\n",
    " 'block2.1.scale.fc1.bias',\t(32,),\t 'layer2.1.se_module.fc1.bias',\t(32,),\n",
    " 'block2.1.scale.fc2.weight',\t(512, 32, 1, 1),\t 'layer2.1.se_module.fc2.weight',\t(512, 32, 1, 1),\n",
    " 'block2.1.scale.fc2.bias',\t(512,),\t 'layer2.1.se_module.fc2.bias',\t(512,),\n",
    " 'block2.2.conv_bn1.conv.weight',\t(256, 512, 1, 1),\t 'layer2.2.conv1.weight',\t(256, 512, 1, 1),\n",
    " 'block2.2.conv_bn1.bn.weight',\t(256,),\t 'layer2.2.bn1.weight',\t(256,),\n",
    " 'block2.2.conv_bn1.bn.bias',\t(256,),\t 'layer2.2.bn1.bias',\t(256,),\n",
    " 'block2.2.conv_bn1.bn.running_mean',\t(256,),\t 'layer2.2.bn1.running_mean',\t(256,),\n",
    " 'block2.2.conv_bn1.bn.running_var',\t(256,),\t 'layer2.2.bn1.running_var',\t(256,),\n",
    " 'block2.2.conv_bn2.conv.weight',\t(256, 8, 3, 3),\t 'layer2.2.conv2.weight',\t(256, 8, 3, 3),\n",
    " 'block2.2.conv_bn2.bn.weight',\t(256,),\t 'layer2.2.bn2.weight',\t(256,),\n",
    " 'block2.2.conv_bn2.bn.bias',\t(256,),\t 'layer2.2.bn2.bias',\t(256,),\n",
    " 'block2.2.conv_bn2.bn.running_mean',\t(256,),\t 'layer2.2.bn2.running_mean',\t(256,),\n",
    " 'block2.2.conv_bn2.bn.running_var',\t(256,),\t 'layer2.2.bn2.running_var',\t(256,),\n",
    " 'block2.2.conv_bn3.conv.weight',\t(512, 256, 1, 1),\t 'layer2.2.conv3.weight',\t(512, 256, 1, 1),\n",
    " 'block2.2.conv_bn3.bn.weight',\t(512,),\t 'layer2.2.bn3.weight',\t(512,),\n",
    " 'block2.2.conv_bn3.bn.bias',\t(512,),\t 'layer2.2.bn3.bias',\t(512,),\n",
    " 'block2.2.conv_bn3.bn.running_mean',\t(512,),\t 'layer2.2.bn3.running_mean',\t(512,),\n",
    " 'block2.2.conv_bn3.bn.running_var',\t(512,),\t 'layer2.2.bn3.running_var',\t(512,),\n",
    " 'block2.2.scale.fc1.weight',\t(32, 512, 1, 1),\t 'layer2.2.se_module.fc1.weight',\t(32, 512, 1, 1),\n",
    " 'block2.2.scale.fc1.bias',\t(32,),\t 'layer2.2.se_module.fc1.bias',\t(32,),\n",
    " 'block2.2.scale.fc2.weight',\t(512, 32, 1, 1),\t 'layer2.2.se_module.fc2.weight',\t(512, 32, 1, 1),\n",
    " 'block2.2.scale.fc2.bias',\t(512,),\t 'layer2.2.se_module.fc2.bias',\t(512,),\n",
    " 'block2.3.conv_bn1.conv.weight',\t(256, 512, 1, 1),\t 'layer2.3.conv1.weight',\t(256, 512, 1, 1),\n",
    " 'block2.3.conv_bn1.bn.weight',\t(256,),\t 'layer2.3.bn1.weight',\t(256,),\n",
    " 'block2.3.conv_bn1.bn.bias',\t(256,),\t 'layer2.3.bn1.bias',\t(256,),\n",
    " 'block2.3.conv_bn1.bn.running_mean',\t(256,),\t 'layer2.3.bn1.running_mean',\t(256,),\n",
    " 'block2.3.conv_bn1.bn.running_var',\t(256,),\t 'layer2.3.bn1.running_var',\t(256,),\n",
    " 'block2.3.conv_bn2.conv.weight',\t(256, 8, 3, 3),\t 'layer2.3.conv2.weight',\t(256, 8, 3, 3),\n",
    " 'block2.3.conv_bn2.bn.weight',\t(256,),\t 'layer2.3.bn2.weight',\t(256,),\n",
    " 'block2.3.conv_bn2.bn.bias',\t(256,),\t 'layer2.3.bn2.bias',\t(256,),\n",
    " 'block2.3.conv_bn2.bn.running_mean',\t(256,),\t 'layer2.3.bn2.running_mean',\t(256,),\n",
    " 'block2.3.conv_bn2.bn.running_var',\t(256,),\t 'layer2.3.bn2.running_var',\t(256,),\n",
    " 'block2.3.conv_bn3.conv.weight',\t(512, 256, 1, 1),\t 'layer2.3.conv3.weight',\t(512, 256, 1, 1),\n",
    " 'block2.3.conv_bn3.bn.weight',\t(512,),\t 'layer2.3.bn3.weight',\t(512,),\n",
    " 'block2.3.conv_bn3.bn.bias',\t(512,),\t 'layer2.3.bn3.bias',\t(512,),\n",
    " 'block2.3.conv_bn3.bn.running_mean',\t(512,),\t 'layer2.3.bn3.running_mean',\t(512,),\n",
    " 'block2.3.conv_bn3.bn.running_var',\t(512,),\t 'layer2.3.bn3.running_var',\t(512,),\n",
    " 'block2.3.scale.fc1.weight',\t(32, 512, 1, 1),\t 'layer2.3.se_module.fc1.weight',\t(32, 512, 1, 1),\n",
    " 'block2.3.scale.fc1.bias',\t(32,),\t 'layer2.3.se_module.fc1.bias',\t(32,),\n",
    " 'block2.3.scale.fc2.weight',\t(512, 32, 1, 1),\t 'layer2.3.se_module.fc2.weight',\t(512, 32, 1, 1),\n",
    " 'block2.3.scale.fc2.bias',\t(512,),\t 'layer2.3.se_module.fc2.bias',\t(512,),\n",
    " 'block3.0.conv_bn1.conv.weight',\t(512, 512, 1, 1),\t 'layer3.0.conv1.weight',\t(512, 512, 1, 1),\n",
    " 'block3.0.conv_bn1.bn.weight',\t(512,),\t 'layer3.0.bn1.weight',\t(512,),\n",
    " 'block3.0.conv_bn1.bn.bias',\t(512,),\t 'layer3.0.bn1.bias',\t(512,),\n",
    " 'block3.0.conv_bn1.bn.running_mean',\t(512,),\t 'layer3.0.bn1.running_mean',\t(512,),\n",
    " 'block3.0.conv_bn1.bn.running_var',\t(512,),\t 'layer3.0.bn1.running_var',\t(512,),\n",
    " 'block3.0.conv_bn2.conv.weight',\t(512, 16, 3, 3),\t 'layer3.0.conv2.weight',\t(512, 16, 3, 3),\n",
    " 'block3.0.conv_bn2.bn.weight',\t(512,),\t 'layer3.0.bn2.weight',\t(512,),\n",
    " 'block3.0.conv_bn2.bn.bias',\t(512,),\t 'layer3.0.bn2.bias',\t(512,),\n",
    " 'block3.0.conv_bn2.bn.running_mean',\t(512,),\t 'layer3.0.bn2.running_mean',\t(512,),\n",
    " 'block3.0.conv_bn2.bn.running_var',\t(512,),\t 'layer3.0.bn2.running_var',\t(512,),\n",
    " 'block3.0.conv_bn3.conv.weight',\t(1024, 512, 1, 1),\t 'layer3.0.conv3.weight',\t(1024, 512, 1, 1),\n",
    " 'block3.0.conv_bn3.bn.weight',\t(1024,),\t 'layer3.0.bn3.weight',\t(1024,),\n",
    " 'block3.0.conv_bn3.bn.bias',\t(1024,),\t 'layer3.0.bn3.bias',\t(1024,),\n",
    " 'block3.0.conv_bn3.bn.running_mean',\t(1024,),\t 'layer3.0.bn3.running_mean',\t(1024,),\n",
    " 'block3.0.conv_bn3.bn.running_var',\t(1024,),\t 'layer3.0.bn3.running_var',\t(1024,),\n",
    " 'block3.0.scale.fc1.weight',\t(64, 1024, 1, 1),\t 'layer3.0.se_module.fc1.weight',\t(64, 1024, 1, 1),\n",
    " 'block3.0.scale.fc1.bias',\t(64,),\t 'layer3.0.se_module.fc1.bias',\t(64,),\n",
    " 'block3.0.scale.fc2.weight',\t(1024, 64, 1, 1),\t 'layer3.0.se_module.fc2.weight',\t(1024, 64, 1, 1),\n",
    " 'block3.0.scale.fc2.bias',\t(1024,),\t 'layer3.0.se_module.fc2.bias',\t(1024,),\n",
    " 'block3.0.shortcut.conv.weight',\t(1024, 512, 1, 1),\t 'layer3.0.downsample.0.weight',\t(1024, 512, 1, 1),\n",
    " 'block3.0.shortcut.bn.weight',\t(1024,),\t 'layer3.0.downsample.1.weight',\t(1024,),\n",
    " 'block3.0.shortcut.bn.bias',\t(1024,),\t 'layer3.0.downsample.1.bias',\t(1024,),\n",
    " 'block3.0.shortcut.bn.running_mean',\t(1024,),\t 'layer3.0.downsample.1.running_mean',\t(1024,),\n",
    " 'block3.0.shortcut.bn.running_var',\t(1024,),\t 'layer3.0.downsample.1.running_var',\t(1024,),\n",
    " 'block3.1.conv_bn1.conv.weight',\t(512, 1024, 1, 1),\t 'layer3.1.conv1.weight',\t(512, 1024, 1, 1),\n",
    " 'block3.1.conv_bn1.bn.weight',\t(512,),\t 'layer3.1.bn1.weight',\t(512,),\n",
    " 'block3.1.conv_bn1.bn.bias',\t(512,),\t 'layer3.1.bn1.bias',\t(512,),\n",
    " 'block3.1.conv_bn1.bn.running_mean',\t(512,),\t 'layer3.1.bn1.running_mean',\t(512,),\n",
    " 'block3.1.conv_bn1.bn.running_var',\t(512,),\t 'layer3.1.bn1.running_var',\t(512,),\n",
    " 'block3.1.conv_bn2.conv.weight',\t(512, 16, 3, 3),\t 'layer3.1.conv2.weight',\t(512, 16, 3, 3),\n",
    " 'block3.1.conv_bn2.bn.weight',\t(512,),\t 'layer3.1.bn2.weight',\t(512,),\n",
    " 'block3.1.conv_bn2.bn.bias',\t(512,),\t 'layer3.1.bn2.bias',\t(512,),\n",
    " 'block3.1.conv_bn2.bn.running_mean',\t(512,),\t 'layer3.1.bn2.running_mean',\t(512,),\n",
    " 'block3.1.conv_bn2.bn.running_var',\t(512,),\t 'layer3.1.bn2.running_var',\t(512,),\n",
    " 'block3.1.conv_bn3.conv.weight',\t(1024, 512, 1, 1),\t 'layer3.1.conv3.weight',\t(1024, 512, 1, 1),\n",
    " 'block3.1.conv_bn3.bn.weight',\t(1024,),\t 'layer3.1.bn3.weight',\t(1024,),\n",
    " 'block3.1.conv_bn3.bn.bias',\t(1024,),\t 'layer3.1.bn3.bias',\t(1024,),\n",
    " 'block3.1.conv_bn3.bn.running_mean',\t(1024,),\t 'layer3.1.bn3.running_mean',\t(1024,),\n",
    " 'block3.1.conv_bn3.bn.running_var',\t(1024,),\t 'layer3.1.bn3.running_var',\t(1024,),\n",
    " 'block3.1.scale.fc1.weight',\t(64, 1024, 1, 1),\t 'layer3.1.se_module.fc1.weight',\t(64, 1024, 1, 1),\n",
    " 'block3.1.scale.fc1.bias',\t(64,),\t 'layer3.1.se_module.fc1.bias',\t(64,),\n",
    " 'block3.1.scale.fc2.weight',\t(1024, 64, 1, 1),\t 'layer3.1.se_module.fc2.weight',\t(1024, 64, 1, 1),\n",
    " 'block3.1.scale.fc2.bias',\t(1024,),\t 'layer3.1.se_module.fc2.bias',\t(1024,),\n",
    " 'block3.2.conv_bn1.conv.weight',\t(512, 1024, 1, 1),\t 'layer3.2.conv1.weight',\t(512, 1024, 1, 1),\n",
    " 'block3.2.conv_bn1.bn.weight',\t(512,),\t 'layer3.2.bn1.weight',\t(512,),\n",
    " 'block3.2.conv_bn1.bn.bias',\t(512,),\t 'layer3.2.bn1.bias',\t(512,),\n",
    " 'block3.2.conv_bn1.bn.running_mean',\t(512,),\t 'layer3.2.bn1.running_mean',\t(512,),\n",
    " 'block3.2.conv_bn1.bn.running_var',\t(512,),\t 'layer3.2.bn1.running_var',\t(512,),\n",
    " 'block3.2.conv_bn2.conv.weight',\t(512, 16, 3, 3),\t 'layer3.2.conv2.weight',\t(512, 16, 3, 3),\n",
    " 'block3.2.conv_bn2.bn.weight',\t(512,),\t 'layer3.2.bn2.weight',\t(512,),\n",
    " 'block3.2.conv_bn2.bn.bias',\t(512,),\t 'layer3.2.bn2.bias',\t(512,),\n",
    " 'block3.2.conv_bn2.bn.running_mean',\t(512,),\t 'layer3.2.bn2.running_mean',\t(512,),\n",
    " 'block3.2.conv_bn2.bn.running_var',\t(512,),\t 'layer3.2.bn2.running_var',\t(512,),\n",
    " 'block3.2.conv_bn3.conv.weight',\t(1024, 512, 1, 1),\t 'layer3.2.conv3.weight',\t(1024, 512, 1, 1),\n",
    " 'block3.2.conv_bn3.bn.weight',\t(1024,),\t 'layer3.2.bn3.weight',\t(1024,),\n",
    " 'block3.2.conv_bn3.bn.bias',\t(1024,),\t 'layer3.2.bn3.bias',\t(1024,),\n",
    " 'block3.2.conv_bn3.bn.running_mean',\t(1024,),\t 'layer3.2.bn3.running_mean',\t(1024,),\n",
    " 'block3.2.conv_bn3.bn.running_var',\t(1024,),\t 'layer3.2.bn3.running_var',\t(1024,),\n",
    " 'block3.2.scale.fc1.weight',\t(64, 1024, 1, 1),\t 'layer3.2.se_module.fc1.weight',\t(64, 1024, 1, 1),\n",
    " 'block3.2.scale.fc1.bias',\t(64,),\t 'layer3.2.se_module.fc1.bias',\t(64,),\n",
    " 'block3.2.scale.fc2.weight',\t(1024, 64, 1, 1),\t 'layer3.2.se_module.fc2.weight',\t(1024, 64, 1, 1),\n",
    " 'block3.2.scale.fc2.bias',\t(1024,),\t 'layer3.2.se_module.fc2.bias',\t(1024,),\n",
    " 'block3.3.conv_bn1.conv.weight',\t(512, 1024, 1, 1),\t 'layer3.3.conv1.weight',\t(512, 1024, 1, 1),\n",
    " 'block3.3.conv_bn1.bn.weight',\t(512,),\t 'layer3.3.bn1.weight',\t(512,),\n",
    " 'block3.3.conv_bn1.bn.bias',\t(512,),\t 'layer3.3.bn1.bias',\t(512,),\n",
    " 'block3.3.conv_bn1.bn.running_mean',\t(512,),\t 'layer3.3.bn1.running_mean',\t(512,),\n",
    " 'block3.3.conv_bn1.bn.running_var',\t(512,),\t 'layer3.3.bn1.running_var',\t(512,),\n",
    " 'block3.3.conv_bn2.conv.weight',\t(512, 16, 3, 3),\t 'layer3.3.conv2.weight',\t(512, 16, 3, 3),\n",
    " 'block3.3.conv_bn2.bn.weight',\t(512,),\t 'layer3.3.bn2.weight',\t(512,),\n",
    " 'block3.3.conv_bn2.bn.bias',\t(512,),\t 'layer3.3.bn2.bias',\t(512,),\n",
    " 'block3.3.conv_bn2.bn.running_mean',\t(512,),\t 'layer3.3.bn2.running_mean',\t(512,),\n",
    " 'block3.3.conv_bn2.bn.running_var',\t(512,),\t 'layer3.3.bn2.running_var',\t(512,),\n",
    " 'block3.3.conv_bn3.conv.weight',\t(1024, 512, 1, 1),\t 'layer3.3.conv3.weight',\t(1024, 512, 1, 1),\n",
    " 'block3.3.conv_bn3.bn.weight',\t(1024,),\t 'layer3.3.bn3.weight',\t(1024,),\n",
    " 'block3.3.conv_bn3.bn.bias',\t(1024,),\t 'layer3.3.bn3.bias',\t(1024,),\n",
    " 'block3.3.conv_bn3.bn.running_mean',\t(1024,),\t 'layer3.3.bn3.running_mean',\t(1024,),\n",
    " 'block3.3.conv_bn3.bn.running_var',\t(1024,),\t 'layer3.3.bn3.running_var',\t(1024,),\n",
    " 'block3.3.scale.fc1.weight',\t(64, 1024, 1, 1),\t 'layer3.3.se_module.fc1.weight',\t(64, 1024, 1, 1),\n",
    " 'block3.3.scale.fc1.bias',\t(64,),\t 'layer3.3.se_module.fc1.bias',\t(64,),\n",
    " 'block3.3.scale.fc2.weight',\t(1024, 64, 1, 1),\t 'layer3.3.se_module.fc2.weight',\t(1024, 64, 1, 1),\n",
    " 'block3.3.scale.fc2.bias',\t(1024,),\t 'layer3.3.se_module.fc2.bias',\t(1024,),\n",
    " 'block3.4.conv_bn1.conv.weight',\t(512, 1024, 1, 1),\t 'layer3.4.conv1.weight',\t(512, 1024, 1, 1),\n",
    " 'block3.4.conv_bn1.bn.weight',\t(512,),\t 'layer3.4.bn1.weight',\t(512,),\n",
    " 'block3.4.conv_bn1.bn.bias',\t(512,),\t 'layer3.4.bn1.bias',\t(512,),\n",
    " 'block3.4.conv_bn1.bn.running_mean',\t(512,),\t 'layer3.4.bn1.running_mean',\t(512,),\n",
    " 'block3.4.conv_bn1.bn.running_var',\t(512,),\t 'layer3.4.bn1.running_var',\t(512,),\n",
    " 'block3.4.conv_bn2.conv.weight',\t(512, 16, 3, 3),\t 'layer3.4.conv2.weight',\t(512, 16, 3, 3),\n",
    " 'block3.4.conv_bn2.bn.weight',\t(512,),\t 'layer3.4.bn2.weight',\t(512,),\n",
    " 'block3.4.conv_bn2.bn.bias',\t(512,),\t 'layer3.4.bn2.bias',\t(512,),\n",
    " 'block3.4.conv_bn2.bn.running_mean',\t(512,),\t 'layer3.4.bn2.running_mean',\t(512,),\n",
    " 'block3.4.conv_bn2.bn.running_var',\t(512,),\t 'layer3.4.bn2.running_var',\t(512,),\n",
    " 'block3.4.conv_bn3.conv.weight',\t(1024, 512, 1, 1),\t 'layer3.4.conv3.weight',\t(1024, 512, 1, 1),\n",
    " 'block3.4.conv_bn3.bn.weight',\t(1024,),\t 'layer3.4.bn3.weight',\t(1024,),\n",
    " 'block3.4.conv_bn3.bn.bias',\t(1024,),\t 'layer3.4.bn3.bias',\t(1024,),\n",
    " 'block3.4.conv_bn3.bn.running_mean',\t(1024,),\t 'layer3.4.bn3.running_mean',\t(1024,),\n",
    " 'block3.4.conv_bn3.bn.running_var',\t(1024,),\t 'layer3.4.bn3.running_var',\t(1024,),\n",
    " 'block3.4.scale.fc1.weight',\t(64, 1024, 1, 1),\t 'layer3.4.se_module.fc1.weight',\t(64, 1024, 1, 1),\n",
    " 'block3.4.scale.fc1.bias',\t(64,),\t 'layer3.4.se_module.fc1.bias',\t(64,),\n",
    " 'block3.4.scale.fc2.weight',\t(1024, 64, 1, 1),\t 'layer3.4.se_module.fc2.weight',\t(1024, 64, 1, 1),\n",
    " 'block3.4.scale.fc2.bias',\t(1024,),\t 'layer3.4.se_module.fc2.bias',\t(1024,),\n",
    " 'block3.5.conv_bn1.conv.weight',\t(512, 1024, 1, 1),\t 'layer3.5.conv1.weight',\t(512, 1024, 1, 1),\n",
    " 'block3.5.conv_bn1.bn.weight',\t(512,),\t 'layer3.5.bn1.weight',\t(512,),\n",
    " 'block3.5.conv_bn1.bn.bias',\t(512,),\t 'layer3.5.bn1.bias',\t(512,),\n",
    " 'block3.5.conv_bn1.bn.running_mean',\t(512,),\t 'layer3.5.bn1.running_mean',\t(512,),\n",
    " 'block3.5.conv_bn1.bn.running_var',\t(512,),\t 'layer3.5.bn1.running_var',\t(512,),\n",
    " 'block3.5.conv_bn2.conv.weight',\t(512, 16, 3, 3),\t 'layer3.5.conv2.weight',\t(512, 16, 3, 3),\n",
    " 'block3.5.conv_bn2.bn.weight',\t(512,),\t 'layer3.5.bn2.weight',\t(512,),\n",
    " 'block3.5.conv_bn2.bn.bias',\t(512,),\t 'layer3.5.bn2.bias',\t(512,),\n",
    " 'block3.5.conv_bn2.bn.running_mean',\t(512,),\t 'layer3.5.bn2.running_mean',\t(512,),\n",
    " 'block3.5.conv_bn2.bn.running_var',\t(512,),\t 'layer3.5.bn2.running_var',\t(512,),\n",
    " 'block3.5.conv_bn3.conv.weight',\t(1024, 512, 1, 1),\t 'layer3.5.conv3.weight',\t(1024, 512, 1, 1),\n",
    " 'block3.5.conv_bn3.bn.weight',\t(1024,),\t 'layer3.5.bn3.weight',\t(1024,),\n",
    " 'block3.5.conv_bn3.bn.bias',\t(1024,),\t 'layer3.5.bn3.bias',\t(1024,),\n",
    " 'block3.5.conv_bn3.bn.running_mean',\t(1024,),\t 'layer3.5.bn3.running_mean',\t(1024,),\n",
    " 'block3.5.conv_bn3.bn.running_var',\t(1024,),\t 'layer3.5.bn3.running_var',\t(1024,),\n",
    " 'block3.5.scale.fc1.weight',\t(64, 1024, 1, 1),\t 'layer3.5.se_module.fc1.weight',\t(64, 1024, 1, 1),\n",
    " 'block3.5.scale.fc1.bias',\t(64,),\t 'layer3.5.se_module.fc1.bias',\t(64,),\n",
    " 'block3.5.scale.fc2.weight',\t(1024, 64, 1, 1),\t 'layer3.5.se_module.fc2.weight',\t(1024, 64, 1, 1),\n",
    " 'block3.5.scale.fc2.bias',\t(1024,),\t 'layer3.5.se_module.fc2.bias',\t(1024,),\n",
    " 'block4.0.conv_bn1.conv.weight',\t(1024, 1024, 1, 1),\t 'layer4.0.conv1.weight',\t(1024, 1024, 1, 1),\n",
    " 'block4.0.conv_bn1.bn.weight',\t(1024,),\t 'layer4.0.bn1.weight',\t(1024,),\n",
    " 'block4.0.conv_bn1.bn.bias',\t(1024,),\t 'layer4.0.bn1.bias',\t(1024,),\n",
    " 'block4.0.conv_bn1.bn.running_mean',\t(1024,),\t 'layer4.0.bn1.running_mean',\t(1024,),\n",
    " 'block4.0.conv_bn1.bn.running_var',\t(1024,),\t 'layer4.0.bn1.running_var',\t(1024,),\n",
    " 'block4.0.conv_bn2.conv.weight',\t(1024, 32, 3, 3),\t 'layer4.0.conv2.weight',\t(1024, 32, 3, 3),\n",
    " 'block4.0.conv_bn2.bn.weight',\t(1024,),\t 'layer4.0.bn2.weight',\t(1024,),\n",
    " 'block4.0.conv_bn2.bn.bias',\t(1024,),\t 'layer4.0.bn2.bias',\t(1024,),\n",
    " 'block4.0.conv_bn2.bn.running_mean',\t(1024,),\t 'layer4.0.bn2.running_mean',\t(1024,),\n",
    " 'block4.0.conv_bn2.bn.running_var',\t(1024,),\t 'layer4.0.bn2.running_var',\t(1024,),\n",
    " 'block4.0.conv_bn3.conv.weight',\t(2048, 1024, 1, 1),\t 'layer4.0.conv3.weight',\t(2048, 1024, 1, 1),\n",
    " 'block4.0.conv_bn3.bn.weight',\t(2048,),\t 'layer4.0.bn3.weight',\t(2048,),\n",
    " 'block4.0.conv_bn3.bn.bias',\t(2048,),\t 'layer4.0.bn3.bias',\t(2048,),\n",
    " 'block4.0.conv_bn3.bn.running_mean',\t(2048,),\t 'layer4.0.bn3.running_mean',\t(2048,),\n",
    " 'block4.0.conv_bn3.bn.running_var',\t(2048,),\t 'layer4.0.bn3.running_var',\t(2048,),\n",
    " 'block4.0.scale.fc1.weight',\t(128, 2048, 1, 1),\t 'layer4.0.se_module.fc1.weight',\t(128, 2048, 1, 1),\n",
    " 'block4.0.scale.fc1.bias',\t(128,),\t 'layer4.0.se_module.fc1.bias',\t(128,),\n",
    " 'block4.0.scale.fc2.weight',\t(2048, 128, 1, 1),\t 'layer4.0.se_module.fc2.weight',\t(2048, 128, 1, 1),\n",
    " 'block4.0.scale.fc2.bias',\t(2048,),\t 'layer4.0.se_module.fc2.bias',\t(2048,),\n",
    " 'block4.0.shortcut.conv.weight',\t(2048, 1024, 1, 1),\t 'layer4.0.downsample.0.weight',\t(2048, 1024, 1, 1),\n",
    " 'block4.0.shortcut.bn.weight',\t(2048,),\t 'layer4.0.downsample.1.weight',\t(2048,),\n",
    " 'block4.0.shortcut.bn.bias',\t(2048,),\t 'layer4.0.downsample.1.bias',\t(2048,),\n",
    " 'block4.0.shortcut.bn.running_mean',\t(2048,),\t 'layer4.0.downsample.1.running_mean',\t(2048,),\n",
    " 'block4.0.shortcut.bn.running_var',\t(2048,),\t 'layer4.0.downsample.1.running_var',\t(2048,),\n",
    " 'block4.1.conv_bn1.conv.weight',\t(1024, 2048, 1, 1),\t 'layer4.1.conv1.weight',\t(1024, 2048, 1, 1),\n",
    " 'block4.1.conv_bn1.bn.weight',\t(1024,),\t 'layer4.1.bn1.weight',\t(1024,),\n",
    " 'block4.1.conv_bn1.bn.bias',\t(1024,),\t 'layer4.1.bn1.bias',\t(1024,),\n",
    " 'block4.1.conv_bn1.bn.running_mean',\t(1024,),\t 'layer4.1.bn1.running_mean',\t(1024,),\n",
    " 'block4.1.conv_bn1.bn.running_var',\t(1024,),\t 'layer4.1.bn1.running_var',\t(1024,),\n",
    " 'block4.1.conv_bn2.conv.weight',\t(1024, 32, 3, 3),\t 'layer4.1.conv2.weight',\t(1024, 32, 3, 3),\n",
    " 'block4.1.conv_bn2.bn.weight',\t(1024,),\t 'layer4.1.bn2.weight',\t(1024,),\n",
    " 'block4.1.conv_bn2.bn.bias',\t(1024,),\t 'layer4.1.bn2.bias',\t(1024,),\n",
    " 'block4.1.conv_bn2.bn.running_mean',\t(1024,),\t 'layer4.1.bn2.running_mean',\t(1024,),\n",
    " 'block4.1.conv_bn2.bn.running_var',\t(1024,),\t 'layer4.1.bn2.running_var',\t(1024,),\n",
    " 'block4.1.conv_bn3.conv.weight',\t(2048, 1024, 1, 1),\t 'layer4.1.conv3.weight',\t(2048, 1024, 1, 1),\n",
    " 'block4.1.conv_bn3.bn.weight',\t(2048,),\t 'layer4.1.bn3.weight',\t(2048,),\n",
    " 'block4.1.conv_bn3.bn.bias',\t(2048,),\t 'layer4.1.bn3.bias',\t(2048,),\n",
    " 'block4.1.conv_bn3.bn.running_mean',\t(2048,),\t 'layer4.1.bn3.running_mean',\t(2048,),\n",
    " 'block4.1.conv_bn3.bn.running_var',\t(2048,),\t 'layer4.1.bn3.running_var',\t(2048,),\n",
    " 'block4.1.scale.fc1.weight',\t(128, 2048, 1, 1),\t 'layer4.1.se_module.fc1.weight',\t(128, 2048, 1, 1),\n",
    " 'block4.1.scale.fc1.bias',\t(128,),\t 'layer4.1.se_module.fc1.bias',\t(128,),\n",
    " 'block4.1.scale.fc2.weight',\t(2048, 128, 1, 1),\t 'layer4.1.se_module.fc2.weight',\t(2048, 128, 1, 1),\n",
    " 'block4.1.scale.fc2.bias',\t(2048,),\t 'layer4.1.se_module.fc2.bias',\t(2048,),\n",
    " 'block4.2.conv_bn1.conv.weight',\t(1024, 2048, 1, 1),\t 'layer4.2.conv1.weight',\t(1024, 2048, 1, 1),\n",
    " 'block4.2.conv_bn1.bn.weight',\t(1024,),\t 'layer4.2.bn1.weight',\t(1024,),\n",
    " 'block4.2.conv_bn1.bn.bias',\t(1024,),\t 'layer4.2.bn1.bias',\t(1024,),\n",
    " 'block4.2.conv_bn1.bn.running_mean',\t(1024,),\t 'layer4.2.bn1.running_mean',\t(1024,),\n",
    " 'block4.2.conv_bn1.bn.running_var',\t(1024,),\t 'layer4.2.bn1.running_var',\t(1024,),\n",
    " 'block4.2.conv_bn2.conv.weight',\t(1024, 32, 3, 3),\t 'layer4.2.conv2.weight',\t(1024, 32, 3, 3),\n",
    " 'block4.2.conv_bn2.bn.weight',\t(1024,),\t 'layer4.2.bn2.weight',\t(1024,),\n",
    " 'block4.2.conv_bn2.bn.bias',\t(1024,),\t 'layer4.2.bn2.bias',\t(1024,),\n",
    " 'block4.2.conv_bn2.bn.running_mean',\t(1024,),\t 'layer4.2.bn2.running_mean',\t(1024,),\n",
    " 'block4.2.conv_bn2.bn.running_var',\t(1024,),\t 'layer4.2.bn2.running_var',\t(1024,),\n",
    " 'block4.2.conv_bn3.conv.weight',\t(2048, 1024, 1, 1),\t 'layer4.2.conv3.weight',\t(2048, 1024, 1, 1),\n",
    " 'block4.2.conv_bn3.bn.weight',\t(2048,),\t 'layer4.2.bn3.weight',\t(2048,),\n",
    " 'block4.2.conv_bn3.bn.bias',\t(2048,),\t 'layer4.2.bn3.bias',\t(2048,),\n",
    " 'block4.2.conv_bn3.bn.running_mean',\t(2048,),\t 'layer4.2.bn3.running_mean',\t(2048,),\n",
    " 'block4.2.conv_bn3.bn.running_var',\t(2048,),\t 'layer4.2.bn3.running_var',\t(2048,),\n",
    " 'block4.2.scale.fc1.weight',\t(128, 2048, 1, 1),\t 'layer4.2.se_module.fc1.weight',\t(128, 2048, 1, 1),\n",
    " 'block4.2.scale.fc1.bias',\t(128,),\t 'layer4.2.se_module.fc1.bias',\t(128,),\n",
    " 'block4.2.scale.fc2.weight',\t(2048, 128, 1, 1),\t 'layer4.2.se_module.fc2.weight',\t(2048, 128, 1, 1),\n",
    " 'block4.2.scale.fc2.bias',\t(2048,),\t 'layer4.2.se_module.fc2.bias',\t(2048,),\n",
    " 'logit.weight',\t(1000, 1280),\t 'last_linear.weight',\t(1000, 2048),\n",
    " 'logit.bias',\t(1000,),\t 'last_linear.bias',\t(1000,),\n",
    "]\n",
    "\n",
    "PRETRAIN_FILE = 'data/pretrained_models/se_resnext50_32x4d-a260b3a4.pth'\n",
    "\n",
    "def load_pretrain(net, skip=[], pretrain_file=PRETRAIN_FILE, conversion=CONVERSION, is_print=True):\n",
    "\n",
    "    print('\\tload pretrain_file: %s'%pretrain_file)\n",
    "\n",
    "    #pretrain_state_dict = torch.load(pretrain_file)\n",
    "    pretrain_state_dict = torch.load(pretrain_file, map_location=lambda storage, loc: storage)\n",
    "    state_dict = net.state_dict()\n",
    "\n",
    "    i = 0\n",
    "    conversion = np.array(CONVERSION).reshape(-1,4)\n",
    "    for key,_,pretrain_key,_ in conversion:\n",
    "        if any(s in key for s in\n",
    "            ['.num_batches_tracked',]+skip):\n",
    "            continue\n",
    "\n",
    "        #print('\\t\\t',key)\n",
    "        if is_print:\n",
    "            print('\\t\\t','%-48s  %-24s  <---  %-32s  %-24s'%(\n",
    "                key, str(state_dict[key].shape),\n",
    "                pretrain_key, str(pretrain_state_dict[pretrain_key].shape),\n",
    "            ))\n",
    "        i = i+1\n",
    "\n",
    "        state_dict[key] = pretrain_state_dict[pretrain_key]\n",
    "\n",
    "    net.load_state_dict(state_dict)\n",
    "    print('')\n",
    "    print('len(pretrain_state_dict.keys()) = %d'%len(pretrain_state_dict.keys()))\n",
    "    print('len(state_dict.keys())          = %d'%len(state_dict.keys()))\n",
    "    print('loaded    = %d'%i)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_file =  glob.glob('data/train_pseudo_images_confident/*.jpg') #train_images\n",
    "#image_file = ['train_pseudo_images_confident/'+i.split('/')[-1] for i in image_file] #train_images\n",
    "#random.shuffle(image_file)\n",
    "#\n",
    "##12568\n",
    "#num_valid = 1000\n",
    "#num_all   = len(image_file)\n",
    "#num_train = num_all-num_valid\n",
    "#\n",
    "#train=np.array(image_file[num_valid:])\n",
    "#valid=np.array(image_file[:num_valid])\n",
    "#\n",
    "#print(len(image_file))\n",
    "#print(len(train))\n",
    "#print(len(valid))\n",
    "#print(len(valid)/len(train))\n",
    "#\n",
    "#np.save('data/split/train0_%d.npy'%len(train),train)\n",
    "#np.save('data/split/valid0_%d.npy'%len(valid),valid)\n",
    "#\n",
    "#print('train0_%d.npy'%len(train))\n",
    "#print('valid0_%d.npy'%len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats for train_pseudo_images_confident:\n",
    "#14045\n",
    "#13045\n",
    "#1000\n",
    "#0.07665772326561901\n",
    "#train0_13045.npy\n",
    "#valid0_1000.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    batch_size = 6\n",
    "    \n",
    "    initial_checkpoint = 'data/classification_models/se-resnext50/00049999_model.pth'\n",
    "    #'resnet34-cls-full-foldb0-0/checkpoint/00007500_model.pth'\n",
    "    \n",
    "    train_dataset = SteelDataset(\n",
    "        mode    = 'train',\n",
    "        csv     = ['train_pseudolabel_segmentation.csv',], \n",
    "        split   = ['train0_13045.npy'], #train0_11968.npy\n",
    "\n",
    "        augment = train_augment,\n",
    "    )\n",
    "    train_loader  = DataLoader(\n",
    "        train_dataset,\n",
    "        #sampler     = BalanceClassSampler(train_dataset, 3*len(train_dataset)),\n",
    "        #sampler    = SequentialSampler(train_dataset),\n",
    "        #sampler    = RandomSampler(train_dataset),\n",
    "        sampler    = FiveBalanceClassSampler(train_dataset),\n",
    "        batch_size  = batch_size,\n",
    "        drop_last   = True,\n",
    "        num_workers = 4,\n",
    "        pin_memory  = True,\n",
    "        collate_fn  = null_collate\n",
    "    )\n",
    "\n",
    "    valid_dataset = SteelDataset(\n",
    "        mode    = 'train',\n",
    "        csv     = ['train_pseudolabel_segmentation.csv'], \n",
    "        split   = ['valid0_1000.npy'], #valid_b1_1000.npy\n",
    "        augment = valid_augment,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        sampler    = SequentialSampler(valid_dataset),\n",
    "        #sampler     = RandomSampler(valid_dataset),\n",
    "        batch_size  = 4,\n",
    "        drop_last   = False,\n",
    "        num_workers = 4,\n",
    "        pin_memory  = True,\n",
    "        collate_fn  = null_collate\n",
    "    )\n",
    "    \n",
    "    assert(len(train_dataset)>=batch_size)\n",
    "    \n",
    "    net = Net().cuda()\n",
    "    \n",
    "    if initial_checkpoint is not None:\n",
    "        state_dict = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)\n",
    "        #for k in ['logit.weight','logit.bias']: state_dict.pop(k, None)\n",
    "        net.load_state_dict(state_dict,strict=False)\n",
    "    else:\n",
    "        net.load_pretrain(skip=['logit'], is_print=False)\n",
    "\n",
    "    num_iters   = 60*1000 #50*1000\n",
    "    iter_smooth = 50\n",
    "    iter_log    = 500\n",
    "    iter_valid  = 500\n",
    "    iter_save   = [num_iters-1] + list(range(0, num_iters, 1000))#1*1000\n",
    "    \n",
    "    #optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=schduler(0), momentum=0.9, weight_decay=0.0001)\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()))\n",
    "    \n",
    "    start_iter = 0\n",
    "    start_epoch= 0\n",
    "    rate       = 0\n",
    "    if initial_checkpoint is not None:\n",
    "        initial_optimizer = initial_checkpoint.replace('_model.pth','_optimizer.pth')\n",
    "        if os.path.exists(initial_optimizer):\n",
    "            checkpoint  = torch.load(initial_optimizer)\n",
    "            start_iter  = checkpoint['iter' ]\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \n",
    "    #scheduler = NullScheduler(lr=0.001)\n",
    "    max_lr = 0.0001 #0.0002\n",
    "    #scheduler = OneCycleLR(optimizer, max_lr=max_lr, div_factor=25, pct_start=0.3, total_steps=num_iters)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=max_lr, div_factor=25, pct_start=0.9, total_steps=num_iters)\n",
    "    lr = scheduler.get_lr()[0]\n",
    "    \n",
    "    train_loss = np.zeros(20,np.float32)\n",
    "    valid_loss = np.zeros(20,np.float32)\n",
    "    batch_loss = np.zeros(20,np.float32)\n",
    "    iter_accum = 8\n",
    "    iter = 0\n",
    "    i    = 0\n",
    "    \n",
    "    start = timer()\n",
    "    while  iter<num_iters:\n",
    "        sum_train_loss = np.zeros(20,np.float32)\n",
    "        sum = np.zeros(20,np.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        for t, (input, truth_mask, truth_label, infor) in enumerate(train_loader):\n",
    "            batch_size = len(infor)\n",
    "            iter  = i + start_iter\n",
    "            epoch = (iter-start_iter)*batch_size/len(train_dataset) + start_epoch\n",
    "            \n",
    "            # Weather to display images or not while in validation loss\n",
    "            displays = None\n",
    "            \n",
    "            if (iter % iter_valid==0):\n",
    "                valid_loss = do_valid(net, valid_loader, displays) # omitted outdir variable\n",
    "                #pass\n",
    "\n",
    "            if (iter % iter_log==0):\n",
    "                print('\\r',end='',flush=True)\n",
    "                asterisk = '*' if iter in iter_save else ' '\n",
    "                print('%0.8f  %5.1f%s %5.1f |  %5.3f   %4.4f [%4.4f,%4.4f,%4.4f,%4.4f]  |  %5.3f   %4.4f [%4.4f,%4.4f,%4.4f,%4.4f]  | %s' % (\\\n",
    "                         lr, iter/1000, asterisk, epoch,\n",
    "                         *valid_loss[:6],\n",
    "                         *train_loss[:6],\n",
    "                         time_to_str((timer() - start)))\n",
    "                )\n",
    "                print('\\n')\n",
    "                \n",
    "            if iter in iter_save:\n",
    "                torch.save(net.state_dict(),'data/classification_models/se-resnext50/%08d_model.pth'%(iter))\n",
    "                torch.save({\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'iter'     : iter,\n",
    "                    'epoch'    : epoch,\n",
    "                }, 'data/classification_models/se-resnext50/%08d_optimizer.pth'%(iter))\n",
    "                pass\n",
    "\n",
    "            # learning rate schduler -------------\n",
    "            lr = scheduler.get_lr()[0]\n",
    "            if lr<0 : break\n",
    "            #adjust_learning_rate(optimizer, lr)\n",
    "            #rate = get_learning_rate(optimizer)\n",
    "            \n",
    "            net.train()\n",
    "            input = input.cuda()\n",
    "            truth_label = truth_label.cuda()\n",
    "            truth_mask  = truth_mask.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logit =  net(input) #data_parallel(net,input)  \n",
    "            logit = logit.max(-1,True)[0]\n",
    "            loss = criterion(logit, truth_label)\n",
    "            tn,tp, num_neg,num_pos = metric_hit(logit, truth_label)\n",
    "            \n",
    "            (loss/iter_accum).backward()\n",
    "            if (iter % iter_accum)==0:\n",
    "                optimizer.step()\n",
    "                if iter < num_iters:\n",
    "                    scheduler.step(iter)\n",
    "\n",
    "            # print statistics  ------------\n",
    "            l = np.array([ loss.item(), tn,*tp ])\n",
    "            n = np.array([ batch_size, num_neg,*num_pos ])\n",
    "\n",
    "            batch_loss[:6] = l\n",
    "            sum_train_loss[:6] += l*n\n",
    "            sum[:6] += n\n",
    "            if iter%iter_smooth == 0:\n",
    "                train_loss = sum_train_loss/(sum+1e-12)\n",
    "                sum_train_loss[...] = 0\n",
    "                sum[...]            = 0\n",
    "\n",
    "\n",
    "            print('\\r',end='',flush=True)\n",
    "            asterisk = ' '\n",
    "            print('%0.8f  %5.1f%s %5.1f |  %5.3f   %4.4f [%4.4f,%4.4f,%4.4f,%4.4f]  |  %5.3f   %4.2f [%4.4f,%4.4f,%4.4f,%4.4f]  | %s' % (\\\n",
    "                         lr, iter/1000, asterisk, epoch,\n",
    "                         *valid_loss[:6],\n",
    "                         *batch_loss[:6],\n",
    "                         time_to_str((timer() - start)))\n",
    "            , end='',flush=True)\n",
    "            i=i+1\n",
    "           \n",
    "            # debug-----------------------------\n",
    "            if 1:\n",
    "                for di in range(3):\n",
    "                    if (iter+di)%1000==0:\n",
    "\n",
    "                        probability = torch.sigmoid(logit)\n",
    "                        image = input_to_image(input, IMAGE_RGB_MEAN,IMAGE_RGB_STD)\n",
    "\n",
    "                        probability_label = probability.data.cpu().numpy()\n",
    "                        truth_label = truth_label.data.cpu().numpy()\n",
    "                        truth_mask  = truth_mask.data.cpu().numpy()\n",
    "\n",
    "\n",
    "                        for b in range(batch_size):\n",
    "                            result = draw_predict_result_label(image[b], truth_mask[b], truth_label[b], probability_label[b], stack='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr          iter   epoch |  loss    tn, [tp1,tp2,tp3,tp4]       |  loss    tn, [tp1,tp2,tp3,tp4]       | time           \n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "0.00004000   50.0*  23.0 |  0.116   0.9692 [0.9324,0.9048,0.9091,0.9610]  |  0.000   0.0000 [0.0000,0.0000,0.0000,0.0000]  |  0 hr 00 min\n",
      "\n",
      "\n",
      "0.00009999   50.5   23.2 |  0.120   0.9692 [0.8919,0.9524,0.8923,0.9481]  |  0.125   0.9717 [0.8806,0.9839,0.7303,0.9839]  |  0 hr 07 min\n",
      "\n",
      "\n",
      "0.00010000   51.0*  23.5 |  0.120   0.9716 [0.9054,0.9048,0.8828,0.9610]  |  0.132   0.9727 [0.8649,0.9048,0.8452,0.9524]  |  0 hr 14 min\n",
      "\n",
      "\n",
      "0.00009925   51.5   23.7 |  0.117   0.9721 [0.9054,0.9524,0.8732,0.9610]  |  0.151   0.9672 [0.8438,0.8387,0.8132,0.9254]  |  0 hr 21 min\n",
      "\n",
      "\n",
      "0.00009703   52.0*  23.9 |  0.132   0.9619 [0.8919,0.9524,0.9019,0.9481]  |  0.130   0.9747 [0.9296,0.9531,0.6915,0.9508]  |  0 hr 28 min\n",
      "\n",
      "\n",
      "0.00009333   52.5   24.1 |  0.128   0.9633 [0.9189,0.9524,0.8900,0.9481]  |  0.118   0.9672 [0.8939,0.9048,0.7849,1.0000]  |  0 hr 35 min\n",
      "\n",
      "\n",
      "0.00008838   53.0*  24.4 |  0.120   0.9718 [0.8784,0.9524,0.8589,0.9351]  |  0.114   0.9738 [0.9000,0.9839,0.7865,0.9375]  |  0 hr 42 min\n",
      "\n",
      "\n",
      "0.00008218   53.5   24.6 |  0.117   0.9710 [0.8784,0.9524,0.8828,0.9481]  |  0.140   0.9706 [0.8529,0.9355,0.7111,0.9839]  |  0 hr 49 min\n",
      "\n",
      "\n",
      "0.00007511   54.0*  24.8 |  0.123   0.9692 [0.9189,0.9524,0.8947,0.9351]  |  0.110   0.9794 [0.9692,0.9206,0.7556,0.9667]  |  0 hr 55 min\n",
      "\n",
      "\n",
      "0.00006715   54.5   25.1 |  0.109   0.9730 [0.8919,0.9524,0.9091,0.9481]  |  0.126   0.9693 [0.8824,0.8833,0.7895,0.9385]  |  1 hr 02 min\n",
      "\n",
      "\n",
      "0.00005880   55.0*  25.3 |  0.108   0.9721 [0.9189,0.9524,0.8612,0.9481]  |  0.115   0.9762 [0.8906,0.8594,0.7619,0.9839]  |  1 hr 09 min\n",
      "\n",
      "\n",
      "0.00005005   55.5   25.5 |  0.111   0.9716 [0.9595,0.9524,0.8660,0.9610]  |  0.126   0.9792 [0.9254,0.8939,0.7340,0.9500]  |  1 hr 16 min\n",
      "\n",
      "\n",
      "0.00004144   56.0*  25.8 |  0.112   0.9707 [0.9459,0.9524,0.8684,0.9740]  |  0.126   0.9762 [0.8906,0.9167,0.7444,0.9672]  |  1 hr 23 min\n",
      "\n",
      "\n",
      "0.00003295   56.5   26.0 |  0.120   0.9654 [0.9865,0.9524,0.8708,0.9740]  |  0.140   0.9749 [0.8507,0.8906,0.7253,0.9672]  |  1 hr 30 min\n",
      "\n",
      "\n",
      "0.00002511   57.0*  26.2 |  0.117   0.9674 [0.9595,0.9524,0.8612,0.9740]  |  0.109   0.9825 [0.8841,0.9683,0.7416,1.0000]  |  1 hr 37 min\n",
      "\n",
      "\n",
      "0.00001790   57.5   26.4 |  0.112   0.9669 [0.9459,0.9524,0.8804,0.9610]  |  0.137   0.9705 [0.8611,0.9516,0.7303,0.9683]  |  1 hr 44 min\n",
      "\n",
      "\n",
      "0.00001178   58.0*  26.7 |  0.106   0.9721 [0.9459,0.9524,0.8565,0.9740]  |  0.102   0.9815 [0.9265,0.9508,0.8140,0.9531]  |  1 hr 50 min\n",
      "\n",
      "\n",
      "0.00000673   58.5   26.9 |  0.110   0.9713 [0.9459,0.9524,0.8780,0.9740]  |  0.143   0.9746 [0.8472,0.9365,0.6947,0.9692]  |  1 hr 57 min\n",
      "\n",
      "\n",
      "0.00000306   59.0*  27.1 |  0.113   0.9677 [0.9459,0.9524,0.8971,0.9610]  |  0.108   0.9696 [0.9231,0.9683,0.7159,1.0000]  |  2 hr 04 min\n",
      "\n",
      "\n",
      "0.00000178   59.2   27.2 |  0.113   0.9677 [0.9459,0.9524,0.8971,0.9610]  |  0.150   0.94 [1.0000,1.0000,1.0000,0.5000]  |  2 hr 07 min"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-75afe7b8ebb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lr          iter   epoch |  loss    tn, [tp1,tp2,tp3,tp4]       |  loss    tn, [tp1,tp2,tp3,tp4]       | time           '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--------------------------------------------------------------------------------------------------------------------\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-57e7c8ae7b7f>\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;31m# print statistics  ------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_pos\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('lr          iter   epoch |  loss    tn, [tp1,tp2,tp3,tp4]       |  loss    tn, [tp1,tp2,tp3,tp4]       | time           ')\n",
    "print('--------------------------------------------------------------------------------------------------------------------\\n')\n",
    "run_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## continued training\n",
    "\n",
    "### 1st try\n",
    "\n",
    "lr          iter   epoch |  loss    tn, [tp1,tp2,tp3,tp4]       |  loss    tn, [tp1,tp2,tp3,tp4]       | time           \n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "0.00004000   50.0*  23.0 |  0.115   0.9698 [0.9324,0.9048,0.9115,0.9481]  |  0.000   0.0000 [0.0000,0.0000,0.0000,0.0000]  |  0 hr 00 min\n",
    "\n",
    "\n",
    "0.00003425   50.5   23.2 |  0.124   0.9686 [0.9324,0.9048,0.8780,0.9610]  |  0.136   0.9747 [0.8824,0.9365,0.7097,0.8939]  |  0 hr 08 min\n",
    "\n",
    "\n",
    "0.00003275   51.0*  23.5 |  0.113   0.9716 [0.9189,0.9524,0.8995,0.9740]  |  0.108   0.9770 [0.9155,0.9048,0.7865,1.0000]  |  0 hr 15 min\n",
    "\n",
    "\n",
    "0.00003125   51.5   23.7 |  0.116   0.9692 [0.9189,0.9524,0.9187,0.9740]  |  0.104   0.9847 [0.8611,0.9355,0.7529,0.9531]  |  0 hr 22 min\n",
    "\n",
    "\n",
    "0.00002978   52.0*  23.9 |  0.115   0.9713 [0.9189,0.9524,0.8852,0.9740]  |  0.117   0.9792 [0.9000,0.9365,0.7473,0.9531]  |  0 hr 29 min\n",
    "\n",
    "\n",
    "0.00002831   52.5   24.1 |  0.118   0.9701 [0.9189,0.9524,0.9091,0.9740]  |  0.176   0.9637 [0.8406,0.8906,0.6842,0.9355]  |  0 hr 36 min\n",
    "\n",
    "\n",
    "0.00002689   53.0*  24.4 |  0.123   0.9648 [0.8919,0.9524,0.9187,0.9740]  |  0.117   0.9804 [0.8923,0.9180,0.7419,0.9531]  |  0 hr 42 min\n",
    "\n",
    "### 2nd try\n",
    "\n",
    "lr          iter   epoch |  loss    tn, [tp1,tp2,tp3,tp4]       |  loss    tn, [tp1,tp2,tp3,tp4]       | time           \n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "0.00004000   50.0*  23.0 |  0.116   0.9692 [0.9324,0.9048,0.9091,0.9610]  |  0.000   0.0000 [0.0000,0.0000,0.0000,0.0000]  |  0 hr 00 min\n",
    "\n",
    "\n",
    "0.00009999   50.5   23.2 |  0.120   0.9692 [0.8919,0.9524,0.8923,0.9481]  |  0.125   0.9717 [0.8806,0.9839,0.7303,0.9839]  |  0 hr 07 min\n",
    "\n",
    "\n",
    "0.00010000   51.0*  23.5 |  0.120   0.9716 [0.9054,0.9048,0.8828,0.9610]  |  0.132   0.9727 [0.8649,0.9048,0.8452,0.9524]  |  0 hr 14 min\n",
    "\n",
    "\n",
    "0.00009925   51.5   23.7 |  0.117   0.9721 [0.9054,0.9524,0.8732,0.9610]  |  0.151   0.9672 [0.8438,0.8387,0.8132,0.9254]  |  0 hr 21 min\n",
    "\n",
    "\n",
    "0.00009703   52.0*  23.9 |  0.132   0.9619 [0.8919,0.9524,0.9019,0.9481]  |  0.130   0.9747 [0.9296,0.9531,0.6915,0.9508]  |  0 hr 28 min\n",
    "\n",
    "\n",
    "0.00009333   52.5   24.1 |  0.128   0.9633 [0.9189,0.9524,0.8900,0.9481]  |  0.118   0.9672 [0.8939,0.9048,0.7849,1.0000]  |  0 hr 35 min\n",
    "\n",
    "\n",
    "0.00008838   53.0*  24.4 |  0.120   0.9718 [0.8784,0.9524,0.8589,0.9351]  |  0.114   0.9738 [0.9000,0.9839,0.7865,0.9375]  |  0 hr 42 min\n",
    "\n",
    "\n",
    "0.00008218   53.5   24.6 |  0.117   0.9710 [0.8784,0.9524,0.8828,0.9481]  |  0.140   0.9706 [0.8529,0.9355,0.7111,0.9839]  |  0 hr 49 min\n",
    "\n",
    "\n",
    "0.00007511   54.0*  24.8 |  0.123   0.9692 [0.9189,0.9524,0.8947,0.9351]  |  0.110   0.9794 [0.9692,0.9206,0.7556,0.9667]  |  0 hr 55 min\n",
    "\n",
    "\n",
    "0.00006715   54.5   25.1 |  0.109   0.9730 [0.8919,0.9524,0.9091,0.9481]  |  0.126   0.9693 [0.8824,0.8833,0.7895,0.9385]  |  1 hr 02 min\n",
    "\n",
    "\n",
    "0.00005880   55.0*  25.3 |  0.108   0.9721 [0.9189,0.9524,0.8612,0.9481]  |  0.115   0.9762 [0.8906,0.8594,0.7619,0.9839]  |  1 hr 09 min\n",
    "\n",
    "\n",
    "0.00005005   55.5   25.5 |  0.111   0.9716 [0.9595,0.9524,0.8660,0.9610]  |  0.126   0.9792 [0.9254,0.8939,0.7340,0.9500]  |  1 hr 16 min\n",
    "\n",
    "\n",
    "0.00004144   56.0*  25.8 |  0.112   0.9707 [0.9459,0.9524,0.8684,0.9740]  |  0.126   0.9762 [0.8906,0.9167,0.7444,0.9672]  |  1 hr 23 min\n",
    "\n",
    "\n",
    "0.00003295   56.5   26.0 |  0.120   0.9654 [0.9865,0.9524,0.8708,0.9740]  |  0.140   0.9749 [0.8507,0.8906,0.7253,0.9672]  |  1 hr 30 min\n",
    "\n",
    "\n",
    "0.00002511   57.0*  26.2 |  0.117   0.9674 [0.9595,0.9524,0.8612,0.9740]  |  0.109   0.9825 [0.8841,0.9683,0.7416,1.0000]  |  1 hr 37 min\n",
    "\n",
    "\n",
    "0.00001790   57.5   26.4 |  0.112   0.9669 [0.9459,0.9524,0.8804,0.9610]  |  0.137   0.9705 [0.8611,0.9516,0.7303,0.9683]  |  1 hr 44 min\n",
    "\n",
    "\n",
    "0.00001178   58.0*  26.7 |  0.106   0.9721 [0.9459,0.9524,0.8565,0.9740]  |  0.102   0.9815 [0.9265,0.9508,0.8140,0.9531]  |  1 hr 50 min\n",
    "\n",
    "\n",
    "0.00000673   58.5   26.9 |  0.110   0.9713 [0.9459,0.9524,0.8780,0.9740]  |  0.143   0.9746 [0.8472,0.9365,0.6947,0.9692]  |  1 hr 57 min\n",
    "\n",
    "\n",
    "0.00000306   59.0*  27.1 |  0.113   0.9677 [0.9459,0.9524,0.8971,0.9610]  |  0.108   0.9696 [0.9231,0.9683,0.7159,1.0000]  |  2 hr 04 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## best so far\n",
    "\n",
    "0.00000810   48.0*  22.1 |  0.108   0.9730 [0.9054,0.9048,0.8876,0.9610]  |  0.144   0.9738 [0.8767,0.9153,0.7326,0.9701]  | 10 hr 45 min\n",
    "\n",
    "\n",
    "0.00000455   48.5   22.3 |  0.122   0.9657 [0.9054,0.9524,0.9378,0.9481]  |  0.122   0.9814 [0.9420,0.8906,0.7391,0.9365]  | 10 hr 52 min\n",
    "\n",
    "\n",
    "0.00000205   49.0*  22.5 |  0.116   0.9704 [0.9459,0.9048,0.8852,0.9610]  |  0.119   0.9716 [0.8732,0.9394,0.7674,0.9683]  | 10 hr 58 min\n",
    "\n",
    "\n",
    "0.00000051   49.5   22.8 |  0.104   0.9771 [0.8919,0.9048,0.8636,0.9481]  |  0.117   0.9693 [0.8841,0.9077,0.7753,0.9545]  | 11 hr 05 min\n",
    "\n",
    "\n",
    "0.00000000   50.0   23.0 |  0.113   0.9718 [0.9189,0.9524,0.9091,0.9481]  |  0.127   0.9661 [0.8382,0.9077,0.7444,0.9524]  | 11 hr 12 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr          iter   epoch |  loss    tn, [tp1,tp2,tp3,tp4]       |  loss    tn, [tp1,tp2,tp3,tp4]       | time           \n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\tload pretrain_file: data/pretrained_models/se_resnext50_32x4d-a260b3a4.pth\n",
    "\n",
    "len(pretrain_state_dict.keys()) = 331\n",
    "len(state_dict.keys())          = 386\n",
    "loaded    = 329\n",
    "\n",
    "0.00004000    0.0*   0.0 |  2.353   0.1147 [0.9459,1.0000,0.9163,0.9740]  |  0.000   0.0000 [0.0000,0.0000,0.0000,0.0000]  |  0 hr 00 min\n",
    "\n",
    "\n",
    "0.00004259    0.5    0.2 |  0.399   0.9962 [0.0000,0.0000,0.0670,0.0000]  |  0.512   0.9967 [0.0000,0.0164,0.0000,0.0000]  |  0 hr 07 min\n",
    "\n",
    "\n",
    "0.00005032    1.0*   0.5 |  0.315   0.9648 [0.0000,0.7143,0.4450,0.8312]  |  0.413   0.9499 [0.0000,0.4098,0.3864,0.6562]  |  0 hr 13 min\n",
    "\n",
    "\n",
    "0.00006337    1.5    0.7 |  0.303   0.9214 [0.2568,0.9524,0.4498,0.8831]  |  0.348   0.9561 [0.0800,0.7937,0.3933,0.8033]  |  0 hr 20 min\n",
    "\n",
    "\n",
    "0.00008118    2.0*   0.9 |  0.340   0.9302 [0.3649,0.8095,0.6268,0.7013]  |  0.351   0.9835 [0.2222,0.6935,0.3711,0.6613]  |  0 hr 27 min\n",
    "\n",
    "\n",
    "0.00010412    2.5    1.1 |  0.247   0.9616 [0.2297,0.9048,0.5239,0.9351]  |  0.319   0.9613 [0.2703,0.8387,0.3711,0.8413]  |  0 hr 34 min\n",
    "\n",
    "\n",
    "0.00013121    3.0*   1.4 |  0.303   0.9044 [0.7568,1.0000,0.6053,0.8701]  |  0.279   0.9541 [0.3514,0.7705,0.6000,0.8750]  |  0 hr 40 min\n",
    "\n",
    "\n",
    "0.00016304    3.5    1.6 |  0.282   0.9044 [0.8784,0.5714,0.5718,0.8831]  |  0.315   0.9394 [0.6000,0.7419,0.4762,0.7077]  |  0 hr 47 min\n",
    "\n",
    "\n",
    "0.00019824    4.0*   1.8 |  0.308   0.9381 [0.7703,0.5238,0.4689,0.9740]  |  0.281   0.9619 [0.2500,0.8889,0.5909,0.9194]  |  0 hr 54 min\n",
    "\n",
    "\n",
    "0.00023756    4.5    2.1 |  0.374   0.8792 [0.8514,1.0000,0.5215,0.8961]  |  0.276   0.9543 [0.5672,0.8226,0.5172,0.9062]  |  1 hr 01 min\n",
    "\n",
    "\n",
    "0.00027933    5.0*   2.3 |  0.306   0.8985 [0.8649,0.4286,0.7081,0.8701]  |  0.301   0.9429 [0.6081,0.8548,0.3407,0.8095]  |  1 hr 07 min\n",
    "\n",
    "\n",
    "0.00032443    5.5    2.5 |  0.373   0.9182 [0.5541,0.4762,0.5718,0.8961]  |  0.322   0.9591 [0.2500,0.6032,0.4699,0.7742]  |  1 hr 14 min\n",
    "\n",
    "\n",
    "0.00037095    6.0*   2.8 |  0.245   0.9185 [0.7703,0.9048,0.7871,0.8701]  |  0.288   0.9441 [0.6389,0.7213,0.4783,0.8730]  |  1 hr 21 min\n",
    "\n",
    "\n",
    "0.00041985    6.5    3.0 |  0.227   0.9733 [0.3243,0.9048,0.6914,0.6234]  |  0.312   0.9467 [0.4638,0.7302,0.5402,0.7869]  |  1 hr 28 min\n",
    "\n",
    "\n",
    "0.00046907    7.0*   3.2 |  0.244   0.9566 [0.5000,0.6190,0.5502,0.9221]  |  0.311   0.9464 [0.4000,0.7258,0.6444,0.7143]  |  1 hr 34 min\n",
    "\n",
    "\n",
    "0.00051965    7.5    3.4 |  0.324   0.9000 [0.6892,0.9524,0.5646,0.9221]  |  0.302   0.9749 [0.3016,0.8788,0.3478,0.8281]  |  1 hr 41 min\n",
    "\n",
    "\n",
    "0.00056943    8.0*   3.7 |  0.323   0.9352 [0.4459,0.9048,0.4115,0.8052]  |  0.312   0.9384 [0.4848,0.8871,0.4643,0.7302]  |  1 hr 48 min\n",
    "\n",
    "\n",
    "0.00061946    8.5    3.9 |  0.320   0.9179 [0.7432,0.8095,0.4019,0.9351]  |  0.293   0.9456 [0.5507,0.7500,0.5393,0.8254]  |  1 hr 55 min\n",
    "\n",
    "\n",
    "0.00066762    9.0*   4.1 |  0.332   0.9188 [0.8243,0.8095,0.4761,0.6494]  |  0.310   0.9518 [0.5972,0.8197,0.5165,0.8889]  |  2 hr 01 min\n",
    "\n",
    "\n",
    "0.00071492    9.5    4.4 |  0.355   0.8938 [0.9054,0.3333,0.4067,0.7273]  |  0.339   0.9347 [0.5000,0.7460,0.4200,0.7576]  |  2 hr 08 min\n",
    "\n",
    "\n",
    "0.00075936   10.0*   4.6 |  0.304   0.9065 [0.7432,0.7143,0.6579,0.6234]  |  0.343   0.9069 [0.5147,0.8065,0.4118,0.9016]  |  2 hr 15 min\n",
    "\n",
    "\n",
    "0.00080187   10.5    4.8 |  0.378   0.9067 [0.7162,0.2381,0.7153,0.8961]  |  0.334   0.9454 [0.6176,0.7742,0.4176,0.7031]  |  2 hr 22 min\n",
    "\n",
    "\n",
    "0.00084064   11.0*   5.1 |  0.288   0.9370 [0.5811,0.9524,0.5670,0.8442]  |  0.331   0.9471 [0.4932,0.8095,0.3483,0.8209]  |  2 hr 28 min\n",
    "\n",
    "\n",
    "0.00087649   11.5    5.3 |  0.437   0.8724 [0.8243,0.5714,0.5718,0.3896]  |  0.362   0.9276 [0.5205,0.6885,0.3913,0.8889]  |  2 hr 35 min\n",
    "\n",
    "\n",
    "0.00090790   12.0*   5.5 |  0.258   0.9367 [0.5135,0.6190,0.4785,0.9351]  |  0.339   0.9575 [0.5147,0.7500,0.3605,0.6719]  |  2 hr 42 min\n",
    "\n",
    "\n",
    "0.00093553   12.5    5.7 |  0.239   0.9208 [0.8243,0.1905,0.7033,0.9221]  |  0.312   0.9573 [0.4154,0.7188,0.3587,0.9231]  |  2 hr 49 min\n",
    "\n",
    "\n",
    "0.00095821   13.0*   6.0 |  0.301   0.9038 [0.7973,0.5714,0.6316,0.8831]  |  0.323   0.9406 [0.5493,0.5672,0.3407,0.9355]  |  2 hr 56 min\n",
    "\n",
    "\n",
    "0.00097641   13.5    6.2 |  0.315   0.9170 [0.7568,0.9048,0.6435,0.8831]  |  0.282   0.9454 [0.5694,0.6667,0.5275,0.8065]  |  3 hr 02 min\n",
    "\n",
    "\n",
    "0.00098936   14.0*   6.4 |  0.341   0.8933 [0.8108,0.6667,0.6220,0.8831]  |  0.275   0.9650 [0.5797,0.7000,0.4842,0.9048]  |  3 hr 09 min\n",
    "\n",
    "\n",
    "0.00099734   14.5    6.7 |  0.302   0.9106 [0.8514,0.9048,0.3900,0.5584]  |  0.306   0.9491 [0.1571,0.7619,0.5309,0.7903]  |  3 hr 16 min\n",
    "\n",
    "\n",
    "0.00100000   15.0*   6.9 |  0.377   0.9073 [0.7432,0.9524,0.3469,0.8182]  |  0.324   0.9500 [0.6812,0.7258,0.2727,0.7869]  |  3 hr 23 min\n",
    "\n",
    "\n",
    "0.00099950   15.5    7.1 |  0.277   0.9352 [0.8243,0.8571,0.4641,0.8312]  |  0.329   0.9532 [0.5735,0.6129,0.2299,0.8000]  |  3 hr 30 min\n",
    "\n",
    "\n",
    "0.00099802   16.0*   7.4 |  0.226   0.9422 [0.7703,0.8095,0.5861,0.9351]  |  0.265   0.9516 [0.6197,0.7538,0.5333,0.9231]  |  3 hr 36 min\n",
    "\n",
    "\n",
    "0.00099549   16.5    7.6 |  0.276   0.9120 [0.7297,0.8095,0.6435,0.7662]  |  0.300   0.9530 [0.6667,0.6508,0.4574,0.7143]  |  3 hr 43 min\n",
    "\n",
    "\n",
    "0.00099202   17.0*   7.8 |  0.266   0.9032 [0.7838,0.8571,0.8014,0.6753]  |  0.310   0.9570 [0.6769,0.7143,0.3400,0.8182]  |  3 hr 50 min\n",
    "\n",
    "\n",
    "0.00098749   17.5    8.0 |  0.270   0.9073 [0.7838,0.9524,0.6220,0.8182]  |  0.326   0.9595 [0.4242,0.7302,0.2717,0.8462]  |  3 hr 57 min\n",
    "\n",
    "\n",
    "0.00098206   18.0*   8.3 |  0.227   0.9282 [0.8514,0.9048,0.7273,0.7662]  |  0.260   0.9568 [0.6447,0.7619,0.6737,0.7656]  |  4 hr 04 min\n",
    "\n",
    "\n",
    "0.00097557   18.5    8.5 |  0.273   0.9416 [0.7432,0.9524,0.5072,0.7273]  |  0.228   0.9397 [0.6970,0.8438,0.7317,0.8833]  |  4 hr 10 min\n",
    "\n",
    "\n",
    "0.00096823   19.0*   8.7 |  0.241   0.9334 [0.8784,0.7619,0.7416,0.7143]  |  0.270   0.9574 [0.6269,0.6923,0.5889,0.8095]  |  4 hr 17 min\n",
    "\n",
    "\n",
    "0.00095982   19.5    9.0 |  0.209   0.9405 [0.8243,0.9524,0.6220,0.8442]  |  0.260   0.9596 [0.5694,0.8033,0.4659,0.8906]  |  4 hr 24 min\n",
    "\n",
    "\n",
    "0.00095062   20.0*   9.2 |  0.209   0.9405 [0.9054,0.9524,0.6938,0.8961]  |  0.273   0.9451 [0.6438,0.8197,0.5889,0.8636]  |  4 hr 31 min\n",
    "\n",
    "\n",
    "0.00094036   20.5    9.4 |  0.318   0.8962 [0.9054,1.0000,0.5191,0.8571]  |  0.273   0.9597 [0.4225,0.8730,0.4535,0.8730]  |  4 hr 38 min\n",
    "\n",
    "\n",
    "0.00092939   21.0*   9.7 |  0.242   0.9243 [0.9054,0.9048,0.8110,0.9351]  |  0.259   0.9470 [0.6389,0.9032,0.5102,0.8387]  |  4 hr 44 min\n",
    "\n",
    "\n",
    "0.00091736   21.5    9.9 |  0.196   0.9613 [0.7703,0.8095,0.6986,0.7532]  |  0.236   0.9728 [0.6479,0.8571,0.4884,0.9167]  |  4 hr 51 min\n",
    "\n",
    "\n",
    "0.00090469   22.0*  10.1 |  0.232   0.9484 [0.8378,0.6667,0.6268,0.6883]  |  0.274   0.9364 [0.5970,0.7333,0.6531,0.8889]  |  4 hr 58 min\n",
    "\n",
    "\n",
    "0.00089100   22.5   10.3 |  0.205   0.9434 [0.7838,0.9048,0.7273,0.9481]  |  0.223   0.9508 [0.8209,0.9167,0.6170,0.8769]  |  5 hr 05 min\n",
    "\n",
    "\n",
    "0.00087674   23.0*  10.6 |  0.188   0.9630 [0.7162,0.9524,0.6077,0.8701]  |  0.221   0.9587 [0.6761,0.8833,0.6023,0.9016]  |  5 hr 11 min\n",
    "\n",
    "\n",
    "0.00086149   23.5   10.8 |  0.200   0.9569 [0.7973,0.9048,0.6316,0.8312]  |  0.238   0.9462 [0.6522,0.8689,0.6907,0.8730]  |  5 hr 18 min\n",
    "\n",
    "\n",
    "0.00084576   24.0*  11.0 |  0.206   0.9490 [0.6757,0.9524,0.6794,0.8961]  |  0.259   0.9466 [0.5303,0.8871,0.6477,0.8485]  |  5 hr 25 min\n",
    "\n",
    "\n",
    "0.00082907   24.5   11.3 |  0.186   0.9446 [0.8649,0.9524,0.8397,0.8701]  |  0.269   0.9515 [0.6957,0.6190,0.6022,0.8088]  |  5 hr 32 min\n",
    "\n",
    "\n",
    "0.00081199   25.0*  11.5 |  0.234   0.9419 [0.8919,0.9524,0.6077,0.8442]  |  0.236   0.9362 [0.7183,0.9180,0.6044,0.8676]  |  5 hr 39 min\n",
    "\n",
    "\n",
    "0.00079400   25.5   11.7 |  0.253   0.9126 [0.8919,0.9524,0.7153,0.9091]  |  0.227   0.9661 [0.7101,0.8254,0.5955,0.8438]  |  5 hr 45 min\n",
    "\n",
    "\n",
    "0.00077571   26.0*  12.0 |  0.172   0.9569 [0.8243,0.9524,0.7871,0.9351]  |  0.224   0.9498 [0.6714,0.8254,0.6279,0.8594]  |  5 hr 52 min\n",
    "\n",
    "\n",
    "0.00075657   26.5   12.2 |  0.201   0.9437 [0.8108,0.8571,0.8062,0.7662]  |  0.256   0.9780 [0.7042,0.8438,0.5000,0.7031]  |  5 hr 59 min\n",
    "\n",
    "\n",
    "0.00073721   27.0*  12.4 |  0.193   0.9469 [0.8243,0.9048,0.7656,0.8961]  |  0.213   0.9675 [0.6957,0.8689,0.5904,0.9375]  |  6 hr 06 min\n",
    "\n",
    "\n",
    "0.00071706   27.5   12.6 |  0.194   0.9595 [0.7838,0.9524,0.7416,0.8312]  |  0.266   0.9397 [0.6438,0.9167,0.6915,0.8361]  |  6 hr 12 min\n",
    "\n",
    "\n",
    "0.00069680   28.0*  12.9 |  0.203   0.9563 [0.7432,0.9524,0.6842,0.8312]  |  0.237   0.9518 [0.6761,0.8413,0.7000,0.8254]  |  6 hr 19 min\n",
    "\n",
    "\n",
    "0.00067581   28.5   13.1 |  0.217   0.9434 [0.8784,0.9524,0.7105,0.9221]  |  0.246   0.9410 [0.7463,0.7742,0.6087,0.9206]  |  6 hr 26 min\n",
    "\n",
    "\n",
    "0.00065481   29.0*  13.3 |  0.180   0.9610 [0.8784,0.9048,0.6507,0.9221]  |  0.218   0.9663 [0.6765,0.9355,0.4318,0.9355]  |  6 hr 32 min\n",
    "\n",
    "\n",
    "0.00063315   29.5   13.6 |  0.231   0.9545 [0.7297,0.9524,0.6148,0.8961]  |  0.180   0.9729 [0.8923,0.9194,0.6703,0.8852]  |  6 hr 39 min\n",
    "\n",
    "\n",
    "0.00061157   30.0*  13.8 |  0.167   0.9604 [0.8378,0.9524,0.7990,0.9221]  |  0.218   0.9634 [0.6866,0.9683,0.5513,0.8594]  |  6 hr 46 min\n",
    "\n",
    "\n",
    "0.00058941   30.5   14.0 |  0.135   0.9730 [0.8378,0.9048,0.7799,0.9091]  |  0.194   0.9694 [0.7059,0.9524,0.5955,0.8594]  |  6 hr 52 min\n",
    "\n",
    "\n",
    "0.00056743   31.0*  14.3 |  0.165   0.9630 [0.8243,0.9524,0.7823,0.8312]  |  0.205   0.9532 [0.7273,0.9206,0.5934,0.9355]  |  6 hr 59 min\n",
    "\n",
    "\n",
    "0.00054496   31.5   14.5 |  0.158   0.9525 [0.7838,0.9524,0.8708,0.9221]  |  0.200   0.9571 [0.7879,0.8154,0.7320,0.9365]  |  7 hr 06 min\n",
    "\n",
    "\n",
    "0.00052275   32.0*  14.7 |  0.187   0.9481 [0.9054,0.9524,0.7871,0.9610]  |  0.215   0.9540 [0.7945,0.8226,0.5843,0.9683]  |  7 hr 12 min\n",
    "\n",
    "\n",
    "0.00050014   32.5   14.9 |  0.173   0.9666 [0.7838,0.9524,0.7560,0.9091]  |  0.216   0.9607 [0.6761,0.9048,0.5882,0.9219]  |  7 hr 19 min\n",
    "\n",
    "\n",
    "0.00047788   33.0*  15.2 |  0.134   0.9674 [0.9189,0.9524,0.7727,0.9351]  |  0.187   0.9686 [0.9365,0.8710,0.5506,0.9365]  |  7 hr 26 min\n",
    "\n",
    "\n",
    "0.00045532   33.5   15.4 |  0.153   0.9598 [0.8784,0.9524,0.8517,0.9351]  |  0.190   0.9575 [0.8000,0.9048,0.6237,0.9677]  |  7 hr 32 min\n",
    "\n",
    "\n",
    "0.00043320   34.0*  15.6 |  0.160   0.9569 [0.8784,0.9524,0.8900,0.9351]  |  0.187   0.9651 [0.7727,0.8852,0.6452,0.9375]  |  7 hr 39 min\n",
    "\n",
    "\n",
    "0.00041086   34.5   15.9 |  0.159   0.9630 [0.8919,0.8571,0.7895,0.9481]  |  0.175   0.9602 [0.8143,0.8333,0.7010,0.9841]  |  7 hr 46 min\n",
    "\n",
    "\n",
    "0.00038905   35.0*  16.1 |  0.149   0.9587 [0.8784,0.9048,0.8014,0.9351]  |  0.171   0.9642 [0.8676,0.7937,0.6867,0.9524]  |  7 hr 52 min\n",
    "\n",
    "\n",
    "0.00036711   35.5   16.3 |  0.167   0.9598 [0.8919,0.9524,0.8158,0.9481]  |  0.190   0.9681 [0.7714,0.9077,0.6250,0.9254]  |  7 hr 59 min\n",
    "\n",
    "\n",
    "0.00034579   36.0*  16.6 |  0.162   0.9622 [0.8919,0.9524,0.7608,0.9740]  |  0.168   0.9683 [0.7432,0.9219,0.6512,0.9677]  |  8 hr 05 min\n",
    "\n",
    "\n",
    "0.00032444   36.5   16.8 |  0.146   0.9657 [0.7973,0.8571,0.8565,0.9481]  |  0.163   0.9682 [0.8000,0.9365,0.6264,0.9531]  |  8 hr 12 min\n",
    "\n",
    "\n",
    "0.00030378   37.0*  17.0 |  0.135   0.9698 [0.9189,0.8571,0.8134,0.9351]  |  0.146   0.9716 [0.7703,0.9180,0.7857,0.9531]  |  8 hr 19 min\n",
    "\n",
    "\n",
    "0.00028318   37.5   17.2 |  0.150   0.9592 [0.8649,0.9524,0.8612,0.9481]  |  0.164   0.9672 [0.7973,0.8361,0.6279,1.0000]  |  8 hr 25 min\n",
    "\n",
    "\n",
    "0.00026335   38.0*  17.5 |  0.136   0.9654 [0.8919,0.9524,0.8541,0.9351]  |  0.157   0.9716 [0.8676,0.9839,0.6559,0.9508]  |  8 hr 32 min\n",
    "\n",
    "\n",
    "0.00024367   38.5   17.7 |  0.132   0.9625 [0.8514,0.9524,0.8876,0.9481]  |  0.130   0.9770 [0.8769,0.9219,0.7889,0.9403]  |  8 hr 39 min\n",
    "\n",
    "\n",
    "0.00022482   39.0*  17.9 |  0.133   0.9607 [0.8784,0.9524,0.9067,0.8961]  |  0.155   0.9576 [0.8056,0.9032,0.8193,0.9206]  |  8 hr 45 min\n",
    "\n",
    "\n",
    "0.00020622   39.5   18.2 |  0.137   0.9598 [0.9189,0.9524,0.8947,0.9221]  |  0.146   0.9726 [0.8209,0.9365,0.7174,0.9385]  |  8 hr 52 min\n",
    "\n",
    "\n",
    "0.00018850   40.0*  18.4 |  0.114   0.9674 [0.8919,0.9048,0.9019,0.9610]  |  0.164   0.9603 [0.7945,0.8525,0.7234,0.9692]  |  8 hr 59 min\n",
    "\n",
    "\n",
    "0.00017114   40.5   18.6 |  0.137   0.9642 [0.9189,0.9048,0.8636,0.9610]  |  0.168   0.9685 [0.7606,0.8710,0.7059,0.9836]  |  9 hr 05 min\n",
    "\n",
    "\n",
    "0.00015470   41.0*  18.9 |  0.116   0.9695 [0.8784,0.9048,0.8804,0.9481]  |  0.127   0.9812 [0.9412,0.9219,0.6768,0.9683]  |  9 hr 12 min\n",
    "\n",
    "\n",
    "0.00013870   41.5   19.1 |  0.123   0.9672 [0.8784,0.9048,0.8636,0.9481]  |  0.150   0.9671 [0.8462,0.8636,0.7065,0.9844]  |  9 hr 19 min\n",
    "\n",
    "\n",
    "0.00012367   42.0*  19.3 |  0.143   0.9610 [0.8784,0.9524,0.8947,0.9870]  |  0.135   0.9726 [0.8551,0.9531,0.7474,0.9667]  |  9 hr 25 min\n",
    "\n",
    "\n",
    "0.00010917   42.5   19.5 |  0.153   0.9584 [0.9324,0.9048,0.8828,0.9610]  |  0.141   0.9692 [0.8971,0.9231,0.6484,0.9848]  |  9 hr 32 min\n",
    "\n",
    "\n",
    "0.00009568   43.0*  19.8 |  0.126   0.9674 [0.9189,0.9048,0.8995,0.9610]  |  0.159   0.9722 [0.8082,0.8923,0.7083,0.8955]  |  9 hr 39 min\n",
    "\n",
    "\n",
    "0.00008279   43.5   20.0 |  0.127   0.9695 [0.9189,0.9048,0.8756,0.9481]  |  0.147   0.9650 [0.9130,0.9194,0.6304,0.9365]  |  9 hr 45 min\n",
    "\n",
    "\n",
    "0.00007094   44.0*  20.2 |  0.117   0.9748 [0.9054,0.9048,0.8852,0.9351]  |  0.120   0.9761 [0.9014,0.9048,0.7407,0.9385]  |  9 hr 52 min\n",
    "\n",
    "\n",
    "0.00005977   44.5   20.5 |  0.124   0.9704 [0.8919,0.9048,0.8804,0.9610]  |  0.138   0.9836 [0.8611,0.9508,0.7222,0.9219]  |  9 hr 59 min\n",
    "\n",
    "\n",
    "0.00004966   45.0*  20.7 |  0.118   0.9721 [0.8784,0.9524,0.8852,0.9610]  |  0.128   0.9684 [0.8611,0.9516,0.7024,0.9375]  | 10 hr 05 min\n",
    "\n",
    "\n",
    "0.00004029   45.5   20.9 |  0.124   0.9663 [0.9054,0.9524,0.9043,0.9740]  |  0.112   0.9848 [0.8824,0.9500,0.7333,0.9841]  | 10 hr 12 min\n",
    "\n",
    "\n",
    "0.00003200   46.0*  21.2 |  0.121   0.9672 [0.9189,0.9048,0.8947,0.9740]  |  0.118   0.9781 [0.8714,0.8923,0.8068,0.9375]  | 10 hr 18 min\n",
    "\n",
    "\n",
    "0.00002452   46.5   21.4 |  0.115   0.9710 [0.9324,0.9048,0.8971,0.9610]  |  0.138   0.9654 [0.8382,0.9180,0.7590,0.9531]  | 10 hr 25 min\n",
    "\n",
    "\n",
    "0.00001811   47.0*  21.6 |  0.120   0.9718 [0.9459,0.9524,0.8612,0.9610]  |  0.149   0.9716 [0.8485,0.9194,0.6526,0.9836]  | 10 hr 32 min\n",
    "\n",
    "\n",
    "0.00001257   47.5   21.8 |  0.116   0.9698 [0.9189,0.9524,0.9211,0.9481]  |  0.137   0.9738 [0.8261,0.8889,0.7753,0.9839]  | 10 hr 38 min\n",
    "\n",
    "\n",
    "0.00000810   48.0*  22.1 |  0.108   0.9730 [0.9054,0.9048,0.8876,0.9610]  |  0.144   0.9738 [0.8767,0.9153,0.7326,0.9701]  | 10 hr 45 min\n",
    "\n",
    "\n",
    "0.00000455   48.5   22.3 |  0.122   0.9657 [0.9054,0.9524,0.9378,0.9481]  |  0.122   0.9814 [0.9420,0.8906,0.7391,0.9365]  | 10 hr 52 min\n",
    "\n",
    "\n",
    "0.00000205   49.0*  22.5 |  0.116   0.9704 [0.9459,0.9048,0.8852,0.9610]  |  0.119   0.9716 [0.8732,0.9394,0.7674,0.9683]  | 10 hr 58 min\n",
    "\n",
    "\n",
    "0.00000051   49.5   22.8 |  0.104   0.9771 [0.8919,0.9048,0.8636,0.9481]  |  0.117   0.9693 [0.8841,0.9077,0.7753,0.9545]  | 11 hr 05 min\n",
    "\n",
    "\n",
    "0.00000000   50.0   23.0 |  0.113   0.9718 [0.9189,0.9524,0.9091,0.9481]  |  0.127   0.9661 [0.8382,0.9077,0.7444,0.9524]  | 11 hr 12 min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
