{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heng's Starter code for training the classification model on Kaggle.**\n",
    "\n",
    "I have not run the kernel with GPU enabled because I do not have much of Kaggle GPU left as of now. So the kernel as expected is giving CUDA error.\n",
    "This is just a simple kernel for training model on Kaggle easily. Made some minor changes to his code and seems like it will run fine here on kaggle.\n",
    "I have not tested the training time. It can exceed the 9 hour limit.\n",
    "\n",
    "The kernel is based on Heng's starter kit version 20190910 you can find it [here](https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/106462#latest-645576) .\n",
    "I have imported 2 utility scripts one for the utility functions with plotting code and another one is for model.\n",
    "You can fork and edit the utility scripts and add the model classes as you feel like.\n",
    "The model architecture can be changed from this kernel below by changing the Net() class.\n",
    "\n",
    "If you face any problems or errors then feel free to comment them.\n",
    "At last thank you very much [Heng](https://www.kaggle.com/hengck23) and other leaderboard rankers for helping newbies like me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "\n",
    "from lib.utility_functions import *\n",
    "from lib.models_all import *\n",
    "from lib.rate import *\n",
    "\n",
    "#import pretrainedmodels\n",
    "\n",
    "PI = np.pi\n",
    "IMAGE_RGB_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGE_RGB_STD  = [0.229, 0.224, 0.225]\n",
    "DEFECT_COLOR = [(0,0,0),(0,0,255),(0,255,0),(255,0,0),(0,255,255)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_DIR = 'data/split'\n",
    "DATA_DIR = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class Net(nn.Module):\n",
    "#    def load_pretrain(self, skip, is_print=True):\n",
    "#        conversion=copy.copy(CONVERSION)\n",
    "#        for i in range(0,len(conversion)-8,4):\n",
    "#            conversion[i] = 'block.' + conversion[i][5:]\n",
    "#        load_pretrain(self, skip, pretrain_file=PRETRAIN_FILE, conversion=conversion, is_print=is_print)\n",
    "#\n",
    "#    def __init__(self, num_class=4, drop_connect_rate=0.2):\n",
    "#        super(Net, self).__init__()\n",
    "#\n",
    "#        e = ResNet34()\n",
    "#        self.block = nn.ModuleList([\n",
    "#            e.block0,\n",
    "#            e.block1,\n",
    "#            e.block2,\n",
    "#            e.block3,\n",
    "#            e.block4,\n",
    "#        ])\n",
    "#        e = None  #dropped\n",
    "#        self.feature = nn.Conv2d(512,32, kernel_size=1) #dummy conv for dim reduction\n",
    "#        self.logit = nn.Conv2d(32,num_class, kernel_size=1)\n",
    "#\n",
    "#    def forward(self, x):\n",
    "#        batch_size,C,H,W = x.shape\n",
    "#\n",
    "#        for i in range( len(self.block)):\n",
    "#            x = self.block[i](x)\n",
    "#            #print(i, x.shape)\n",
    "#\n",
    "#        x = F.dropout(x,0.5,training=self.training)\n",
    "#        x = F.adaptive_avg_pool2d(x, 1)\n",
    "#        x = self.feature(x)\n",
    "#        logit = self.logit(x)\n",
    "#        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def load_pretrain(self, skip=['logit.'], is_print=True):\n",
    "        load_pretrain(self, skip, pretrain_file=PRETRAIN_FILE, conversion=CONVERSION, is_print=is_print)\n",
    "\n",
    "\n",
    "    def __init__(self, num_class=4):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        e = ResNext50()\n",
    "        self.block0 = e.block0\n",
    "        self.block1 = e.block1\n",
    "        self.block2 = e.block2\n",
    "        self.block3 = e.block3\n",
    "        self.block4 = e.block4\n",
    "        e = None  #dropped\n",
    "\n",
    "        self.feature = nn.Conv2d(2048, 64, kernel_size=1) #dummy conv for dim reduction\n",
    "        self.logit   = nn.Conv2d(64, num_class, kernel_size=1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size,C,H,W = x.shape\n",
    "        x = x.clone()\n",
    "        x = x-torch.FloatTensor(IMAGE_RGB_MEAN).to(x.device).view(1,-1,1,1)\n",
    "        x = x/torch.FloatTensor(IMAGE_RGB_STD).to(x.device).view(1,-1,1,1)\n",
    "\n",
    "        x = self.block0(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "\n",
    "        x = F.dropout(x,0.5,training=self.training)\n",
    "        x = F.avg_pool2d(x, kernel_size=(8, 13),stride=(8, 8))\n",
    "        #x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = self.feature(x)\n",
    "\n",
    "        logit = self.logit(x) #.view(batch_size,-1)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class which is used by the infor object in __get_item__\n",
    "class Struct(object):\n",
    "    def __init__(self, is_copy=False, **kwargs):\n",
    "        self.add(is_copy, **kwargs)\n",
    "\n",
    "    def add(self, is_copy=False, **kwargs):\n",
    "        #self.__dict__.update(kwargs)\n",
    "\n",
    "        if is_copy == False:\n",
    "            for key, value in kwargs.items():\n",
    "                setattr(self, key, value)\n",
    "        else:\n",
    "            for key, value in kwargs.items():\n",
    "                try:\n",
    "                    setattr(self, key, copy.deepcopy(value))\n",
    "                    #setattr(self, key, value.copy())\n",
    "                except Exception:\n",
    "                    setattr(self, key, value)\n",
    "\n",
    "    def __str__(self):\n",
    "        text =''\n",
    "        for k,v in self.__dict__.items():\n",
    "            text += '\\t%s : %s\\n'%(k, str(v))\n",
    "        return text\n",
    "\n",
    "# Creating masks\n",
    "def run_length_decode(rle, height=256, width=1600, fill_value=1):\n",
    "    mask = np.zeros((height,width), np.float32)\n",
    "    if rle != '':\n",
    "        mask=mask.reshape(-1)\n",
    "        r = [int(r) for r in rle.split(' ')]\n",
    "        r = np.array(r).reshape(-1, 2)\n",
    "        for start,length in r:\n",
    "            start = start-1  #???? 0 or 1 index ???\n",
    "            mask[start:(start + length)] = fill_value\n",
    "        mask=mask.reshape(width, height).T\n",
    "    return mask\n",
    "\n",
    "# Collations\n",
    "def null_collate(batch):\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    input = []\n",
    "    truth_mask  = []\n",
    "    truth_label = []\n",
    "    infor = []\n",
    "    for b in range(batch_size):\n",
    "        input.append(batch[b][0])\n",
    "        truth_mask.append(batch[b][1])\n",
    "        infor.append(batch[b][2])\n",
    "\n",
    "        label = (batch[b][1].reshape(4,-1).sum(1)>8).astype(np.int32)\n",
    "        truth_label.append(label)\n",
    "\n",
    "\n",
    "    input = np.stack(input)\n",
    "    input = image_to_input(input, IMAGE_RGB_MEAN,IMAGE_RGB_STD)\n",
    "    input = torch.from_numpy(input).float()\n",
    "\n",
    "    truth_mask = np.stack(truth_mask)\n",
    "    truth_mask = (truth_mask>0.5).astype(np.float32)\n",
    "    truth_mask = torch.from_numpy(truth_mask).float()\n",
    "\n",
    "    truth_label = np.array(truth_label)\n",
    "    truth_label = torch.from_numpy(truth_label).float()\n",
    "\n",
    "    return input, truth_mask, truth_label, infor\n",
    "\n",
    "# Metric\n",
    "def metric_hit(logit, truth, threshold=0.5):\n",
    "    batch_size,num_class, H,W = logit.shape\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logit = logit.view(batch_size,num_class,-1)\n",
    "        truth = truth.view(batch_size,num_class,-1)\n",
    "\n",
    "        probability = torch.sigmoid(logit)\n",
    "        p = (probability>threshold).float()\n",
    "        t = (truth>0.5).float()\n",
    "\n",
    "        tp = ((p + t) == 2).float()  # True positives\n",
    "        tn = ((p + t) == 0).float()  # True negatives\n",
    "\n",
    "        tp = tp.sum(dim=[0,2])\n",
    "        tn = tn.sum(dim=[0,2])\n",
    "        num_pos = t.sum(dim=[0,2])\n",
    "        num_neg = batch_size*H*W - num_pos\n",
    "\n",
    "        tp = tp.data.cpu().numpy()\n",
    "        tn = tn.data.cpu().numpy().sum()\n",
    "        num_pos = num_pos.data.cpu().numpy()\n",
    "        num_neg = num_neg.data.cpu().numpy().sum()\n",
    "\n",
    "        tp = np.nan_to_num(tp/(num_pos+1e-12),0)\n",
    "        tn = np.nan_to_num(tn/(num_neg+1e-12),0)\n",
    "\n",
    "        tp = list(tp)\n",
    "        num_pos = list(num_pos)\n",
    "\n",
    "    return tn,tp, num_neg,num_pos\n",
    "\n",
    "# Loss\n",
    "#def criterion(logit, truth, weight=None):\n",
    "#    batch_size,num_class, H,W = logit.shape\n",
    "#    logit = logit.view(batch_size,num_class)\n",
    "#    truth = truth.view(batch_size,num_class)\n",
    "#    assert(logit.shape==truth.shape)\n",
    "#\n",
    "#    loss = F.binary_cross_entropy_with_logits(logit, truth, reduction='none')\n",
    "#\n",
    "#    if weight is None:\n",
    "#        loss = loss.mean()\n",
    "#\n",
    "#    else:\n",
    "#        pos = (truth>0.5).float()\n",
    "#        neg = (truth<0.5).float()\n",
    "#        pos_sum = pos.sum().item() + 1e-12\n",
    "#        neg_sum = neg.sum().item() + 1e-12\n",
    "#        loss = (weight[1]*pos*loss/pos_sum + weight[0]*neg*loss/neg_sum).sum()\n",
    "#        #raise NotImplementedError\n",
    "#\n",
    "#    return loss\n",
    "\n",
    "\n",
    "def criterion(logit, truth, weight=None):\n",
    "    batch_size,num_class = logit.shape[:2]\n",
    "    logit = logit.view(batch_size,num_class)\n",
    "    truth = truth.view(batch_size,num_class)\n",
    "\n",
    "    if weight is None: weight=[1,1,1,1]\n",
    "    weight = torch.FloatTensor(weight).to(truth.device).view(1,-1)\n",
    "\n",
    "    loss = F.binary_cross_entropy_with_logits(logit, truth, reduction='none')\n",
    "\n",
    "    loss = loss*weight\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n",
    "# Learning Rate Adjustments\n",
    "def adjust_learning_rate(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def get_learning_rate(optimizer):\n",
    "    lr=[]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr += [param_group['lr']]\n",
    "\n",
    "    assert(len(lr)==1) #we support only one param_group\n",
    "    lr = lr[0]\n",
    "    return lr\n",
    "\n",
    "# Learning Rate Schedule\n",
    "class NullScheduler():\n",
    "    def __init__(self, lr=0.01 ):\n",
    "        super(NullScheduler, self).__init__()\n",
    "        self.lr    = lr\n",
    "        self.cycle = 0\n",
    "\n",
    "    def __call__(self, time):\n",
    "        return self.lr\n",
    "\n",
    "    def __str__(self):\n",
    "        string = 'NullScheduler\\n' \\\n",
    "                + 'lr=%0.5f '%(self.lr)\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteelDataset(Dataset):\n",
    "    def __init__(self, split, csv, mode, augment=None):\n",
    "        self.split   = split\n",
    "        self.csv     = csv\n",
    "        self.mode    = mode\n",
    "        self.augment = augment\n",
    "\n",
    "        self.uid = list(np.concatenate([np.load(SPLIT_DIR + '/%s'%f , allow_pickle=True) for f in split]))\n",
    "        df = pd.concat([pd.read_csv(DATA_DIR + '/%s'%f) for f in csv])\n",
    "        df.fillna('', inplace=True)\n",
    "        df['Class'] = df['ImageId_ClassId'].str[-1].astype(np.int32)\n",
    "        df['Label'] = (df['EncodedPixels']!='').astype(np.int32)\n",
    "        df = df_loc_by_list(df, 'ImageId_ClassId', [ u.split('/')[-1] + '_%d'%c  for u in self.uid for c in [1,2,3,4] ])\n",
    "        self.df = df\n",
    "\n",
    "    def __str__(self):\n",
    "        num1 = (self.df['Class']==1).sum()\n",
    "        num2 = (self.df['Class']==2).sum()\n",
    "        num3 = (self.df['Class']==3).sum()\n",
    "        num4 = (self.df['Class']==4).sum()\n",
    "        pos1 = ((self.df['Class']==1) & (self.df['Label']==1)).sum()\n",
    "        pos2 = ((self.df['Class']==2) & (self.df['Label']==1)).sum()\n",
    "        pos3 = ((self.df['Class']==3) & (self.df['Label']==1)).sum()\n",
    "        pos4 = ((self.df['Class']==4) & (self.df['Label']==1)).sum()\n",
    "\n",
    "        length = len(self)\n",
    "        num = len(self)*4\n",
    "        pos = (self.df['Label']==1).sum()\n",
    "        neg = num-pos\n",
    "\n",
    "        string  = ''\n",
    "        string += '\\tmode    = %s\\n'%self.mode\n",
    "        string += '\\tsplit   = %s\\n'%self.split\n",
    "        string += '\\tcsv     = %s\\n'%str(self.csv)\n",
    "        string += '\\t\\tlen   = %5d\\n'%len(self)\n",
    "        if self.mode == 'train':\n",
    "            string += '\\t\\tnum   = %5d\\n'%num\n",
    "            string += '\\t\\tneg   = %5d  %0.3f\\n'%(neg,neg/num)\n",
    "            string += '\\t\\tpos   = %5d  %0.3f\\n'%(pos,pos/num)\n",
    "            string += '\\t\\tpos1  = %5d  %0.3f  %0.3f\\n'%(pos1,pos1/length,pos1/pos)\n",
    "            string += '\\t\\tpos2  = %5d  %0.3f  %0.3f\\n'%(pos2,pos2/length,pos2/pos)\n",
    "            string += '\\t\\tpos3  = %5d  %0.3f  %0.3f\\n'%(pos3,pos3/length,pos3/pos)\n",
    "            string += '\\t\\tpos4  = %5d  %0.3f  %0.3f\\n'%(pos4,pos4/length,pos4/pos)\n",
    "        return string\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.uid)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        folder, image_id = self.uid[index].split('/')\n",
    "        rle = [\n",
    "            self.df.loc[self.df['ImageId_ClassId']==image_id + '_1','EncodedPixels'].values[0],\n",
    "            self.df.loc[self.df['ImageId_ClassId']==image_id + '_2','EncodedPixels'].values[0],\n",
    "            self.df.loc[self.df['ImageId_ClassId']==image_id + '_3','EncodedPixels'].values[0],\n",
    "            self.df.loc[self.df['ImageId_ClassId']==image_id + '_4','EncodedPixels'].values[0],\n",
    "        ]\n",
    "        \n",
    "        image = cv2.imread(DATA_DIR + '/%s/%s'%(folder,image_id), cv2.IMREAD_COLOR)\n",
    "        mask  = np.array([run_length_decode(r, height=256, width=1600, fill_value=1) for r in rle])\n",
    "\n",
    "        infor = Struct(\n",
    "            index    = index,\n",
    "            folder   = folder,\n",
    "            image_id = image_id,\n",
    "        )\n",
    "\n",
    "        if self.augment is None:\n",
    "            return image, mask, infor\n",
    "        else:\n",
    "            return self.augment(image, mask, infor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiveBalanceClassSampler(Sampler):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        label = (self.dataset.df['Label'].values)\n",
    "        \n",
    "        #cannot reshape array of size 49155 into shape (4)\n",
    "        label = label.reshape(-1,4)\n",
    "        label = np.hstack([label.sum(1,keepdims=True)==0,label]).T\n",
    "\n",
    "        self.neg_index  = np.where(label[0])[0]\n",
    "        self.pos1_index = np.where(label[1])[0]\n",
    "        self.pos2_index = np.where(label[2])[0]\n",
    "        self.pos3_index = np.where(label[3])[0]\n",
    "        self.pos4_index = np.where(label[4])[0]\n",
    "\n",
    "        #5x\n",
    "        self.num_image = len(self.dataset.df)//4\n",
    "        self.length = self.num_image*5\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        neg  = np.random.choice(self.neg_index,  self.num_image, replace=True)\n",
    "        pos1 = np.random.choice(self.pos1_index, self.num_image, replace=True)\n",
    "        pos2 = np.random.choice(self.pos2_index, self.num_image, replace=True)\n",
    "        pos3 = np.random.choice(self.pos3_index, self.num_image, replace=True)\n",
    "        pos4 = np.random.choice(self.pos4_index, self.num_image, replace=True)\n",
    "\n",
    "        l = np.stack([neg,pos1,pos2,pos3,pos4]).T\n",
    "        l = l.reshape(-1)\n",
    "        return iter(l)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_valid(net, valid_loader, displays=None):\n",
    "    valid_num  = np.zeros(6, np.float32)\n",
    "    valid_loss = np.zeros(6, np.float32)\n",
    "    \n",
    "    for t, (input, truth_mask, truth_label, infor) in enumerate(valid_loader):\n",
    "\n",
    "        net.eval()\n",
    "        input = input.cuda()\n",
    "        truth_mask  = truth_mask.cuda()\n",
    "        truth_label = truth_label.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logit = net(input) #data_parallel(net, input)  \n",
    "            logit = logit.max(-1,True)[0]\n",
    "            loss  = criterion(logit, truth_label)\n",
    "            tn,tp, num_neg,num_pos = metric_hit(logit, truth_label)\n",
    "\n",
    "        batch_size = len(infor)\n",
    "        l = np.array([ loss.item(), tn,*tp])\n",
    "        n = np.array([ batch_size, num_neg,*num_pos])\n",
    "        valid_loss += l*n\n",
    "        valid_num  += n\n",
    "\n",
    "        if displays is not None:\n",
    "            probability = torch.sigmoid(logit)\n",
    "            image = input_to_image(input, IMAGE_RGB_MEAN,IMAGE_RGB_STD)\n",
    "\n",
    "            probability_label = probability.data.cpu().numpy()\n",
    "            truth_label = truth_label.data.cpu().numpy()\n",
    "            truth_mask  = truth_mask.data.cpu().numpy()\n",
    "\n",
    "            for b in range(0, batch_size, 4):\n",
    "                image_id = infor[b].image_id[:-4]\n",
    "                result = draw_predict_result_label(image[b], truth_mask[b], truth_label[b], probability_label[b], stack='vertical')\n",
    "                draw_shadow_text(result,'%05d    %s.jpg'%(valid_num[0]-batch_size+b, image_id),(5,24),0.75,[255,255,255],1)\n",
    "                image_show('result',result,resize=1)\n",
    "\n",
    "        print('\\r %8d /%8d'%(valid_num[0], len(valid_loader.dataset)),end='',flush=True)\n",
    "\n",
    "    assert(valid_num[0] == len(valid_loader.dataset))\n",
    "    valid_loss = valid_loss/valid_num\n",
    "\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'se_resnext50_32x4d'\n",
    "#net = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_head(nf:int, nc:int, lin_ftrs=None, ps=0.5, concat_pool:bool=True, bn_final:bool=False):\n",
    "#    \"Model head that takes `nf` features, runs through `lin_ftrs`, and about `nc` classes.\"\n",
    "#    lin_ftrs = [nf, 512, nc] if lin_ftrs is None else [nf] + lin_ftrs + [nc]\n",
    "#    ps = listify(ps)\n",
    "#    if len(ps) == 1: ps = [ps[0]/2] * (len(lin_ftrs)-2) + ps\n",
    "#    actns = [nn.ReLU(inplace=True)] * (len(lin_ftrs)-2) + [None]\n",
    "#    pool = AdaptiveConcatPool2d() if concat_pool else nn.AdaptiveAvgPool2d(1)\n",
    "#    layers = [pool, Flatten()]\n",
    "#    for ni,no,p,actn in zip(lin_ftrs[:-1], lin_ftrs[1:], ps, actns):\n",
    "#        layers += bn_drop_lin(ni, no, True, p, actn)\n",
    "#    if bn_final: layers.append(nn.BatchNorm1d(lin_ftrs[-1], momentum=0.01))\n",
    "#    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_file =  glob.glob('data/train_images/*.jpg') #train_pseudolabel_images\n",
    "#image_file = ['train_images/'+i.split('/')[-1] for i in image_file] #train_pseudolabel_images\n",
    "#print(len(image_file))\n",
    "#print(image_file[:4])\n",
    "#\n",
    "#random.shuffle(image_file)\n",
    "#print(image_file[:4])\n",
    "#\n",
    "##12568\n",
    "#num_valid = 600\n",
    "#num_all   = len(image_file)\n",
    "#num_train = num_all-num_valid\n",
    "#\n",
    "#train=np.array(image_file[num_valid:])\n",
    "#valid=np.array(image_file[:num_valid])\n",
    "#\n",
    "#np.save('data/split/train0_%d.npy'%len(train),train)\n",
    "#np.save('data/split/valid0_%d.npy'%len(valid),valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    batch_size = 6\n",
    "    \n",
    "    initial_checkpoint = None\n",
    "    #'data/classification_models/00028500_model.pth'\n",
    "    #'resnet34-cls-full-foldb0-0/checkpoint/00007500_model.pth'\n",
    "    \n",
    "    train_dataset = SteelDataset(\n",
    "        mode    = 'train',\n",
    "        csv     = ['train_pseudolabel_segmentation.csv',], #train_pseudolabel_segmentation.csv\n",
    "        split   = ['train0_13769.npy'], #train0_11968.npy\n",
    "        augment = train_augment,\n",
    "    )\n",
    "    train_loader  = DataLoader(\n",
    "        train_dataset,\n",
    "        #sampler     = BalanceClassSampler(train_dataset, 3*len(train_dataset)),\n",
    "        #sampler    = SequentialSampler(train_dataset),\n",
    "        #sampler    = RandomSampler(train_dataset),\n",
    "        sampler    = FiveBalanceClassSampler(train_dataset),\n",
    "        batch_size  = batch_size,\n",
    "        drop_last   = True,\n",
    "        num_workers = 4,\n",
    "        pin_memory  = True,\n",
    "        collate_fn  = null_collate\n",
    "    )\n",
    "\n",
    "    valid_dataset = SteelDataset(\n",
    "        mode    = 'train',\n",
    "        csv     = ['train_pseudolabel_segmentation.csv'], #train_pseudolabel_segmentation.csv\n",
    "        split   = ['valid0_600.npy'], #valid_b1_1000.npy\n",
    "        augment = valid_augment,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        sampler    = SequentialSampler(valid_dataset),\n",
    "        #sampler     = RandomSampler(valid_dataset),\n",
    "        batch_size  = 4,\n",
    "        drop_last   = False,\n",
    "        num_workers = 4,\n",
    "        pin_memory  = True,\n",
    "        collate_fn  = null_collate\n",
    "    )\n",
    "    \n",
    "    assert(len(train_dataset)>=batch_size)\n",
    "    \n",
    "    net = Net().cuda()\n",
    "    #model_name = 'se_resnext50_32x4d'\n",
    "    #net = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\n",
    "    #net.last_linear = torch.nn.Linear(in_features=180224, out_features=4, bias=True)\n",
    "    #net = net.cuda()\n",
    "    \n",
    "    #if initial_checkpoint is not None:\n",
    "    #    state_dict = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)\n",
    "    #    #for k in ['logit.weight','logit.bias']: state_dict.pop(k, None)\n",
    "\n",
    "    #    net.load_state_dict(state_dict,strict=False)\n",
    "    #else:\n",
    "    #    net.load_pretrain(net.e, skip=['logit'], is_print=False)\n",
    "\n",
    "    num_iters   = 50*1000\n",
    "    iter_smooth = 50\n",
    "    iter_log    = 500\n",
    "    iter_valid  = 1000\n",
    "    iter_save   = [num_iters-1] + list(range(0, num_iters, 1000))#1*1000\n",
    "    \n",
    "    #optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=schduler(0), momentum=0.9, weight_decay=0.0001)\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()))\n",
    "    #scheduler = NullScheduler(lr=0.001)\n",
    "    max_lr = 0.001 #0.0002\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=max_lr, div_factor=25, pct_start=0.3, total_steps=num_iters)\n",
    "    lr = scheduler.get_lr()[0]\n",
    "    \n",
    "    start_iter = 0\n",
    "    start_epoch= 0\n",
    "    rate       = 0\n",
    "    if initial_checkpoint is not None:\n",
    "        initial_optimizer = initial_checkpoint.replace('_model.pth','_optimizer.pth')\n",
    "        if os.path.exists(initial_optimizer):\n",
    "            checkpoint  = torch.load(initial_optimizer)\n",
    "            start_iter  = checkpoint['iter' ]\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        pass\n",
    "    \n",
    "    train_loss = np.zeros(20,np.float32)\n",
    "    valid_loss = np.zeros(20,np.float32)\n",
    "    batch_loss = np.zeros(20,np.float32)\n",
    "    iter_accum = 8\n",
    "    iter = 0\n",
    "    i    = 0\n",
    "    \n",
    "    start = timer()\n",
    "    while  iter<num_iters:\n",
    "        sum_train_loss = np.zeros(20,np.float32)\n",
    "        sum = np.zeros(20,np.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        for t, (input, truth_mask, truth_label, infor) in enumerate(train_loader):\n",
    "            batch_size = len(infor)\n",
    "            iter  = i + start_iter\n",
    "            epoch = (iter-start_iter)*batch_size/len(train_dataset) + start_epoch\n",
    "            \n",
    "            # Weather to display images or not! While in validation loss\n",
    "            displays = None\n",
    "            #if 0:\n",
    "            if (iter % iter_valid==0):\n",
    "                valid_loss = do_valid(net, valid_loader, displays) # omitted outdir variable\n",
    "                #pass\n",
    "\n",
    "            if (iter % iter_log==0):\n",
    "                print('\\r',end='',flush=True)\n",
    "                asterisk = '*' if iter in iter_save else ' '\n",
    "                print('%0.8f  %5.1f%s %5.1f |  %5.3f   %4.4f [%4.4f,%4.4f,%4.4f,%4.4f]  |  %5.3f   %4.4f [%4.4f,%4.4f,%4.4f,%4.4f]  | %s' % (\\\n",
    "                         lr, iter/1000, asterisk, epoch,\n",
    "                         *valid_loss[:6],\n",
    "                         *train_loss[:6],\n",
    "                         time_to_str((timer() - start)))\n",
    "                )\n",
    "                print('\\n')\n",
    "                \n",
    "            #if 0:\n",
    "            if iter in iter_save:\n",
    "                torch.save(net.state_dict(),'data/classification_models/%08d_model.pth'%(iter))\n",
    "                torch.save({\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'iter'     : iter,\n",
    "                    'epoch'    : epoch,\n",
    "                }, 'data/classification_models/%08d_optimizer.pth'%(iter))\n",
    "                pass\n",
    "\n",
    "            # learning rate schduler -------------\n",
    "            lr = scheduler.get_lr()[0]\n",
    "            if lr<0 : break\n",
    "            #print(lr)\n",
    "            #adjust_learning_rate(optimizer, lr)\n",
    "            #rate = get_learning_rate(optimizer)\n",
    "            \n",
    "            net.train()\n",
    "            input = input.cuda()\n",
    "            truth_label = truth_label.cuda()\n",
    "            truth_mask  = truth_mask.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logit =  net(input) #data_parallel(net,input)  \n",
    "            logit = logit.max(-1,True)[0]\n",
    "            loss = criterion(logit, truth_label)\n",
    "            tn,tp, num_neg,num_pos = metric_hit(logit, truth_label)\n",
    "            \n",
    "            (loss/iter_accum).backward()\n",
    "            if (iter % iter_accum)==0:\n",
    "                optimizer.step()\n",
    "                scheduler.step(iter)\n",
    "\n",
    "            # print statistics  ------------\n",
    "            l = np.array([ loss.item(), tn,*tp ])\n",
    "            n = np.array([ batch_size, num_neg,*num_pos ])\n",
    "\n",
    "            batch_loss[:6] = l\n",
    "            sum_train_loss[:6] += l*n\n",
    "            sum[:6] += n\n",
    "            if iter%iter_smooth == 0:\n",
    "                train_loss = sum_train_loss/(sum+1e-12)\n",
    "                sum_train_loss[...] = 0\n",
    "                sum[...]            = 0\n",
    "\n",
    "\n",
    "            print('\\r',end='',flush=True)\n",
    "            asterisk = ' '\n",
    "            print('%0.8f  %5.1f%s %5.1f |  %5.3f   %4.4f [%4.4f,%4.4f,%4.4f,%4.4f]  |  %5.3f   %4.2f [%4.4f,%4.4f,%4.4f,%4.4f]  | %s' % (\\\n",
    "                         lr, iter/1000, asterisk, epoch,\n",
    "                         *valid_loss[:6],\n",
    "                         *batch_loss[:6],\n",
    "                         time_to_str((timer() - start)))\n",
    "            , end='',flush=True)\n",
    "            i=i+1\n",
    "            \n",
    "            # debug-----------------------------\n",
    "            if 1:\n",
    "                for di in range(3):\n",
    "                    if (iter+di)%1000==0:\n",
    "\n",
    "                        probability = torch.sigmoid(logit)\n",
    "                        image = input_to_image(input, IMAGE_RGB_MEAN,IMAGE_RGB_STD)\n",
    "\n",
    "                        probability_label = probability.data.cpu().numpy()\n",
    "                        truth_label = truth_label.data.cpu().numpy()\n",
    "                        truth_mask  = truth_mask.data.cpu().numpy()\n",
    "\n",
    "\n",
    "                        for b in range(batch_size):\n",
    "                            result = draw_predict_result_label(image[b], truth_mask[b], truth_label[b], probability_label[b], stack='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr          iter   epoch |  loss    tn, [tp1,tp2,tp3,tp4]       |  loss    tn, [tp1,tp2,tp3,tp4]       | time           \n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "0.00004    0.0*   0.0 |  0.686   0.7153 [0.0000,1.0000,0.0000,0.0000]  |  0.000   0.0000 [0.0000,0.0000,0.0000,0.0000]  |  0 hr 00 min\n",
      "\n",
      "\n",
      "0.00004017    0.1    0.1 |  0.686   0.7153 [0.0000,1.0000,0.0000,0.0000]  |  0.567   1.00 [0.0000,0.0000,0.0000,0.0000]  |  0 hr 02 min"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-75afe7b8ebb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lr          iter   epoch |  loss    tn, [tp1,tp2,tp3,tp4]       |  loss    tn, [tp1,tp2,tp3,tp4]       | time           '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--------------------------------------------------------------------------------------------------------------------\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-d0beadca228a>\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# print statistics  ------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_pos\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('lr          iter   epoch |  loss    tn, [tp1,tp2,tp3,tp4]       |  loss    tn, [tp1,tp2,tp3,tp4]       | time           ')\n",
    "print('--------------------------------------------------------------------------------------------------------------------\\n')\n",
    "run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
